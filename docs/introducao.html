<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="pt" xml:lang="pt"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Resolvendo Captchas - 1&nbsp; Introdução</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 1em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./metodologia.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "Nenhum resultado",
    "search-matching-documents-text": "documentos correspondentes",
    "search-copy-link-title": "Copiar link para a busca",
    "search-hide-matches-text": "Esconder correspondências adicionais",
    "search-more-match-text": "mais correspondência neste documento",
    "search-more-matches-text": "mais correspondências neste documento",
    "search-clear-button-title": "Limpar",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar"
  }
}</script>


<meta name="citation_title" content="[[1]{.chapter-number}&nbsp; [Introdução]{.chapter-title}]{#sec-introducao .quarto-section-identifier}">
<meta name="citation_language" content="pt">
<meta name="citation_reference" content="citation_title=Understanding dropout;,citation_author=Pierre Baldi;,citation_author=Peter J. Sadowski;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_volume=26;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=A note on learning from multiple-instance examples;,citation_author=Avrim Blum;,citation_author=Adam Kalai;,citation_publication_date=1998;,citation_cover_date=1998;,citation_year=1998;,citation_issue=1;,citation_volume=30;,citation_journal_title=Machine learning;,citation_publisher=Springer;">
<meta name="citation_reference" content="citation_title=Captcha - Google Acadêmico;,citation_publisher=https://scholar.google.com.br/scholar?hl=pt-BR&amp;amp;amp;as_sdt=0%2C5&amp;q=captcha&amp;btnG=;">
<meta name="citation_reference" content="citation_title=Building Segmentation Based Human-Friendly Human Interaction Proofs (HIPs);,citation_abstract=Human interaction proofs (HIPs) have become common place on the internet due to their effectiveness in deterring automated abuse of online services intended for humans. However, there is a co-evolutionary arms race in progress and these proofs are becoming more difficult for genuine users while attackers are getting better at breaking existing HIPs. We studied various popular HIPs on the internet to understand their strength and human friendliness. To determine HIP strength, we adopted a direct approach of building computer attacks using image processing and machine learning techniques. To understand human-friendliness, a sequence of users studies were conducted to investigate HIP character recognition by humans under a variety of visual distortions and clutter commonly employed in reading-based HIPs. We found that many of the online HIPs are pure recognition tasks that can be easily broken using machine learning. The stronger HIPs tend to pose a combination of segmentation and recognition challenges. Further, the HIP user studies show that given correct segmentation, computers are much better at HIP character recognition than humans. In light of these results, we propose that segmentation-based reading challenges are the future for building stronger human-friendly HIPs. An example of such a segmentation-based HIP is presented with a preliminary assessment of its strength and human-friendliness.;,citation_author=Kumar Chellapilla;,citation_author=Kevin Larson;,citation_author=Patrice Y. Simard;,citation_author=Mary Czerwinski;,citation_editor=Henry S. Baird;,citation_editor=Daniel P. Lopresti;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_doi=10.1007/11427896_1;,citation_isbn=978-3-540-32117-0;,citation_language=en-US;,citation_conference_title=Human Interactive Proofs;,citation_conference=Springer;,citation_series_title=Lecture Notes in Computer Science;">
<meta name="citation_reference" content="citation_title=Designing human friendly human interaction proofs (HIPs);,citation_abstract=HIPs, or Human Interactive Proofs, are challenges meant to be easily solved by humans, while remaining too hard to be economically solved by computers. HIPs are increasingly used to protect services against automatic script attacks. To be effective, a HIP must be difficult enough to discourage script attacks by raising the computation and/or development cost of breaking the HIP to an unprofitable level. At the same time, the HIP must be easy enough to solve in order to not discourage humans from using the service. Early HIP designs have successfully met these criteria [1]. However, the growing sophistication of attackers and correspondingly increasing profit incentives have rendered most of the currently deployed HIPs vulnerable to attack [2,7,12]. Yet, most companies have been reluctant to increase the difficulty of their HIPs for fear of making them too complex or unappealing to humans. The purpose of this study is to find the visual distortions that are most effective at foiling computer attacks without hindering humans. The contribution of this research is that we discovered that 1) automatically generating HIPs by varying particular distortion parameters renders HIPs that are too easy for computer hackers to break, yet humans still have difficulty recognizing them, and 2) it is possible to build segmentation-based HIPs that are extremely difficult and expensive for computers to solve, while remaining relatively easy for humans.;,citation_author=Kumar Chellapilla;,citation_author=Kevin Larson;,citation_author=Patrice Simard;,citation_author=Mary Czerwinski;,citation_publication_date=2005-04;,citation_cover_date=2005-04;,citation_year=2005;,citation_doi=10.1145/1054972.1055070;,citation_isbn=978-1-58113-998-3;,citation_conference_title=Proceedings of the SIGCHI Conference on Human Factors in Computing Systems;,citation_conference=Association for Computing Machinery;,citation_series_title=CHI ’05;">
<meta name="citation_reference" content="citation_title=Using machine learning to break visual human interaction proofs (HIPs);,citation_author=Kumar Chellapilla;,citation_author=Patrice Simard;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_volume=17;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=Consistency of losses for learning from weak labels;,citation_author=Jesús Cid-Sueiro;,citation_author=Darío García-García;,citation_author=Raúl Santos-Rodríguez;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_conference_title=Joint European Conference on Machine Learning and Knowledge Discovery in Databases;,citation_conference=Springer;">
<meta name="citation_reference" content="citation_title=Análise de sobrevivência aplicada;,citation_author=Enrico Antonio Colosimo;,citation_author=Suely Ruiz Giolo;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;">
<meta name="citation_reference" content="citation_title=Learning from partial labels;,citation_author=Timothee Cour;,citation_author=Ben Sapp;,citation_author=Ben Taskar;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_volume=12;,citation_journal_title=The Journal of Machine Learning Research;,citation_publisher=JMLR. org;">
<meta name="citation_reference" content="citation_title=Analysis of perceptron-based active learning;,citation_author=Sanjoy Dasgupta;,citation_author=Adam Tauman Kalai;,citation_author=Claire Monteleoni;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_conference_title=International conference on computational learning theory;,citation_conference=Springer;">
<meta name="citation_reference" content="citation_title=Diagnóstico do Contencioso Tributário Administrativo;,citation_abstract=Estudo sobre estrutura do contencioso, processos administrativos tributários e percepções dos agentes;,citation_publisher=https://abj.org.br/pesquisas/bid-tributario/;">
<meta name="citation_reference" content="citation_title=Partial Label Learning with competitive learning graph neural network;,citation_abstract=Partial Label Learning (PLL) is a weakly supervised learning framework where each instance may be associated with more than one candidate label, among which only one is true. Traditionally, the PLL problem is solved by removing the false candidate labels based on the instance relationship, while the potentially useful information between instances and labels as well as the potential candidate label relationship is ignored. In this paper, a new PLL framework PL-CGNN is proposed, which treats the instances with false labels as noise, and the PLL is reformulated to remove the noise instances. First of all, the feature of each label class is approximately represented by the center point of all the related instances. The significant operation enables the similarity between instances and labels measurable. Next, all the candidate labels for each instance compete for the biggest similarity. To further improve the robustness of the model, the competition procedure for the most similar label is extended to the neighbors of this instance. The label with the most wins is the final ground-truth one. The relationship between candidate labels guides the situation that the competition process develops into. Through iterative competitive learning, each label class approaches the true value. Experiments carried out on diverse datasets show that the performance of the PL-CGNN model is outstanding.;,citation_author=Jinfu Fan;,citation_author=Yang Yu;,citation_author=Zhongjie Wang;,citation_publication_date=2022-05;,citation_cover_date=2022-05;,citation_year=2022;,citation_doi=10.1016/j.engappai.2022.104779;,citation_issn=0952-1976;,citation_volume=111;,citation_language=en-US;,citation_journal_title=Engineering Applications of Artificial Intelligence;">
<meta name="citation_reference" content="citation_title=Learning with multiple complementary labels;,citation_author=Lei Feng;,citation_author=Takuo Kaneko;,citation_author=Bo Han;,citation_author=Gang Niu;,citation_author=Bo An;,citation_author=Masashi Sugiyama;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_conference_title=International Conference on Machine Learning;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=Learning with multiple complementary labels;,citation_author=Lei Feng;,citation_author=Takuo Kaneko;,citation_author=Bo Han;,citation_author=Gang Niu;,citation_author=Bo An;,citation_author=Masashi Sugiyama;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_conference_title=International Conference on Machine Learning;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=Provably consistent partial-label learning;,citation_author=Lei Feng;,citation_author=Jiaqi Lv;,citation_author=Bo Han;,citation_author=Miao Xu;,citation_author=Gang Niu;,citation_author=Xin Geng;,citation_author=Bo An;,citation_author=Masashi Sugiyama;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_volume=33;,citation_journal_title=Advances in Neural Information Processing Systems;">
<meta name="citation_reference" content="citation_title=Provably consistent partial-label learning;,citation_author=Lei Feng;,citation_author=Jiaqi Lv;,citation_author=Bo Han;,citation_author=Miao Xu;,citation_author=Gang Niu;,citation_author=Xin Geng;,citation_author=Bo An;,citation_author=Masashi Sugiyama;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_volume=33;,citation_journal_title=Advances in Neural Information Processing Systems;">
<meta name="citation_reference" content="citation_title=A review on ensembles for the class imbalance problem: Bagging-, boosting-, and hybrid-based approaches;,citation_author=Mikel Galar;,citation_author=Alberto Fernandez;,citation_author=Edurne Barrenechea;,citation_author=Humberto Bustince;,citation_author=Francisco Herrera;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=4;,citation_volume=42;,citation_journal_title=IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews);,citation_publisher=IEEE;">
<meta name="citation_reference" content="citation_title=A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs;,citation_author=Dileep George;,citation_author=Wolfgang Lehrach;,citation_author=Ken Kansky;,citation_author=Miguel Lázaro-Gredilla;,citation_author=Christopher Laan;,citation_author=Bhaskara Marthi;,citation_author=Xinghua Lou;,citation_author=Zhaoshi Meng;,citation_author=Yi Liu;,citation_author=Huayan Wang;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=6368;,citation_volume=358;,citation_journal_title=Science;,citation_publisher=American Association for the Advancement of Science;">
<meta name="citation_reference" content="citation_title=Partial Label Learning via Label Influence Function;,citation_author=Xiuwen Gong;,citation_author=Dong Yuan;,citation_author=Wei Bao;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_conference_title=International Conference on Machine Learning;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=Generative Adversarial Networks;,citation_abstract=We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.;,citation_author=Ian J. Goodfellow;,citation_author=Jean Pouget-Abadie;,citation_author=Mehdi Mirza;,citation_author=Bing Xu;,citation_author=David Warde-Farley;,citation_author=Sherjil Ozair;,citation_author=Aaron Courville;,citation_author=Yoshua Bengio;,citation_publication_date=2014-06;,citation_cover_date=2014-06;,citation_year=2014;,citation_fulltext_html_url=https://arxiv.org/abs/1406.2661;,citation_doi=10.48550/arXiv.1406.2661;,citation_publisher=arXiv;">
<meta name="citation_reference" content="citation_title=Generative adversarial networks;,citation_author=Ian Goodfellow;,citation_author=Jean Pouget-Abadie;,citation_author=Mehdi Mirza;,citation_author=Bing Xu;,citation_author=David Warde-Farley;,citation_author=Sherjil Ozair;,citation_author=Aaron Courville;,citation_author=Yoshua Bengio;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=11;,citation_volume=63;,citation_journal_title=Communications of the ACM;,citation_publisher=ACM New York, NY, USA;">
<meta name="citation_reference" content="citation_title=Multi-digit number recognition from street view imagery using deep convolutional neural networks;,citation_author=Ian J. Goodfellow;,citation_author=Yaroslav Bulatov;,citation_author=Julian Ibarz;,citation_author=Sacha Arnoud;,citation_author=Vinay Shet;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_fulltext_html_url=https://arxiv.org/abs/1312.6082;,citation_journal_title=arXiv preprint arXiv:1312.6082;">
<meta name="citation_reference" content="citation_title=Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks;,citation_abstract=Recognizing arbitrary multi-character text in unconstrained natural photographs is a hard problem. In this paper, we address an equally hard sub-problem in this domain viz. recognizing arbitrary multi-digit numbers from Street View imagery. Traditional approaches to solve this problem typically separate out the localization, segmentation, and recognition steps. In this paper we propose a unified approach that integrates these three steps via the use of a deep convolutional neural network that operates directly on the image pixels. We employ the DistBelief implementation of deep neural networks in order to train large, distributed neural networks on high quality images. We find that the performance of this approach increases with the depth of the convolutional network, with the best performance occurring in the deepest architecture we trained, with eleven hidden layers. We evaluate this approach on the publicly available SVHN dataset and achieve over $96\%$ accuracy in recognizing complete street numbers. We show that on a per-digit recognition task, we improve upon the state-of-the-art, achieving $97.84\%$ accuracy. We also evaluate this approach on an even more challenging dataset generated from Street View imagery containing several tens of millions of street number annotations and achieve over $90\%$ accuracy. To further explore the applicability of the proposed system to broader text recognition tasks, we apply it to synthetic distorted text from reCAPTCHA. reCAPTCHA is one of the most secure reverse turing tests that uses distorted text to distinguish humans from bots. We report a $99.8\%$ accuracy on the hardest category of reCAPTCHA. Our evaluations on both tasks indicate that at specific operating thresholds, the performance of the proposed system is comparable to, and in some cases exceeds, that of human operators.;,citation_author=Ian J. Goodfellow;,citation_author=Yaroslav Bulatov;,citation_author=Julian Ibarz;,citation_author=Sacha Arnoud;,citation_author=Vinay Shet;,citation_publication_date=2014-04;,citation_cover_date=2014-04;,citation_year=2014;,citation_fulltext_html_url=https://arxiv.org/abs/1312.6082;,citation_doi=10.48550/arXiv.1312.6082;,citation_publisher=arXiv;">
<meta name="citation_reference" content="citation_title=Logistic regression for partial labels;,citation_author=Yves Grandvalet;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_conference_title=Proc. IPMU;">
<meta name="citation_reference" content="citation_title=Weak supervision and other non-standard classification problems: A taxonomy;,citation_author=Jerónimo Hernández-González;,citation_author=Inaki Inza;,citation_author=Jose A. Lozano;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_volume=69;,citation_journal_title=Pattern Recognition Letters;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Learning from ambiguously labeled examples;,citation_author=Eyke Hüllermeier;,citation_author=Jürgen Beringer;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_issue=5;,citation_volume=10;,citation_journal_title=Intelligent Data Analysis;,citation_publisher=IOS Press;">
<meta name="citation_reference" content="citation_title=Inaccessibility of CAPTCHA;,citation_publisher=https://www.w3.org/TR/turingtest/;">
<meta name="citation_reference" content="citation_title=Batch normalization: Accelerating deep network training by reducing internal covariate shift;,citation_author=Sergey Ioffe;,citation_author=Christian Szegedy;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_conference_title=International conference on machine learning;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=Learning from complementary labels;,citation_author=Takashi Ishida;,citation_author=Gang Niu;,citation_author=Weihua Hu;,citation_author=Masashi Sugiyama;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_volume=30;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=Learning from complementary labels;,citation_author=Takashi Ishida;,citation_author=Gang Niu;,citation_author=Weihua Hu;,citation_author=Masashi Sugiyama;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_volume=30;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=Learning with multiple labels;,citation_author=Rong Jin;,citation_author=Zoubin Ghahramani;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_volume=15;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=Online multiclass classification based on prediction margin for partial feedback;,citation_author=Takuo Kaneko;,citation_author=Issei Sato;,citation_author=Masashi Sugiyama;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://arxiv.org/abs/1902.01056;,citation_journal_title=arXiv preprint arXiv:1902.01056;">
<meta name="citation_reference" content="citation_title=Captcha and Its Techniques: A Review;,citation_abstract=Captcha (Completely Automated public Turing test to tell Computers and Humans Apart) system is used to verify whether a user is human or computer program. It’s also known as human Interactive Proof (HIP) and based upon Artificial Intelligence. Captcha is a program that protects websites from web-bots by generating tests that computer cannot pass but human can pass .This paper comprise introduction of captcha, various techniques, application of captcha and drawbacks of captcha.;,citation_author=Kiranjot Kaur;,citation_author=Sunny Behal;,citation_publication_date=2014-01;,citation_cover_date=2014-01;,citation_year=2014;,citation_volume=5;,citation_journal_title=International Journal of Computer Science and Information Technologies,;">
<meta name="citation_reference" content="citation_title=Adam: A Method for Stochastic Optimization;,citation_abstract=We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.;,citation_author=Diederik P. Kingma;,citation_author=Jimmy Ba;,citation_publication_date=2017-01;,citation_cover_date=2017-01;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1412.6980;,citation_doi=10.48550/arXiv.1412.6980;,citation_publisher=arXiv;">
<meta name="citation_reference" content="citation_title=Feature engineering and selection: A practical approach for predictive models;,citation_author=Max Kuhn;,citation_author=Kjell Johnson;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;">
<meta name="citation_reference" content="citation_title=Feature engineering and selection: A practical approach for predictive models;,citation_author=Max Kuhn;,citation_author=Kjell Johnson;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;">
<meta name="citation_reference" content="citation_title=Restricted set classification: Who is there?;,citation_author=Ludmila I. Kuncheva;,citation_author=Juan J. Rodriguez;,citation_author=Aaron S. Jackson;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_volume=63;,citation_journal_title=Pattern Recognition;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Deep learning;,citation_author=Yann LeCun;,citation_author=Yoshua Bengio;,citation_author=Geoffrey Hinton;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_issue=7553;,citation_volume=521;,citation_journal_title=nature;,citation_publisher=Nature Publishing Group;">
<meta name="citation_reference" content="citation_title=Efficient backprop;,citation_author=Yann A. LeCun;,citation_author=Léon Bottou;,citation_author=Genevieve B. Orr;,citation_author=Klaus-Robert Müller;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_inbook_title=Neural networks: Tricks of the trade;">
<meta name="citation_reference" content="citation_title=Gradient-based learning applied to document recognition;,citation_author=Yann LeCun;,citation_author=Léon Bottou;,citation_author=Yoshua Bengio;,citation_author=Patrick Haffner;,citation_publication_date=1998;,citation_cover_date=1998;,citation_year=1998;,citation_issue=11;,citation_volume=86;,citation_journal_title=Proceedings of the IEEE;,citation_publisher=Ieee;">
<meta name="citation_reference" content="citation_title=Method for selectively restricting access to computer systems;,citation_author=Mark D. Lillibridge;,citation_author=Martin Abadi;,citation_author=Krishna Bharat;,citation_author=Andrei Z. Broder;,citation_publication_date=2001-02;,citation_cover_date=2001-02;,citation_year=2001;">
<meta name="citation_reference" content="citation_title=Multivariate binomial/multinomial control chart;,citation_author=Jian Li;,citation_author=Fugee Tsung;,citation_author=Changliang Zou;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_issue=5;,citation_volume=46;,citation_journal_title=IIE Transactions;,citation_publisher=Taylor &amp;amp;amp; Francis;">
<meta name="citation_reference" content="citation_title=A conditional multinomial mixture model for superset label learning;,citation_author=Liping Liu;,citation_author=Thomas Dietterich;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_volume=25;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=Fuzzy-rough set based semi-supervised learning;,citation_author=Neil Mac Parthaláin;,citation_author=Richard Jensen;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_conference_title=2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Estado brasileiro e transparência avaliando a aplicação da Lei de Acesso à Informação;,citation_author=Gregory Michener;,citation_author=Luiz Fernando Moncau;,citation_author=Rafael Braem Velasco;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=Dynamic models;,citation_author=Helio S. Migon;,citation_author=Dani Gamerman;,citation_author=Hedibert F. Lopes;,citation_author=Marco AR Ferreira;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_volume=25;,citation_journal_title=Handbook of statistics;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Recognizing objects in adversarial clutter: Breaking a visual CAPTCHA;,citation_author=Greg Mori;,citation_author=Jitendra Malik;,citation_publication_date=2003;,citation_cover_date=2003;,citation_year=2003;,citation_volume=1;,citation_conference_title=2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings.;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Open data in science;,citation_author=Peter Murray-Rust;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_journal_title=Nature Precedings;,citation_publisher=Nature Publishing Group;">
<meta name="citation_reference" content="citation_title=AVA: A large-scale database for aesthetic visual analysis;,citation_author=Naila Murray;,citation_author=Luca Marchesotti;,citation_author=Florent Perronnin;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_conference_title=2012 IEEE conference on computer vision and pattern recognition;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Deep Generative Positive-Unlabeled Learning under Selection Bias;,citation_abstract=Learning in the positive-unlabeled (PU) setting is prevalent in real world applications. Many previous works depend upon theSelected Completely At Random (SCAR) assumption to utilize unlabeled data, but the SCAR assumption is not often applicable to the real world due to selection bias in label observations. This paper is the first generative PU learning model without the SCAR assumption. Specifically, we derive the PU risk function without the SCAR assumption, and we generate a set of virtual PU examples to train the classifier. Although our PU risk function is more generalizable, the function requires PU instances that do not exist in the observations. Therefore, we introduce the VAE-PU, which is a variant of variational autoencoders to separate two latent variables that generate either features or observation indicators. The separated latent information enables the model to generate virtual PU instances. We test the VAE-PU on benchmark datasets with and without the SCAR assumption. The results indicate that the VAE-PU is superior when selection bias exists, and the VAE-PU is also competent under the SCAR assumption. The results also emphasize that the VAE-PU is effective when there are few positive-labeled instances due to modeling on selection bias.;,citation_author=Byeonghu Na;,citation_author=Hyemi Kim;,citation_author=Kyungwoo Song;,citation_author=Weonyoung Joo;,citation_author=Yoon-Yeong Kim;,citation_author=Il-Chul Moon;,citation_publication_date=2020-10;,citation_cover_date=2020-10;,citation_year=2020;,citation_doi=10.1145/3340531.3411971;,citation_isbn=978-1-4503-6859-9;,citation_conference_title=Proceedings of the 29th ACM International Conference on Information &amp;amp;amp; Knowledge Management;,citation_conference=Association for Computing Machinery;,citation_series_title=CIKM ’20;">
<meta name="citation_reference" content="citation_title=Generalized linear models;,citation_author=John Ashworth Nelder;,citation_author=Robert WM Wedderburn;,citation_publication_date=1972;,citation_cover_date=1972;,citation_year=1972;,citation_issue=3;,citation_volume=135;,citation_journal_title=Journal of the Royal Statistical Society: Series A (General);,citation_publisher=Wiley Online Library;">
<meta name="citation_reference" content="citation_title=Classification with partial labels;,citation_author=Nam Nguyen;,citation_author=Rich Caruana;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_conference_title=Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining;">
<meta name="citation_reference" content="citation_title=Regularizing deep neural networks by noise: Its interpretation and optimization;,citation_author=Hyeonwoo Noh;,citation_author=Tackgeun You;,citation_author=Jonghwan Mun;,citation_author=Bohyung Han;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_volume=30;,citation_journal_title=Advances in Neural Information Processing Systems;">
<meta name="citation_reference" content="citation_title=Observatório da insolvência: Rio de Janeiro;,citation_abstract=Estudo sobre as recuperações judiciais do estado do Rio de Janeiro.;,citation_publisher=https://abj.org.br/pesquisas/obsrjrj/;">
<meta name="citation_reference" content="citation_title=Hierarchical Text-Conditional Image Generation with CLIP Latents;,citation_abstract=Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.;,citation_author=Aditya Ramesh;,citation_author=Prafulla Dhariwal;,citation_author=Alex Nichol;,citation_author=Casey Chu;,citation_author=Mark Chen;,citation_publication_date=2022-04;,citation_cover_date=2022-04;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2204.06125;,citation_doi=10.48550/arXiv.2204.06125;,citation_publisher=arXiv;">
<meta name="citation_reference" content="citation_title=Hierarchical text-conditional image generation with clip latents;,citation_author=Aditya Ramesh;,citation_author=Prafulla Dhariwal;,citation_author=Alex Nichol;,citation_author=Casey Chu;,citation_author=Mark Chen;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2204.06125;,citation_journal_title=arXiv preprint arXiv:2204.06125;">
<meta name="citation_reference" content="citation_title=Method and system for discriminating a human action from a computerized action;,citation_author=Eran Reshef;,citation_author=Gil Raanan;,citation_author=Eilon Solan;,citation_publication_date=2005-05;,citation_cover_date=2005-05;,citation_year=2005;">
<meta name="citation_reference" content="citation_title=Deep learning is robust to massive label noise;,citation_author=David Rolnick;,citation_author=Andreas Veit;,citation_author=Serge Belongie;,citation_author=Nir Shavit;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1705.10694;,citation_journal_title=arXiv preprint arXiv:1705.10694;">
<meta name="citation_reference" content="citation_title=Partially supervised learning for pattern recognition;,citation_author=Friedhelm Schwenker;,citation_author=Edmondo Trentin;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_volume=37;,citation_journal_title=Pattern Recognition Letters;">
<meta name="citation_reference" content="citation_title=Reinforcement learning: An introduction;,citation_author=Richard S. Sutton;,citation_author=Andrew G. Barto;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Tempo dos processos relacionados à adoção;,citation_abstract=Análise do tempo dos processos relacionados à adoção no Brasil, especialmente de processos relativos a desconstituição do poder familiar.;,citation_publisher=https://abj.org.br/pesquisas/adocao/;">
<meta name="citation_reference" content="citation_title=Computing machinery and intelligence;,citation_author=Alan M. Turing;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_inbook_title=Parsing the turing test;">
<meta name="citation_reference" content="citation_title=Partially supervised learning by a credal EM approach;,citation_author=Patrick Vannoorenberghe;,citation_author=Philippe Smets;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_conference_title=European Conference on Symbolic and Quantitative Approaches to Reasoning and Uncertainty;,citation_conference=Springer;">
<meta name="citation_reference" content="citation_title=Captcha: Telling humans and computers apart automatically;,citation_author=Luis Von Ahn;,citation_author=Manuel Blum;,citation_author=Nicholas Hopper;,citation_author=John Langford;,citation_publication_date=2003;,citation_cover_date=2003;,citation_year=2003;,citation_conference_title=Proceedings of eurocrypt;">
<meta name="citation_reference" content="citation_title=reCAPTCHA: Human-Based Character Recognition via Web Security Measures;,citation_abstract=CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart) are widespread security measures on the World Wide Web that prevent automated programs from abusing online services. They do so by asking humans to perform a task that computers cannot yet perform, such as deciphering distorted characters. Our research explored whether such human effort can be channeled into a useful purpose: helping to digitize old printed material by asking users to decipher scanned words from books that computerized optical character recognition failed to recognize. We showed that this method can transcribe text with a word accuracy exceeding 99%, matching the guarantee of professional human transcribers. Our apparatus is deployed in more than 40,000 Web sites and has transcribed over 440 million words.;,citation_author=Luis Ahn;,citation_author=Benjamin Maurer;,citation_author=Colin McMillen;,citation_author=David Abraham;,citation_author=Manuel Blum;,citation_publication_date=2008-09;,citation_cover_date=2008-09;,citation_year=2008;,citation_issue=5895;,citation_doi=10.1126/science.1160379;,citation_issn=0036-8075, 1095-9203;,citation_volume=321;,citation_language=en-US;,citation_journal_title=Science;">
<meta name="citation_reference" content="citation_title=Telling humans and computers apart automatically or how lazy cryptographers do AI (Tech. Rep. No. CMU-CS-02-117);,citation_author=L. Ahn;,citation_author=M. Blum;,citation_author=J. Langford;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_technical_report_institution=Carnegie Mellon University;">
<meta name="citation_reference" content="citation_title=Telling humans and computers apart automatically;,citation_author=Luis Von Ahn;,citation_author=Manuel Blum;,citation_author=John Langford;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_issue=2;,citation_volume=47;,citation_journal_title=Communications of the ACM;,citation_publisher=ACM New York, NY, USA;">
<meta name="citation_reference" content="citation_title=Make complex captchas simple: A fast text captcha solver based on a small number of samples;,citation_author=Yao Wang;,citation_author=Yuliang Wei;,citation_author=Mingjin Zhang;,citation_author=Yang Liu;,citation_author=Bailing Wang;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_volume=578;,citation_journal_title=Information Sciences;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=A survey of CAPTCHA technologies to distinguish between human and computer;,citation_author=Xin Xu;,citation_author=Lei Liu;,citation_author=Bo Li;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_volume=408;,citation_journal_title=Neurocomputing;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Understanding Sigmoid, Logistic, Softmax Functions, and Cross-Entropy Loss (Log Loss);,citation_abstract=Practical Maths for Key Concepts in Logistic Regression and Deep Learning;,citation_author=Zhou (Joe) Xu;,citation_publication_date=2022-07;,citation_cover_date=2022-07;,citation_year=2022;,citation_language=en-US;,citation_journal_title=Medium;,citation_publisher=https://towardsdatascience.com/understanding-sigmoid-logistic-softmax-functions-and-cross-entropy-loss-log-loss-dbbbe0a17efb;">
<meta name="citation_reference" content="citation_title=Deep discriminative cnn with temporal ensembling for ambiguously-labeled image classification;,citation_author=Yao Yao;,citation_author=Jiehui Deng;,citation_author=Xiuhua Chen;,citation_author=Chen Gong;,citation_author=Jianxin Wu;,citation_author=Jian Yang;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_volume=34;,citation_conference_title=Proceedings of the AAAI Conference on Artificial Intelligence;">
<meta name="citation_reference" content="citation_title=Yet another text captcha solver: A generative adversarial network based approach;,citation_author=Guixin Ye;,citation_author=Zhanyong Tang;,citation_author=Dingyi Fang;,citation_author=Zhanxing Zhu;,citation_author=Yansong Feng;,citation_author=Pengfei Xu;,citation_author=Xiaojiang Chen;,citation_author=Zheng Wang;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_conference_title=Proceedings of the 2018 ACM SIGSAC conference on computer and communications security;">
<meta name="citation_reference" content="citation_title=Adversarial examples: Attacks and defenses for deep learning;,citation_author=Xiaoyong Yuan;,citation_author=Pan He;,citation_author=Qile Zhu;,citation_author=Xiaolin Li;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=9;,citation_volume=30;,citation_journal_title=IEEE transactions on neural networks and learning systems;,citation_publisher=IEEE;">
<meta name="citation_reference" content="citation_title=Learning with biased complementary labels;,citation_author=Xiyu Yu;,citation_author=Tongliang Liu;,citation_author=Mingming Gong;,citation_author=Dacheng Tao;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_conference_title=Proceedings of the European conference on computer vision (ECCV);">
<meta name="citation_reference" content="citation_title=Neural networks incorporating unlabeled and partially-labeled data for cross-domain chinese word segmentation.;,citation_author=Lujun Zhao;,citation_author=Qi Zhang;,citation_author=Peng Wang;,citation_author=Xiaoyu Liu;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_conference_title=IJCAI;">
<meta name="citation_reference" content="citation_title=Web scraping;,citation_author=Bo Zhao;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_journal_title=Encyclopedia of big data;,citation_publisher=Springer Living ed. Cham;">
<meta name="citation_reference" content="citation_title=A brief introduction to weakly supervised learning;,citation_author=Zhi-Hua Zhou;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=1;,citation_volume=5;,citation_journal_title=National science review;,citation_publisher=Oxford University Press;">
<meta name="citation_reference" content="citation_title=A brief introduction to weakly supervised learning;,citation_author=Zhi-Hua Zhou;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=1;,citation_volume=5;,citation_journal_title=National science review;,citation_publisher=Oxford University Press;">
<meta name="citation_reference" content="citation_title=Prior-aware neural network for partially-supervised multi-organ segmentation;,citation_author=Yuyin Zhou;,citation_author=Zhe Li;,citation_author=Song Bai;,citation_author=Chong Wang;,citation_author=Xinlei Chen;,citation_author=Mei Han;,citation_author=Elliot Fishman;,citation_author=Alan L. Yuille;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_conference_title=Proceedings of the IEEE/CVF International Conference on Computer Vision;">
<meta name="citation_reference" content="citation_title=Semi-supervised learning literature survey;,citation_author=Xiaojin Jerry Zhu;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_publisher=University of Wisconsin-Madison Department of Computer Sciences;">
<meta name="citation_reference" content="citation_title=Recognizing objects in adversarial clutter: Breaking a visual CAPTCHA;,citation_author=Greg Mori;,citation_author=Jitendra Malik;,citation_publication_date=2003;,citation_cover_date=2003;,citation_year=2003;,citation_volume=1;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Recognizing objects in adversarial clutter: Breaking a visual CAPTCHA;,citation_author=Greg Mori;,citation_author=Jitendra Malik;,citation_publication_date=2003;,citation_cover_date=2003;,citation_year=2003;,citation_volume=1;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Designing human friendly human interaction proofs (HIPs);,citation_author=Kumar Chellapilla;,citation_author=Kevin Larson;,citation_author=Patrice Simard;,citation_author=Mary Czerwinski;,citation_publication_date=2005-04-02;,citation_cover_date=2005-04-02;,citation_year=2005;,citation_fulltext_html_url=https://doi.org/10.1145/1054972.1055070;,citation_doi=10.1145/1054972.1055070;,citation_conference=Association for Computing Machinery;,citation_series_title=CHI ’05;">
<meta name="citation_reference" content="citation_title=Using machine learning to break visual human interaction proofs (HIPs);,citation_author=Kumar Chellapilla;,citation_author=Patrice Simard;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_volume=17;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=Multi-digit number recognition from street view imagery using deep convolutional neural networks;,citation_author=Ian J. Goodfellow;,citation_author=Yaroslav Bulatov;,citation_author=Julian Ibarz;,citation_author=Sacha Arnoud;,citation_author=Vinay Shet;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_journal_title=arXiv preprint arXiv:1312.6082;">
<meta name="citation_reference" content="citation_title=Deep learning;,citation_author=Yann LeCun;,citation_author=Yoshua Bengio;,citation_author=Geoffrey Hinton;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_issue=7553;,citation_volume=521;,citation_journal_title=nature;">
<meta name="citation_reference" content="citation_title=Generative adversarial networks;,citation_author=Ian Goodfellow;,citation_author=Jean Pouget-Abadie;,citation_author=Mehdi Mirza;,citation_author=Bing Xu;,citation_author=David Warde-Farley;,citation_author=Sherjil Ozair;,citation_author=Aaron Courville;,citation_author=Yoshua Bengio;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=11;,citation_volume=63;,citation_journal_title=Communications of the ACM;">
<meta name="citation_reference" content="citation_title=Generative adversarial networks;,citation_author=Ian J. Goodfellow;,citation_author=Jean Pouget-Abadie;,citation_author=Mehdi Mirza;,citation_author=Bing Xu;,citation_author=David Warde-Farley;,citation_author=Sherjil Ozair;,citation_author=Aaron Courville;,citation_author=Yoshua Bengio;,citation_doi=10.48550/arXiv.1406.2661;">
<meta name="citation_reference" content="citation_title=A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs;,citation_author=Dileep George;,citation_author=Wolfgang Lehrach;,citation_author=Ken Kansky;,citation_author=Miguel Lázaro-Gredilla;,citation_author=Christopher Laan;,citation_author=Bhaskara Marthi;,citation_author=Xinghua Lou;,citation_author=Zhaoshi Meng;,citation_author=Yi Liu;,citation_author=Huayan Wang;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=6368;,citation_volume=358;,citation_journal_title=Science;">
<meta name="citation_reference" content="citation_title=Yet another text captcha solver: A generative adversarial network based approach;,citation_author=Guixin Ye;,citation_author=Zhanyong Tang;,citation_author=Dingyi Fang;,citation_author=Zhanxing Zhu;,citation_author=Yansong Feng;,citation_author=Pengfei Xu;,citation_author=Xiaojiang Chen;,citation_author=Zheng Wang;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Make complex captchas simple: A fast text captcha solver based on a small number of samples;,citation_author=Yao Wang;,citation_author=Yuliang Wei;,citation_author=Mingjin Zhang;,citation_author=Yang Liu;,citation_author=Bailing Wang;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_volume=578;,citation_journal_title=Information Sciences;">
<meta name="citation_reference" content="citation_title=A survey of CAPTCHA technologies to distinguish between human and computer;,citation_author=Xin Xu;,citation_author=Lei Liu;,citation_author=Bo Li;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_volume=408;,citation_journal_title=Neurocomputing;">
<meta name="citation_reference" content="citation_title=Adversarial examples: Attacks and defenses for deep learning;,citation_author=Xiaoyong Yuan;,citation_author=Pan He;,citation_author=Qile Zhu;,citation_author=Xiaolin Li;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=9;,citation_volume=30;,citation_journal_title=IEEE transactions on neural networks and learning systems;">
<meta name="citation_reference" content="citation_title=Regularizing deep neural networks by noise: Its interpretation and optimization;,citation_author=Hyeonwoo Noh;,citation_author=Tackgeun You;,citation_author=Jonghwan Mun;,citation_author=Bohyung Han;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_volume=30;,citation_journal_title=Advances in Neural Information Processing Systems;">
<meta name="citation_reference" content="citation_title=reCAPTCHA: Human-Based Character Recognition via Web Security Measures;,citation_author=Luis Ahn;,citation_author=Benjamin Maurer;,citation_author=Colin McMillen;,citation_author=David Abraham;,citation_author=Manuel Blum;,citation_publication_date=2008-09-12;,citation_cover_date=2008-09-12;,citation_year=2008;,citation_fulltext_html_url=https://www.science.org/doi/10.1126/science.1160379;,citation_issue=5895;,citation_doi=10.1126/science.1160379;,citation_volume=321;,citation_language=en;,citation_journal_title=Science;">
<meta name="citation_reference" content="citation_title=Reinforcement learning: An introduction;,citation_author=Richard S. Sutton;,citation_author=Andrew G. Barto;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Deep generative positive-unlabeled learning under selection bias;,citation_author=Byeonghu Na;,citation_author=Hyemi Kim;,citation_author=Kyungwoo Song;,citation_author=Weonyoung Joo;,citation_author=Yoon-Yeong Kim;,citation_author=Il-Chul Moon;,citation_publication_date=2020-10-19;,citation_cover_date=2020-10-19;,citation_year=2020;,citation_fulltext_html_url=https://doi.org/10.1145/3340531.3411971;,citation_doi=10.1145/3340531.3411971;,citation_conference=Association for Computing Machinery;,citation_series_title=CIKM ’20;">
<meta name="citation_reference" content="citation_title=Análise de sobrevivência aplicada;,citation_author=Enrico Antonio Colosimo;,citation_author=Suely Ruiz Giolo;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;">
<meta name="citation_reference" content="citation_title=A brief introduction to weakly supervised learning;,citation_author=Zhi-Hua Zhou;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=1;,citation_volume=5;,citation_journal_title=National science review;">
<meta name="citation_reference" content="citation_title=Computing machinery and intelligence;,citation_author=Alan M. Turing;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Telling humans and computers apart automatically or how lazy cryptographers do AI (tech. Rep. No. CMU-CS-02-117);,citation_author=L. Ahn;,citation_author=M. Blum;,citation_author=J. Langford;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_fulltext_html_url=http://reports-archive.adm.cs.cmu.edu/anon/2002/CMU-CS-02-117.pdf;">
<meta name="citation_reference" content="citation_title=Inaccessibility of CAPTCHA;,citation_author=undefined W3C;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://www.w3.org/TR/turingtest/;">
<meta name="citation_reference" content="citation_title=Estado brasileiro e transparência avaliando a aplicação da lei de acesso à informação;,citation_author=Gregory Michener;,citation_author=Luiz Fernando Moncau;,citation_author=Rafael Braem Velasco;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=Open data in science;,citation_author=Peter Murray-Rust;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_journal_title=Nature Precedings;">
<meta name="citation_reference" content="citation_title=Observatório da insolvência: Rio de janeiro;,citation_author=undefined ABJ;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://abj.org.br/pesquisas/obsrjrj/;">
<meta name="citation_reference" content="citation_title=Tempo dos processos relacionados à adoção;,citation_author=undefined ABJ;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_fulltext_html_url=https://abj.org.br/pesquisas/adocao/;">
<meta name="citation_reference" content="citation_title=Diagnóstico do contencioso tributário administrativo;,citation_author=undefined ABJ;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://abj.org.br/pesquisas/bid-tributario/;">
<meta name="citation_reference" content="citation_title=Diagnóstico do contencioso tributário administrativo;,citation_author=undefined ABJ;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://abj.org.br/pesquisas/bid-tributario/;">
<meta name="citation_reference" content="citation_title=Web scraping;,citation_author=Bo Zhao;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://www.researchgate.net/profile/Bo-Zhao-3/publication/317177787_Web_Scraping/links/5c293f85a6fdccfc7073192f/Web-Scraping.pdf;,citation_journal_title=Encyclopedia of big data;">
<meta name="citation_reference" content="citation_title=A brief introduction to weakly supervised learning;,citation_author=Zhi-Hua Zhou;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=1;,citation_volume=5;,citation_journal_title=National science review;">
<meta name="citation_reference" content="citation_title=Semi-supervised learning literature survey;,citation_author=Xiaojin Jerry Zhu;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;">
<meta name="citation_reference" content="citation_title=A note on learning from multiple-instance examples;,citation_author=Avrim Blum;,citation_author=Adam Kalai;,citation_publication_date=1998;,citation_cover_date=1998;,citation_year=1998;,citation_issue=1;,citation_volume=30;,citation_journal_title=Machine learning;">
<meta name="citation_reference" content="citation_title=Learning with multiple labels;,citation_author=Rong Jin;,citation_author=Zoubin Ghahramani;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_volume=15;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=Gradient-based learning applied to document recognition;,citation_author=Yann LeCun;,citation_author=Léon Bottou;,citation_author=Yoshua Bengio;,citation_author=Patrick Haffner;,citation_publication_date=1998;,citation_cover_date=1998;,citation_year=1998;,citation_issue=11;,citation_volume=86;,citation_journal_title=Proceedings of the IEEE;">
<meta name="citation_reference" content="citation_title=Generalized linear models;,citation_author=John Ashworth Nelder;,citation_author=Robert WM Wedderburn;,citation_publication_date=1972;,citation_cover_date=1972;,citation_year=1972;,citation_issue=3;,citation_volume=135;,citation_journal_title=Journal of the Royal Statistical Society: Series A (General);">
<meta name="citation_reference" content="citation_title=Hierarchical Text-Conditional Image Generation with CLIP Latents;,citation_author=Aditya Ramesh;,citation_author=Prafulla Dhariwal;,citation_author=Alex Nichol;,citation_author=Casey Chu;,citation_author=Mark Chen;,citation_publication_date=2022-04;,citation_cover_date=2022-04;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2204.06125;,citation_issue=arXiv:2204.06125;,citation_doi=10.48550/arXiv.2204.06125;,citation_publisher=arXiv;">
<meta name="citation_reference" content="citation_title=Feature engineering and selection: A practical approach for predictive models;,citation_author=Max Kuhn;,citation_author=Kjell Johnson;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;">
<meta name="citation_reference" content="citation_title=Understanding dropout;,citation_author=Pierre Baldi;,citation_author=Peter J. Sadowski;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_volume=26;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=A review on ensembles for the class imbalance problem: Bagging-, boosting-, and hybrid-based approaches;,citation_author=Mikel Galar;,citation_author=Alberto Fernandez;,citation_author=Edurne Barrenechea;,citation_author=Humberto Bustince;,citation_author=Francisco Herrera;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=4;,citation_volume=42;,citation_journal_title=IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews);">
<meta name="citation_reference" content="citation_title=Adam: A Method for Stochastic Optimization;,citation_author=Diederik P. Kingma;,citation_author=Jimmy Ba;,citation_publication_date=2017-01;,citation_cover_date=2017-01;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1412.6980;,citation_issue=arXiv:1412.6980;,citation_doi=10.48550/arXiv.1412.6980;,citation_publisher=arXiv;">
<meta name="citation_reference" content="citation_title=Efficient backprop;,citation_author=Yann A. LeCun;,citation_author=Léon Bottou;,citation_author=Genevieve B. Orr;,citation_author=Klaus-Robert Müller;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Learning from partial labels;,citation_author=Timothee Cour;,citation_author=Ben Sapp;,citation_author=Ben Taskar;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_volume=12;,citation_journal_title=The Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=Provably consistent partial-label learning;,citation_author=Lei Feng;,citation_author=Jiaqi Lv;,citation_author=Bo Han;,citation_author=Miao Xu;,citation_author=Gang Niu;,citation_author=Xin Geng;,citation_author=Bo An;,citation_author=Masashi Sugiyama;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_volume=33;,citation_journal_title=Advances in Neural Information Processing Systems;">
<meta name="citation_reference" content="citation_title=Logistic regression for partial labels;,citation_author=Yves Grandvalet;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;">
<meta name="citation_reference" content="citation_title=Learning from ambiguously labeled examples;,citation_author=Eyke Hüllermeier;,citation_author=Jürgen Beringer;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_issue=5;,citation_volume=10;,citation_journal_title=Intelligent Data Analysis;">
<meta name="citation_reference" content="citation_title=A conditional multinomial mixture model for superset label learning;,citation_author=Liping Liu;,citation_author=Thomas Dietterich;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_volume=25;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=Learning from complementary labels;,citation_author=Takashi Ishida;,citation_author=Gang Niu;,citation_author=Weihua Hu;,citation_author=Masashi Sugiyama;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_volume=30;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=Magick: Advanced graphics and image-processing in r;,citation_author=Jeroen Ooms;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://CRAN.R-project.org/package=magick;">
<meta name="citation_reference" content="citation_title=Learning with biased complementary labels;,citation_author=Xiyu Yu;,citation_author=Tongliang Liu;,citation_author=Mingming Gong;,citation_author=Dacheng Tao;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Learning with multiple complementary labels;,citation_author=Lei Feng;,citation_author=Takuo Kaneko;,citation_author=Bo Han;,citation_author=Gang Niu;,citation_author=Bo An;,citation_author=Masashi Sugiyama;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=xml2: Parse XML;,citation_author=Hadley Wickham;,citation_author=Jim Hester;,citation_author=Jeroen Ooms;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://CRAN.R-project.org/package=xml2;">
<meta name="citation_reference" content="citation_title=Stringr: Simple, consistent wrappers for common string operations;,citation_author=Hadley Wickham;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=stringr;">
<meta name="citation_reference" content="citation_title=Rvest: Easily harvest (scrape) web pages;,citation_author=Hadley Wickham;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=rvest;">
<meta name="citation_reference" content="citation_title=Captcha and its techniques: A review;,citation_author=Kiranjot Kaur;,citation_author=Sunny Behal;,citation_publication_date=2014-01-01;,citation_cover_date=2014-01-01;,citation_year=2014;,citation_volume=5;,citation_journal_title=International Journal of Computer Science and Information Technologies,;">
<meta name="citation_reference" content="citation_title=Multivariate binomial/multinomial control chart;,citation_author=Jian Li;,citation_author=Fugee Tsung;,citation_author=Changliang Zou;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_issue=5;,citation_volume=46;,citation_journal_title=IIE Transactions;">
<meta name="citation_reference" content="citation_title=Batch normalization: Accelerating deep network training by reducing internal covariate shift;,citation_author=Sergey Ioffe;,citation_author=Christian Szegedy;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=Torch: Tensors and neural networks with ’GPU’ acceleration;,citation_author=Daniel Falbel;,citation_author=Javier Luraschi;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=torch;">
<meta name="citation_reference" content="citation_title=Luz: Higher level ’API’ for ’torch’;,citation_author=Daniel Falbel;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=luz;">
<meta name="citation_reference" content="citation_title=Torchvision: Models, datasets and transformations for images;,citation_author=Daniel Falbel;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=torchvision;">
<meta name="citation_reference" content="citation_title=R: A language and environment for statistical computing;,citation_author=R Core Team;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://www.R-project.org/;">
<meta name="citation_reference" content="citation_title=Feature engineering and selection: A practical approach for predictive models;,citation_author=Max Kuhn;,citation_author=Kjell Johnson;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;">
<meta name="citation_reference" content="citation_title=Tensorflow: R interface to ’TensorFlow’;,citation_author=JJ Allaire;,citation_author=Yuan Tang;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=tensorflow;">
<meta name="citation_reference" content="citation_title=Decryptr: An extensible API for breaking captchas;,citation_author=Julio Trecenti;,citation_author=Caio Lente;,citation_author=Daniel Falbel;,citation_author=Milene Farhat;,citation_author=Beatriz Vianna;,citation_author=Evelin Angelica;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Reticulate: Interface to ’python’;,citation_author=Kevin Ushey;,citation_author=JJ Allaire;,citation_author=Yuan Tang;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=reticulate;">
<meta name="citation_reference" content="citation_title=Piggyback: Managing larger data on a GitHub repository;,citation_author=Carl Boettiger;,citation_author=Tan Ho;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=piggyback;">
<meta name="citation_reference" content="citation_title=Usethis: Automate package and project setup;,citation_author=Hadley Wickham;,citation_author=Jennifer Bryan;,citation_author=Malcolm Barrett;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=usethis;">
<meta name="citation_reference" content="citation_title=Hierarchical Text-Conditional Image Generation with CLIP Latents;,citation_author=Aditya Ramesh;,citation_author=Prafulla Dhariwal;,citation_author=Alex Nichol;,citation_author=Casey Chu;,citation_author=Mark Chen;,citation_publication_date=2022-04;,citation_cover_date=2022-04;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2204.06125;,citation_issue=arXiv:2204.06125;,citation_doi=10.48550/arXiv.2204.06125;,citation_publisher=arXiv;">
<meta name="citation_reference" content="citation_title=AVA: A large-scale database for aesthetic visual analysis;,citation_author=Naila Murray;,citation_author=Luca Marchesotti;,citation_author=Florent Perronnin;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_conference=IEEE;">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introdução</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Resolvendo Captchas</a> 
        <div class="sidebar-tools-main">
    <a href="./Resolvendo-Captchas.pdf" title="Download PDF" class="sidebar-tool px-1"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle sidebar-tool" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Sobre este documento</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introducao.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introdução</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./metodologia.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Metodologia</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./resultados.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Resultados</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./conclusoes.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Conclusões</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliografia.html" class="sidebar-item-text sidebar-link">Bibliografia</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Apêndices</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pacote.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Pacotes</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Índice</h2>
   
  <ul>
  <li><a href="#sec-captchas-publicos" id="toc-sec-captchas-publicos" class="nav-link active" data-scroll-target="#sec-captchas-publicos">Captchas em serviços públicos</a></li>
  <li><a href="#sec-historia" id="toc-sec-historia" class="nav-link" data-scroll-target="#sec-historia">Uma luta entre geradores e resolvedores</a></li>
  <li><a href="#sec-intro-oraculo" id="toc-sec-intro-oraculo" class="nav-link" data-scroll-target="#sec-intro-oraculo">Oráculo</a></li>
  <li><a href="#sec-objetivos" id="toc-sec-objetivos" class="nav-link" data-scroll-target="#sec-objetivos">Objetivo</a></li>
  <li><a href="#sec-justificativa" id="toc-sec-justificativa" class="nav-link" data-scroll-target="#sec-justificativa">Justificativa</a></li>
  <li><a href="#sec-hipoteses" id="toc-sec-hipoteses" class="nav-link" data-scroll-target="#sec-hipoteses">Hipóteses</a></li>
  <li><a href="#sec-organizacao" id="toc-sec-organizacao" class="nav-link" data-scroll-target="#sec-organizacao">Organização do trabalho</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-introducao" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introdução</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div>
<blockquote class="blockquote">
<p>Why do we need to prove we’re not robots to a robot? Isn’t that a robot’s job?</p>
<p>– ChatGPT, a robot</p>
</blockquote>
</div>
<p>Captcha (<em>Completely Automated Public Turing test to tell Computers and Humans Apart</em>) é um desafio utilizado para identificar se o acesso à uma página na internet é realizada por uma pessoa ou um robô<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. O desafio é projetado para ser fácil de resolver por humanos, mas difícil de resolver por máquinas. Outro nome para os Captchas é <em>Human Interaction Proofs,</em> ou HIPs <span class="citation" data-cites="chellapilla2005">(<a href="bibliografia.html#ref-chellapilla2005" role="doc-biblioref">CHELLAPILLA et al., 2005</a>)</span>.</p>
<p>Um Captcha pode ser classificado como uma variação do teste de Turing <span class="citation" data-cites="turing2009">(<a href="bibliografia.html#ref-turing2009" role="doc-biblioref">TURING, 2009</a>)</span>. A diferença no caso do Captcha é que a avaliação da humanidade do agente é feita por um robô ao invés de uma pessoa – por isso o termo <em>automated</em>. Em algumas situações, Captchas também podem ser entendidos como <strong>testes de Turing reversos</strong>, apesar dos autores originais afastarem essa caracterização <span class="citation" data-cites="vonahnTellingHumansComputers2002">(<a href="bibliografia.html#ref-vonahnTellingHumansComputers2002" role="doc-biblioref">AHN; BLUM; LANGFORD, 2002</a>)</span>.</p>
<p>A tarefa de resolver Captchas também pode ser pensada como uma variação do reconhecimento óptico de caracteres (<em>Optical Character Recognition</em>, OCR). No entanto, os Captchas objetivam enganar justamente as ferramentas de OCR, através de distorções aplicadas às imagens. Como efeito, tais ferramentas costumam apresentar baixo poder preditivo nesse desafio.</p>
<p>Captchas estão presentes em toda a internet. Inicialmente criados para prevenir <em>spam</em> (<em>Sending and Posting Advertisement in Mass</em>), os desafios se tornaram populares rapidamente <span class="citation" data-cites="vonahnReCAPTCHAHumanBasedCharacter2008">(<a href="bibliografia.html#ref-vonahnReCAPTCHAHumanBasedCharacter2008" role="doc-biblioref">AHN et al., 2008</a>)</span>, sendo utilizados como forma de evitar o uso indevido de aplicações da <em>web</em>. Algumas ações que os desafios podem ajudar a evitar são:</p>
<ul>
<li>Criação de contas falsas nos sites.</li>
<li>Envio automático de mensagens, via <em>email</em> ou formulários de contato.</li>
<li>Operações automatizadas, como compra de ingressos para eventos e voto automático em sites de votação.</li>
<li>Consulta automatizada em sites para obtenção de dados.</li>
</ul>
<p>Por princípio, o uso de Captchas tem como objetivos aumentar a segurança das pessoas que acessam a internet e proteger os sistemas <em>web</em> de uso abusivo. Para pessoas que acessam os sites pontualmente, a presença de Captchas representa um mero dissabor; para quem realiza acessos massivos, uma grande dificuldade.</p>
<p>No entanto, o uso de Captchas não é adequado em todas as situações. Um exemplo são os sites de vendas: o uso dos desafios pode aborrecer usuários, reduzindo a qualidade da experiência ao consumir nesses sites. Os sites devem levar esse fator de aborrecimento em conta para não reduzir a taxa de conversão. Em alguns casos, pode fazer mais sentido abandonar os Captchas e utilizar outros mecanismos de prevenção à fraude, como monitoramento da sessão do usuário <span class="citation" data-cites="inaccess">(<a href="bibliografia.html#ref-inaccess" role="doc-biblioref">W3C, 2021</a>)</span>.</p>
<p>Também existem casos em que o uso de Captchas é prejudicial. Por exemplo, sua utilização em serviços públicos do Brasil é problemática. Para explicar esse problema, no entanto, é necessário descrever o contexto jurídico e como pesquisas aplicadas podem ser prejudicadas com o uso de Captchas.</p>
<section id="sec-captchas-publicos" class="level2">
<h2 class="anchored" data-anchor-id="sec-captchas-publicos">Captchas em serviços públicos</h2>
<p>A Constituição Federal de 1988 (CF), em seu <a href="https://constituicao.stf.jus.br/dispositivo/cf-88-parte-1-titulo-2-capitulo-1-artigo-5-inciso-33">inciso XXXIII do art. 5º</a>, prevê que “todos têm direito a receber dos órgãos públicos informações de seu interesse particular, ou de interesse coletivo ou geral, que serão prestadas no prazo da lei, sob pena de responsabilidade, ressalvadas aquelas cujo sigilo seja imprescindível à segurança da sociedade e do Estado;”. Essa previsão é implementada pela Lei de Acesso à Informação (<a href="https://www.planalto.gov.br/ccivil_03/_ato2011-2014/2011/lei/l12527.htm">Lei 12.527/2011</a>; LAI), que se aplica “aos órgãos públicos integrantes da administração direta dos Poderes Executivo, Legislativo, incluindo as Cortes de Contas, e Judiciário e do Ministério Público”, bem como “as autarquias, as fundações públicas, as empresas públicas, as sociedades de economia mista e demais entidades controladas direta ou indiretamente pela União, Estados, Distrito Federal e Municípios” (Art. 1º).</p>
<p>A LAI, apesar de trazer diversos benefícios à sociedade, tem dois problemas. O primeiro é o <strong>esforço</strong>: tanto a pessoa/órgão que solicita os dados, quanto o órgão que retorna os dados precisam trabalhar para disponibilizar as informações, sendo necessário deslocar equipes para realizar os levantamentos pedidos. O segundo é o <strong>formato</strong>: os dados enviados como resultado de pedidos de LAI podem chegar em formatos inadequados para consumo da solicitante, muitas vezes em <em>Portable Document Format</em> (PDF), que dificulta a leitura e análise dos dados <span class="citation" data-cites="michener2015">(<a href="bibliografia.html#ref-michener2015" role="doc-biblioref">MICHENER; MONCAU; VELASCO, 2015</a>, pág. 55)</span>; além disso, como o levantamento é realizado de forma individualizada, o mesmo pedido feito em diferentes períodos (e.g.&nbsp;uma atualização mensal dos dados) pode vir em formatos diferentes, dificultando a leitura e arrumação dos dados.</p>
<p>Uma forma eficiente de evitar os problemas de esforço e formato em pedidos de LAI é disponibilizar os dados de <strong>forma aberta</strong>. Como definido pela <em>Open Knowledge Foundation</em> (OKFN), a base de dados “deve ser fornecida em uma forma conveniente e modificável, isenta de obstáculos tecnológicos desnecessários para a realização dos direitos licenciados. Especificamente, os dados devem ser legíveis por máquina, disponíveis em todo o seu volume, e fornecidos em um formato aberto (ou seja, um formato com sua especificação livremente disponível, e publicada sem quaisquer restrições, monetárias ou não, da sua utilização) ou, no mínimo, podem ser processados com pelo menos uma ferramenta de software livre e gratuita.”<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>As vantagens ao disponibilizar dados públicos de forma aberta para a sociedade é um tema pacífico na comunidade científica <span class="citation" data-cites="murray-rust2008">(<a href="bibliografia.html#ref-murray-rust2008" role="doc-biblioref">MURRAY-RUST, 2008</a>)</span>. No Brasil, existem plataformas dedicadas à abertura de dados governamentais, como o <a href="https://dados.gov.br">dados.gov.br</a>. No entanto, existem diversos dados públicos que ainda não estão disponíveis de forma aberta.</p>
<p>A dificuldade de acesso é particularmente evidente no Poder Judiciário, que além de não disponibilizar um portal de dados abertos, impõe barreiras aos pedidos de acesso à informação por utilizar diversos sistemas para armazenar os dados. Por exemplo, para pedir uma lista de todos os processos judiciais relacionados à recuperação judicial de empresas, as únicas alternativas são i) pedir os dados ao Conselho Nacional de Justiça (CNJ), que não possui informações suficientes para obter a lista<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> ou ii) expedir ofícios aos 27 Tribunais Estaduais. Cada tribunal apresentaria diferentes opções e critérios de acesso aos dados, diferentes prazos para atendimento e diferentes formatos, podendo, inclusive, negar o pedido de acesso.</p>
<p>A dificuldade para acessar os dados do judiciário é a principal barreira para realização de pesquisas pela Associação Brasileira de Jurimetria (ABJ), empresa na qual o autor desta tese trabalha. A entidade tem como missão principal realizar estudos empíricos para implementar políticas públicas utilizando dados do judiciário.</p>
<p>Dos 16 projetos disponibilizados na <a href="https://abj.org.br/pesquisas/">página de pesquisas no site da ABJ</a>, pelo menos 12 (75%) apresentaram dificuldades na obtenção dos dados via pedidos de acesso aos órgãos. Três exemplos emblemáticos são o da pesquisa sobre Tempo dos processos relacionados à adoção no Brasil <span class="citation" data-cites="tempodo">(<a href="bibliografia.html#ref-tempodo" role="doc-biblioref">ABJ, 2014</a>)</span>, o Observatório da Insolvência: Rio de Janeiro <span class="citation" data-cites="observat">(<a href="bibliografia.html#ref-observat" role="doc-biblioref">ABJ, 2021</a>)</span> e o Diagnóstico do Contencioso Tributário Administrativo <span class="citation" data-cites="diagnosticoABJ">(<a href="bibliografia.html#ref-diagnosticoABJ" role="doc-biblioref">ABJ, 2022</a>)</span>. No primeiro caso, dois tribunais enviaram os dados em arquivos em papel, sendo que um deles ultrapassou mil páginas com números de processos impressos. No segundo caso, o pedido foi respondido com uma planilha de contagens ao invés da lista de processos. No último caso, até mesmo órgãos que faziam parte do grupo de trabalho da pesquisa negaram pedido de acesso a dados de processos tributários em primeira instância, com argumentos que variavam desde a dificuldade técnica de levantar os dados até a Lei Geral de Proteção de Dados (LGPD).</p>
<p>Em muitas situações a única alternativa para realizar as pesquisas é acessando os dados via coleta automatizada nos sites. Todos os tribunais possuem ferramentas de consulta individualizadas de processos, por conta do que está previsto na CF. A solução, portanto, passa a ser construir uma ferramenta que obtém todos os dados automaticamente. Tal conceito é conhecido como <em>raspagem de dados</em> <span class="citation" data-cites="zhao2017">(<a href="bibliografia.html#ref-zhao2017" role="doc-biblioref">ZHAO, 2017</a>)</span> e será desenvolvido com maiores detalhes no <a href="metodologia.html"><span>Capítulo&nbsp;2</span></a>.</p>
<p>Os Captchas se tornam prejudiciais à sociedade quando o acesso automatizado é necessário para realizar pesquisas científicas. Infelizmente, vários tribunais utilizam a barreira do Captcha. Alguns tribunais, inclusive, têm o entendimento de que o acesso automatizado é prejudicial, como o Tribunal de Justiça do Rio Grande do Sul (TJRS), que emitiu um <a href="https://www.tjrs.jus.br/novo/processos-e-servicos/processo-eletronico/acesso-robotizado-a-dados-publicos-e-duplamente-arriscado/">comunicado</a> sobre o tema.</p>
<p>Uma justificativa comum para implementar Captchas em consultas públicas é a estabilidade dos sistemas. Ao realizar muitas consultas de forma automática, um robô que faz consultas automatizadas pode tornar o sistema instável e, em algumas situações, até mesmo derrubar o servidor ou banco de dados que disponibiliza as consultas.</p>
<p>O problema é que utilizar Captchas não impede o acesso automatizado. As empresas que fazem acesso automatizado em tribunais podem construir ferramentas ou utilizar serviços externos de resolução de Captchas. Ou seja, ao utilizar Captchas, o acesso não é impedido, apenas especializado.</p>
<p>Utilizar Captchas também é uma solução ineficiente. Do ponto de vista técnico, a solução mais eficiente para disponibilizar os dados é através de ferramentas de dados abertos como o <a href="https://ckan.org/"><em>Comprehensive Knowledge Archive Network</em> (CKAN)</a>. Ao disponibilizar os dados de forma aberta, as consultas automatizadas ficariam isoladas dos sites de consulta pública, o que garantiria o acesso das pessoas sem problemas de indisponibilidade.</p>
<p>Não é só quem faz pesquisa com dados públicos que o uso de Captchas pode ser prejudicial. No mercado, existem serviços de resolução de Captchas que utilizam mão de obra humana, em regimes que pagam muito menos do que um salário-mínimo a 8 horas de trabalho. Um exemplo é o 2Captcha<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>, que funciona como um Uber dos Captchas: o algoritmo automatizado envia o Captcha para a plataforma, que é acessado e resolvido por uma pessoa, retornando a solução para o algoritmo. O 2Captcha é operado pela ALTWEB LLC-FZ, uma empresa com base em Dubai<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<p>Segundo o site, o valor pago pelo 2Captcha é de US$ 0,5 para 1 a 2 horas de trabalho. No regime da Consolidação das Leis do Trabalho (<a href="https://www.planalto.gov.br/ccivil_03/decreto-lei/del5452.htm">Decreto-Lei 5.452/1943</a>, CLT) as horas mensais de trabalho são 220. Trabalhando continuamente no 2Captcha, isso daria um salário de 55 a 110 dólares por mês, valor bem abaixo do salário-mínimo do Brasil, que no ano de 2022 era de R$ 1.100,00<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>, mesmo considerando os valores mais altos de taxa de câmbio. Ou seja, os serviços públicos acabam, indiretamente, incentivando um mercado que paga abaixo do salário-mínimo. Luis von Ahn, um dos criadores dos Captchas, define o 2Captcha como um <em>sweatshop</em>, um termo utilizado para caracterizar empresas que têm condições de trabalho inaceitáveis.</p>
<p>A solução definitiva para os problemas gerados pelos Captchas é a disponibilização dos dados públicos de forma aberta. Na ausência dessa solução, seja por falta de interesse ou iniciativa dos órgãos públicos, a alternativa é desenvolver uma solução para resolver Captchas que seja gratuita e aberta. Tal solução desincentivaria economicamente o uso de sistemas como o 2Captcha, protegendo as pessoas que fazem as resoluções e auxiliando pesquisadores em seus estudos.</p>
<p>O presente trabalho busca avançar nesse sentido. A solução desenvolvida envolve um modelo que resolve alguns Captchas automaticamente, reduzindo significativamente a necessidade de anotação manual.</p>
<p>Para compreender completamente o avanço que a tese representa, no entanto, é necessário apresentar o histórico de desenvolvimento dos Captchas. A descrição é feita através de uma luta entre geradores e resolvedores de Captchas, que pode ser dada como encerrada no ano de 2018, com o advento do <em>reCaptcha v3</em>.</p>
</section>
<section id="sec-historia" class="level2">
<h2 class="anchored" data-anchor-id="sec-historia">Uma luta entre geradores e resolvedores</h2>
<p>O primeiro texto técnico sobre Captchas foi publicado por <span class="citation" data-cites="vonahnTellingHumansComputers2002">AHN; BLUM; LANGFORD (<a href="bibliografia.html#ref-vonahnTellingHumansComputers2002" role="doc-biblioref">2002</a>)</span>. O texto apresenta o Captcha e seu significado através do problema de geração de <em>emails</em> automáticos no Yahoo. Em seguida, apresenta alguns exemplos de candidatos a Captcha, com tarefas de reconhecimento de padrões ou textos. Uma característica interessante dos autores sobre o Captcha é que suas imagens devem ser disponíveis publicamente. O texto também faz a conexão entre a tarefa dos Captchas e os desafios da inteligência artificial. Um ponto a destacar é que os autores incentivam pesquisas para resolver Captchas, pois isso implica em avanços na inteligência artificial. O site original do projeto, <a href="https://web.archive.org/web/20010723114217/http://www.captcha.net/"><em>The Captcha Project</em></a>, foi lançado em 2000.</p>
<p>O relatório técnico de <span class="citation" data-cites="vonahnTellingHumansComputers2002">AHN; BLUM; LANGFORD (<a href="bibliografia.html#ref-vonahnTellingHumansComputers2002" role="doc-biblioref">2002</a>)</span> não foi o primeiro a apresentar o nome Captcha, nem suas aplicações. <span class="citation" data-cites="reshefMethodSystemDiscriminating2005">RESHEF; RAANAN; SOLAN (<a href="bibliografia.html#ref-reshefMethodSystemDiscriminating2005" role="doc-biblioref">2005</a>)</span> foi o primeiro registro de patente com o termo e <span class="citation" data-cites="lillibridgeMethodSelectivelyRestricting2001">LILLIBRIDGE et al. (<a href="bibliografia.html#ref-lillibridgeMethodSelectivelyRestricting2001" role="doc-biblioref">2001</a>)</span> foi o primeiro registro de patente que implementou uma solução aos sistemas de Captchas. No entanto, o relatório técnico de 2002 é o primeiro que reconhecidamente trata do tema como um problema de inteligência artificial.</p>
<p>Os artigos mais conhecidos de introdução aos Captchas são <span class="citation" data-cites="vonahnCaptchaTellingHumans2003">VON AHN et al. (<a href="bibliografia.html#ref-vonahnCaptchaTellingHumans2003" role="doc-biblioref">2003</a>)</span> e <span class="citation" data-cites="vonahnTellingHumansComputers2004">VON AHN; BLUM; LANGFORD (<a href="bibliografia.html#ref-vonahnTellingHumansComputers2004" role="doc-biblioref">2004</a>)</span>. O conteúdo dos trabalhos é o mesmo, sendo o primeiro deles na forma de apresentação e o segundo na forma de relatório. Um detalhe interessante é a ênfase dos autores no termo <em>Public</em> dos Captchas, mostrando a preocupação em manter os códigos públicos.</p>
<p>Os autores também defendem que o Captcha é uma forma de fazer com que pessoas mal-intencionadas contribuam com os avanços da inteligência artificial. Se uma pessoa (ainda que mal-intencionada) resolve um Captcha e publica essa solução, isso significa que a comunidade científica avançou na área de inteligência artificial.</p>
<p>Não demorou para surgirem os primeiros resolvedores de Captchas<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. <span class="citation" data-cites="mori2003a">MORI; MALIK (<a href="bibliografia.html#ref-mori2003a" role="doc-biblioref">2003</a>)</span> foi um dos primeiros trabalhos publicados sobre o tema e utiliza diversas técnicas de processamento de imagens para obter os rótulos corretos. Também não demorou para a comunidade científica perceber que redes neurais eram úteis nesse contexto <span class="citation" data-cites="chellapilla2004">(<a href="bibliografia.html#ref-chellapilla2004" role="doc-biblioref">CHELLAPILLA; SIMARD, 2004</a>)</span>. No artigo de 2004, Chellapilla e Simard desenvolvem um algoritmo baseado em heurísticas para segmentar a imagem e redes neurais para identificar as imagens individuais.</p>
<p>A partir desse ponto, foi iniciada uma luta entre geradores e resolvedores de Captchas. Do lado dos geradores, as pessoas envolvidas foram tanto acadêmicos tentando desenvolver desafios cada vez mais difíceis para avançar na pesquisa em inteligência artificial, quanto empresas de tecnologia tentando se proteger contra robôs sofisticados. Do lado dos resolvedores, as pessoas envolvidas foram tanto acadêmicos tentando desenvolver novas técnicas para avançar nos modelos de reconhecimento de imagens, quanto <em>spammers</em> buscando novas formas de realizar ataques cibernéticos.</p>
<p>Uma das pessoas envolvidas com geradores de Captchas mais conhecidas é Luis von Ahn, um dos criadores do artigo original do Captcha. Um pedaço da história dos Captchas está disponível nos primeiros cinco minutos de sua entrevista em um programa britânico chamado <em>Spark</em><a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. Na entrevista, Von Ahn conta um pouco sobre origem dos Captchas em Carnegie Mellon, contando que ficou frustrado com o fato de as pessoas perderem tempo de inteligência humana ao resolver Captchas, o que deu origem ao reCaptcha. Outro vídeo instrutivo é uma palestra de Von Ahn na <em>Thinking Digital Conference</em> sobre a história do reCaptcha<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>. Segundo ele, a <em>startup</em> foi criada em maio de 2007<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>, depois de Von Ahn verificar que aproximadamente 200 milhões de Captchas eram resolvidos diariamente.</p>
<p>O reCaptcha v1 aproveitou o tempo das pessoas que resolvem Captchas para digitalizar livros <span class="citation" data-cites="vonahnReCAPTCHAHumanBasedCharacter2008">(<a href="bibliografia.html#ref-vonahnReCAPTCHAHumanBasedCharacter2008" role="doc-biblioref">AHN et al., 2008</a>)</span>. A ideia do reCaptcha, como demonstrada na <a href="#fig-recaptcha-v1">Figura&nbsp;<span>1.1</span></a>, foi apresentar duas palavras distorcidas para a pessoa. Imagine que uma ferramenta de OCR (<em>Optical Character Recognition</em>) está digitalizando a página de um livro que foi escaneada de um documento físico, ou seja, está transformando a foto da página do livro em um arquivo de texto. Na Figura, “upon” seria uma palavra que a ferramenta de OCR conseguiu transformar a imagem em texto com sucesso, enquanto “between” seria uma palavra em que a ferramenta de OCR falhou. A primeira palavra seria utilizada para verificar se o agente era ou não humano, e a segunda seria utilizada para decifrar a palavra e aprimorar as ferramentas de OCR. Em 2009, a empresa foi comprada pela Google, que utilizou o reCaptcha v1 para digitalizar os livros que estão presentes no site <a href="https://books.google.com.br/">Google Books</a>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-recaptcha-v1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assets/img/recaptcha-v1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;1.1: Explicação de von Ahn sobre o funcionamento do reCaptcha</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Curiosamente, foi com a própria Google que os resolvedores ficaram em vantagem na luta contra geradores. Os modelos de inteligência artificial continuaram avançando, notadamente com o avanço dos modelos de redes neurais profundas <span class="citation" data-cites="lecun2015">(<a href="bibliografia.html#ref-lecun2015" role="doc-biblioref">LECUN; BENGIO; HINTON, 2015</a>)</span>. No trabalho de <span class="citation" data-cites="goodfellow2013">GOODFELLOW et al. (<a href="bibliografia.html#ref-goodfellow2013" role="doc-biblioref">2013</a>)</span>, foi apresentado um modelo de redes neurais convolucionais que resolvia o reCaptcha v1 com 99,8% de acurácia. No ano seguinte, em 2014, a Google descontinuou o reCaptcha v1, lançando o reCaptcha v2.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<p>O reCaptcha v2 apresentou duas inovações importantes. O primeiro foi o botão “<em>I’m not a robot</em>”, um verificador automático do navegador que utiliza heurísticas para detectar se o padrão de acesso ao site se assemelha com um robô ou humano. O segundo foi a mudança no tipo de tarefa: ao invés de rotular um texto distorcido, o desafio passou a ser identificar objetos e animais, como na <a href="#fig-turkey">Figura&nbsp;<span>1.2</span></a>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-turkey" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assets/img/turkey.png" class="img-fluid figure-img" style="width:40.0%"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;1.2: Exemplo do reCaptcha v2 com a imagens de perus</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>A mudança do tipo de tarefa de visão computacional foi importante para o sucesso do reCaptcha v2. O desafio é mais difícil, já que existem muito mais objetos e imagens do que letras e números, aumentando significativamente o suporte da variável resposta. Por exemplo, um modelo para identificar perus pode ser facilmente desenvolvido a partir de uma base anotada, potencialmente custosa para ser construída. O reCaptcha v2, no entanto, pode facilmente mudar a tarefa para identificar leões, cães, hidrantes e semáforos, inutilizando o modelo criado para classificar perus.</p>
<p>O reCaptcha v2 também foi usado para gerar novas bases de dados para a Google, usando o mesmo princípio do reCaptcha v1. A humanidade do agente é verificada apenas com uma parte das imagens. As outras imagens eram utilizadas para anotar imagens, utilizadas para aprimorar modelos utilizados nos projetos de carros autônomos, Google Street View e outras iniciativas da empresa.</p>
<p>Com o advento do reCaptcha v2, a pergunta de interesse dos resolvedores de Captcha passou a ser: como criar modelos que funcionam razoavelmente bem sem a necessidade de anotar muitas imagens? Se esse desafio fosse resolvido, dois avanços aconteceriam: i) um grande avanço na inteligência artificial, especificamente na área de visão computacional, e ii) uma nova forma de vencer a luta de geradores e resolvedores.</p>
<p>Até o momento de escrita da tese, não existia um modelo geral que resolvesse com alta acurácia todos os desafios colocados pelo reCaptcha v2 e seus concorrentes. No entanto, vários avanços apareceram no sentido de reduzir a quantidade de imagens anotadas para criar candidatos a resolvedores. Dentre eles, os mais significativos são os baseados nas redes generativas adversariais (<em>Generative Adversarial Networks,</em> ou GANs), propostas no famoso trabalho de <span class="citation" data-cites="goodfellowGenerativeAdversarialNetworks2014">GOODFELLOW et al. (<a href="bibliografia.html#ref-goodfellowGenerativeAdversarialNetworks2014" role="doc-biblioref">2014</a>)</span>. O primeiro trabalho que utiliza modelos generativos no contexto de Captchas mostrou uma redução de 300x na quantidade de dados anotados necessários para resolver um Captcha <span class="citation" data-cites="george2017">(<a href="bibliografia.html#ref-george2017" role="doc-biblioref">GEORGE et al., 2017</a>)</span>. Nesse caso, os autores propõem uma rede diferente do GAN, chamada <em>Recursive Cortical Network</em>, ou RCN. Outros trabalhos mais recentes <span class="citation" data-cites="ye2018 wang2021">(<a href="bibliografia.html#ref-wang2021" role="doc-biblioref">WANG et al., 2021</a>; <a href="bibliografia.html#ref-ye2018" role="doc-biblioref">YE et al., 2018</a>)</span> avançam ainda mais na pesquisa, reduzindo o trabalho de classificação para um novo Captcha de texto para aproximadamente 2 horas.</p>
<p>Mas foi em 2018, com o reCaptcha v3, que a Google fez um passo definitivo. Com a nova versão, as verificações do navegador <em>web</em> passaram a ser muito mais poderosas, sendo raros os casos em que o site fica em dúvidas se a pessoa é ou não um robô. Versões mais recentes, como o reCaptcha <em>Enterprise</em>, de 2020, ainda permitem que as mantenedoras dos sites façam o ajuste fino de modelos de detecção de robôs. Dessa forma, desafios de reconhecimento de texto e objetos em imagens perderam a importância.</p>
<p>Então, no final, quem venceu a luta de geradores e resolvedores? Na verdade, nenhuma das duas! O que ocorreu com o reCaptcha v3 e seus sucessores foi, no fundo, uma mudança de perspectiva: o Captcha deixou de ser um sistema <strong>passivo</strong> e passou a ser um sistema <strong>ativo</strong> de verificação do agente. Ao invés de criar uma tarefa difícil de resolver por robôs e fácil de resolver por pessoas, os sistemas criaram uma camada de verificação da sessão de acesso do usuário, incluindo análises do navegador, dos <em>cookies</em> e dos padrões de cliques. Antes mesmo de chegar no desafio de reconhecimento, o algoritmo de acesso precisa enganar os verificadores. Essa tarefa é muito mais parecida com um problema de <em>cyberataque</em> do que uma tarefa de inteligência artificial.</p>
<p>A luta entre sites e <em>spammers</em> continua, mas não é mais uma luta entre geradores e resolvedores. Por conta disso, os desafios dos Captchas, sejam de texto ou de imagem, são hoje muito mais uma questão acadêmica do que uma questão de segurança. A pesquisa sobre Captchas ainda é promissora e pode gerar muitos resultados importantes para a área de inteligência artificial.</p>
<p>Apesar dos avanços do reCaptcha v3, Captchas de textos em imagens continuam sendo populares na <em>internet</em>. Isso é especialmente evidente nos serviços públicos – objeto deste trabalho –, já que os serviços raramente são atualizados com ferramentas mais recentes. Desenvolver uma ferramenta que facilita a resolução de Captchas em sites públicos é uma forma de incentivar os sites a serem atualizados, disponibilizando os dados públicos de forma mais eficiente.</p>
<p>Desenvolver e disponibilizar novos métodos para resolução de Captchas de textos em imagens pode ter um impacto positivo na transparência dos serviços públicos. Essa é a lacuna identificada a partir da observação do estado atual dos serviços públicos e dos trabalhos acadêmicos analisados.</p>
<p>A pesquisa apresenta um fluxo de trabalho que pode ser facilmente aplicado a diferentes modelos de resolução de Captchas, incluindo arquiteturas que ainda não foram desenvolvidas. O fluxo de trabalho funcionará como um acelerador do aprendizado, possibilitando a criação de modelos que não precisam de intervenção humana.</p>
<p>O resultado será encontrado explorando o potencial de uso do <strong>oráculo</strong>, disponível em todos os Captchas de textos em imagens. Para definir e contextualizar o uso do oráculo, no entanto, é necessário apresentar algumas características sobre o problema estudado.</p>
</section>
<section id="sec-intro-oraculo" class="level2">
<h2 class="anchored" data-anchor-id="sec-intro-oraculo">Oráculo</h2>
<p>Modelos de aprendizagem profunda usuais podem ser sensíveis a perturbações pequenas nas imagens <span class="citation" data-cites="yuan2019">(<a href="bibliografia.html#ref-yuan2019" role="doc-biblioref">YUAN et al., 2019</a>)</span>. Por isso, para resolver o Captcha de um tribunal, um modelo que resolve o Captcha de outro tribunal pode não ser eficaz, sendo necessário baixar e anotar uma nova base e treinar um novo modelo.</p>
<p>Avanços em técnicas de regularização fazem com que o modelo seja menos afetado por mudanças nos desafios gerados. Uma técnica de regularização que ajuda na capacidade de generalização é a aumentação de dados com adição de ruídos <span class="citation" data-cites="noh2017">(<a href="bibliografia.html#ref-noh2017" role="doc-biblioref">NOH et al., 2017</a>)</span>. No entanto, nenhuma técnica garante que o modelo terá excelentes resultados em novos desafios.</p>
<p>Uma alternativa é desenvolver modelos que aprendem com poucos dados anotados. Como comentado anteriormente, GANs e modelos relacionados podem apresentar bons resultados na resolução de tarefas de imagens, mesmo com uma base de dados pequena. Nesse sentido, ainda que um site mude seu Captcha, é possível ajustar um modelo que resolve esse Captcha sem a necessidade de anotar muitos exemplos para construir uma nova base de treino.</p>
<p>Nessa tese, apresenta-se uma nova técnica para resolver Captchas com poucas ou nenhuma imagem anotada, chamada <em>Web Automatic Weak Learning</em> (WAWL). A técnica alia técnicas de raspagem de dados com técnicas de aprendizado fracamente supervisionado, especificamente o aprendizado com rótulos parciais, explorando uma característica específica dos Captchas, que é a presença de um <em>oráculo</em>.</p>
<p>Oráculo é a resposta do site pesquisado, afirmando se o rótulo enviado está correto ou errado. Eles estão disponíveis em todos os sites com Captchas, já que, por definição, o Captcha precisa apresentar o resultado do teste para o usuário. O nome “oráculo” foi inspirado na mitologia grega, partindo do fato de que o site já possui a informação correta, como um deus. O site, no entanto, se comunica com o usuário através de um intermediário (o oráculo) que apresenta a resposta de forma limitada.</p>
<p>Oráculos se manifestam de diversas formas nos sites com Captchas. Por exemplo, pode dar a possibilidade de realizar apenas um teste por imagem, vários testes por imagem, ou ainda retornar informações ruidosas. Um exemplo de oráculo ruidoso é o reCaptcha v1, que pode retornar com um “bom o suficiente” quando o rótulo não está totalmente correto <span class="citation" data-cites="vonahnReCAPTCHAHumanBasedCharacter2008">(<a href="bibliografia.html#ref-vonahnReCAPTCHAHumanBasedCharacter2008" role="doc-biblioref">AHN et al., 2008</a>)</span>.</p>
<p>O oráculo é uma forma de obter uma base de dados virtualmente infinita. Do ponto de vista de modelagem, é similar a um problema de aprendizado por reforço <span class="citation" data-cites="sutton2018">(<a href="bibliografia.html#ref-sutton2018" role="doc-biblioref">SUTTON; BARTO, 2018</a>)</span>, mas com uma resposta binária (acertou ou errou) no lugar de um escore.</p>
<p>O método WAWL consiste em aproveitar o fato de que o Captcha, por definição, aplica um teste de Turing automático para gerar bases de dados parcialmente anotadas. Ou seja, a técnica resolve o problema não com modelos mais sofisticados, mas com a utilização eficiente dos recursos disponíveis. Qualquer modelo pode se aproveitar dessa característica dos Captchas, incluindo as arquiteturas mais sofisticadas ou técnicas que ainda não foram desenvolvidas.</p>
<p>A metodologia parte de um modelo inicial, que pode ter baixo poder preditivo. O modelo inicial pode ser ajustado com as técnicas usuais de modelagem, ou utilizando um modelo mais sofisticado como GAN. Em seguida, o site na <em>web</em> é acessado múltiplas vezes, gerando uma nova base de dados virtualmente infinita, que é completamente anotada nos casos de acerto e que apresenta o histórico de erros no caso de erro. Os dados gerados automaticamente são então aproveitados para aprimorar o modelo inicial.</p>
<p>Um ponto importante do WAWL é como aproveitar a informação oferecida pelo oráculo. Utilizar somente os casos anotados corretamente, obtidos de acertos no teste do oráculo, induz viés de seleção na amostra <span class="citation" data-cites="na2020">(<a href="bibliografia.html#ref-na2020" role="doc-biblioref">NA et al., 2020</a>)</span>. Como o modelo só tem acesso aos casos em que já funciona bem, a informação obtida não é tão relevante. O desafio de modelagem da tese reside em como considerar a informação fornecida pelo oráculo nos casos em que o modelo inicial erra.</p>
<p>Do ponto de vista estatístico, a informação produzida pelo oráculo pode ser entendida como uma informação censurada <span class="citation" data-cites="colosimo2006">(<a href="bibliografia.html#ref-colosimo2006" role="doc-biblioref">COLOSIMO; GIOLO, 2006</a>)</span>. Isso acontece pois a informação existe e é correta, mas não está completa. No entanto, como a informação é resultado do teste de um rótulo produzido por um modelo, faz sentido afirmar que a censura não é gerada por acaso.</p>
<p>Na área de aprendizado de máquinas, um modelo que apresenta resposta censurada ou incompleta faz parte da classe de <strong>aprendizado fracamente supervisionado</strong> <span class="citation" data-cites="zhou2018">(<a href="bibliografia.html#ref-zhou2018" role="doc-biblioref">ZHOU, 2018</a>)</span>. Trata-se de uma área ainda pouco investigada na literatura, mas bastante ampla, englobando não só os métodos supervisionados como também os métodos semi-supervisionados. A tese apresentará os conceitos de aprendizado fracamente supervisionado, com foco na classe de problemas que a modelagem utilizando Captchas representa.</p>
<p>O custo técnico de implementar o WAWL está na necessidade de utilizar técnicas de raspagem de dados para criar uma nova base usando o oráculo. Essas técnicas imitam repetidamente o que um humano faria para acessar o site, precisando ser desenvolvidas de forma customizada para cada Captcha analisado.</p>
<p>No entanto, resolver Captchas é uma tarefa meio, não uma tarefa fim. Na prática, o interesse é construir ferramentas que acessam os sites e realizar pesquisas com os dados obtidos. E as ferramentas que acessam os sites para obter dados já envolvem a construção de raspadores de dados. Como o desenvolvimento de raspadores de dados é necessário em todas as pesquisas, a parte de raspagem de dados no método WAWL possui tempo de desenvolvimento negligenciável.</p>
<p>A tese tem como foco principal descrever e testar a eficácia do método WAWL. Mas a tese também tem objetivos práticos, relacionados à resolução de Captchas que estão presentes em serviços públicos e disponibilização das soluções desenvolvidas para a comunidade de programadores. A seguir, apresenta-se a lista de objetivos completa, de forma concisa.</p>
</section>
<section id="sec-objetivos" class="level2">
<h2 class="anchored" data-anchor-id="sec-objetivos">Objetivo</h2>
<p>O objetivo geral da tese é desenvolver um método inovador, chamado WAWL (<em>Web Automatic Weak Learning</em>) para resolver Captchas, misturando técnicas de aprendizado profundo com raspagem de dados e aproveitando os dados fornecidos pelo oráculo.</p>
<p>Especificamente, a pesquisa tem como objetivos:</p>
<ol type="1">
<li>Descrever o método proposto e estudar suas características.</li>
<li>Construir e disponibilizar um repositório de dados para realização de mais pesquisas no ramo.</li>
<li>Ajustar modelos e testar a eficácia do método.</li>
<li>Disponibilizar um pacote computacional aberto que possibilita a implementação de soluções para resolver Captchas presentes em serviços públicos.</li>
</ol>
</section>
<section id="sec-justificativa" class="level2">
<h2 class="anchored" data-anchor-id="sec-justificativa">Justificativa</h2>
<p>O presente trabalho é relevante para a ciência por três motivos: importância teórica, viabilidade técnica e importância prática. Os pontos são explicados abaixo.</p>
<p>Do ponto de vista teórico, a tese é importante por apresentar uma aplicação muito especial do aprendizado fracamente supervisionado. No caso do Captcha, como a base de dados fracamente supervisionados é virtualmente infinita, trata-se de uma excelente oportunidade para testar novas técnicas e verificar como elas se comportam empiricamente. Os objetivos 1 e 2 estão relacionados a essa justificativa.</p>
<p>Com relação à viabilidade técnica, o trabalho parte de uma lista de Captchas que já foram resolvidos utilizando técnicas tradicionais de aprendizado profundo. Como os Captchas já foram resolvidos previamente, mesmo que a WAWL não apresentasse bons resultados – e apresenta – o projeto ainda teria como subprodutos as bases de dados e o pacote computacional disponibilizados abertamente. O objetivo 3 é o que torna a proposta tecnicamente viável.</p>
<p>Finalmente, do ponto de vista prático, Captchas em serviços públicos causam desequilíbrio de mercado e incentivam o uso de serviços com formas de remuneração duvidosas. O objetivo 4 vai de encontro direto com esse problema, ao disponibilizar uma ferramenta gratuita e aberta para resolução de Captchas que pode ser utilizada em diversos serviços públicos.</p>
</section>
<section id="sec-hipoteses" class="level2">
<h2 class="anchored" data-anchor-id="sec-hipoteses">Hipóteses</h2>
<p>O projeto foi desenvolvido em torno de duas hipóteses principais. As hipóteses têm origem tanto do levantamento bibliográfico realizado para desenvolver a pesquisa, quanto da experiência pessoal do autor em projetos de pesquisa aplicados.</p>
<ol type="1">
<li><p>A utilização do WAWL gera modelos que resolvem Captchas de textos em imagens sem a necessidade de criar grandes bases anotadas. <!-- a.  Sub-hipótese: É possível criar um modelo genérico que funciona bem e se adapta com o uso do oráculo. --> <!-- b.  Sub-hipótese: Com a teoria de aprendizado fracamente supervisionado, é possível demonstrar que modelos criados dessa forma apresentam desempenho análogo ao que seria obtido com bases totalmente supervisionadas. --></p></li>
<li><p>É possível aliar a área de raspagem de dados com a área de modelagem estatística.</p></li>
</ol>
<!-- a.  Sub-hipótese: O uso de raspagem de dados como passo intermediário do processo de modelagem apresenta resultados positivos no poder preditivo dos Captchas. -->
<!-- b.  Sub-hipótese: É possível criar um modelo com aprendizado ativo, que melhora continuamente conforme é utilizado nos sites. -->
</section>
<section id="sec-organizacao" class="level2">
<h2 class="anchored" data-anchor-id="sec-organizacao">Organização do trabalho</h2>
<p>O segundo capítulo, “metodologia”, contém todos os passos dados para construção da tese, tanto do ponto de vista teórico como prático. Parte-se da definição técnica dos Captchas, chegando até as redes neurais e a classe problema trabalhada de forma ampla. Em seguida, apresenta-se o método WAWL e suas características. Depois, a base de dados é descrita, mostrando as fontes de dados consideradas e as técnicas de raspagem de dados utilizadas. Por último, descreve-se, com detalhes, as simulações realizadas para obter os resultados empíricos.</p>
<p>O terceiro capítulo, “resultados”, apresenta os resultados da pesquisa. Primeiro são apresentados os resultados das simulações e outros experimentos realizados com a técnica WAWL. Em seguida, descreve-se o pacote <code>{captcha}</code>, criado para atingir o objetivo 4 da pesquisa. O capítulo também apresenta uma breve discussão dos resultados obtidos.</p>
<p>No quarto e último capítulo, “conclusão”, a pesquisa é concluída, com apresentação das considerações finais e próximos passos. No final, também foi incluído um apêndice descrevendo e documentando os pacotes <code>{captchaDownload}</code> e <code>{captchaOracle}</code>, criados para atuar em conjunto com o pacote <code>{captcha}</code> para implementar o método WAWL.</p>


<div id="refs" class="references csl-bib-body" data-entry-spacing="1" role="doc-bibliography" style="display: none">
<div id="ref-tempodo" class="csl-entry" role="doc-biblioentry">
ABJ. <strong>Tempo dos processos relacionados à adoção</strong>., 2014. Disponível em: &lt;<a href="https://abj.org.br/pesquisas/adocao/">https://abj.org.br/pesquisas/adocao/</a>&gt;.
</div>
<div id="ref-observat" class="csl-entry" role="doc-biblioentry">
ABJ. <strong>Observatório da insolvência: Rio de Janeiro</strong>., 2021. Disponível em: &lt;<a href="https://abj.org.br/pesquisas/obsrjrj/">https://abj.org.br/pesquisas/obsrjrj/</a>&gt;.
</div>
<div id="ref-diagnosticoABJ" class="csl-entry" role="doc-biblioentry">
ABJ. <strong>Diagnóstico do Contencioso Tributário Administrativo</strong>., 2022. Disponível em: &lt;<a href="https://abj.org.br/pesquisas/bid-tributario/">https://abj.org.br/pesquisas/bid-tributario/</a>&gt;.
</div>
<div id="ref-vonahnReCAPTCHAHumanBasedCharacter2008" class="csl-entry" role="doc-biblioentry">
AHN, L. VON et al. reCAPTCHA: Human-Based Character Recognition via Web Security Measures. <strong>Science</strong>, v. 321, n. 5895, p. 1465–1468, 12 set. 2008. Disponível em: &lt;<a href="https://www.science.org/doi/10.1126/science.1160379">https://www.science.org/doi/10.1126/science.1160379</a>&gt;.
</div>
<div id="ref-vonahnTellingHumansComputers2002" class="csl-entry" role="doc-biblioentry">
AHN, L. VON; BLUM, M.; LANGFORD, J. <strong>Telling humans and computers apart automatically or how lazy cryptographers do AI (Tech. Rep. No. CMU-CS-02-117)</strong>. Disponível em: &lt;<a href="http://reports-archive.adm.cs.cmu.edu/anon/2002/CMU-CS-02-117.pdf">http://reports-archive.adm.cs.cmu.edu/anon/2002/CMU-CS-02-117.pdf</a>&gt;.
</div>
<div id="ref-chellapilla2005" class="csl-entry" role="doc-biblioentry">
CHELLAPILLA, K. et al. <strong>Designing human friendly human interaction proofs (HIPs)</strong>. : CHI ’05.New York, NY, USA: Association for Computing Machinery, 2 abr. 2005. Disponível em: &lt;<a href="https://doi.org/10.1145/1054972.1055070">https://doi.org/10.1145/1054972.1055070</a>&gt;.
</div>
<div id="ref-chellapilla2004" class="csl-entry" role="doc-biblioentry">
CHELLAPILLA, K.; SIMARD, P. Using machine learning to break visual human interaction proofs (HIPs). <strong>Advances in neural information processing systems</strong>, v. 17, 2004.
</div>
<div id="ref-colosimo2006" class="csl-entry" role="doc-biblioentry">
COLOSIMO, E. A.; GIOLO, S. R. <strong>Análise de sobrevivência aplicada</strong>. Editora Blucher, 2006.
</div>
<div id="ref-george2017" class="csl-entry" role="doc-biblioentry">
GEORGE, D. et al. A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs. <strong>Science</strong>, v. 358, n. 6368, p. eaag2612, 2017.
</div>
<div id="ref-goodfellow2013" class="csl-entry" role="doc-biblioentry">
GOODFELLOW, I. J. et al. Multi-digit number recognition from street view imagery using deep convolutional neural networks. <strong>arXiv preprint arXiv:1312.6082</strong>, 2013.
</div>
<div id="ref-goodfellowGenerativeAdversarialNetworks2014" class="csl-entry" role="doc-biblioentry">
GOODFELLOW, I. J. et al. <strong>Generative <span>Adversarial Networks</span></strong>. <span>arXiv</span>, jun. 2014. Disponível em: &lt;<a href="https://arxiv.org/abs/1406.2661">https://arxiv.org/abs/1406.2661</a>&gt;.
</div>
<div id="ref-lecun2015" class="csl-entry" role="doc-biblioentry">
LECUN, Y.; BENGIO, Y.; HINTON, G. Deep learning. <strong>nature</strong>, v. 521, n. 7553, p. 436444, 2015.
</div>
<div id="ref-lillibridgeMethodSelectivelyRestricting2001" class="csl-entry" role="doc-biblioentry">
LILLIBRIDGE, M. D. et al. <strong>Method for Selectively Restricting Access to Computer Systems</strong>., fev. 2001.
</div>
<div id="ref-michener2015" class="csl-entry" role="doc-biblioentry">
MICHENER, G.; MONCAU, L. F.; VELASCO, R. B. <strong>Estado brasileiro e transparência avaliando a aplicação da Lei de Acesso à Informação</strong>.
</div>
<div id="ref-mori2003a" class="csl-entry" role="doc-biblioentry">
MORI, G.; MALIK, J. <strong>Recognizing objects in adversarial clutter: Breaking a visual CAPTCHA</strong>. IEEE, 2003.
</div>
<div id="ref-murray-rust2008" class="csl-entry" role="doc-biblioentry">
MURRAY-RUST, P. Open data in science. <strong>Nature Precedings</strong>, p. 11, 2008.
</div>
<div id="ref-na2020" class="csl-entry" role="doc-biblioentry">
NA, B. et al. <strong>Deep Generative Positive-Unlabeled Learning under Selection Bias</strong>. : CIKM ’20.New York, NY, USA: Association for Computing Machinery, 19 out. 2020. Disponível em: &lt;<a href="https://doi.org/10.1145/3340531.3411971">https://doi.org/10.1145/3340531.3411971</a>&gt;.
</div>
<div id="ref-noh2017" class="csl-entry" role="doc-biblioentry">
NOH, H. et al. Regularizing deep neural networks by noise: Its interpretation and optimization. <strong>Advances in Neural Information Processing Systems</strong>, v. 30, 2017.
</div>
<div id="ref-reshefMethodSystemDiscriminating2005" class="csl-entry" role="doc-biblioentry">
RESHEF, E.; RAANAN, G.; SOLAN, E. <strong>Method and System for Discriminating a Human Action from a Computerized Action</strong>., 2005.
</div>
<div id="ref-sutton2018" class="csl-entry" role="doc-biblioentry">
SUTTON, R. S.; BARTO, A. G. <strong>Reinforcement learning: An introduction</strong>. MIT press, 2018.
</div>
<div id="ref-turing2009" class="csl-entry" role="doc-biblioentry">
TURING, A. M. Computing machinery and intelligence. Em: Springer, 2009. p. 2365.
</div>
<div id="ref-vonahnCaptchaTellingHumans2003" class="csl-entry" role="doc-biblioentry">
VON AHN, L. et al. <strong>Captcha: <span>Telling</span> Humans and Computers Apart Automatically</strong>. Proceedings of Eurocrypt. <strong>Anais</strong>...2003.
</div>
<div id="ref-vonahnTellingHumansComputers2004" class="csl-entry" role="doc-biblioentry">
VON AHN, L.; BLUM, M.; LANGFORD, J. Telling Humans and Computers Apart Automatically. <strong>Communications of the ACM</strong>, v. 47, n. 2, p. 56–60, 2004.
</div>
<div id="ref-inaccess" class="csl-entry" role="doc-biblioentry">
W3C. <strong>Inaccessibility of CAPTCHA</strong>., 2021. Disponível em: &lt;<a href="https://www.w3.org/TR/turingtest/">https://www.w3.org/TR/turingtest/</a>&gt;.
</div>
<div id="ref-wang2021" class="csl-entry" role="doc-biblioentry">
WANG, Y. et al. Make complex captchas simple: a fast text captcha solver based on a small number of samples. <strong>Information Sciences</strong>, v. 578, p. 181194, 2021.
</div>
<div id="ref-ye2018" class="csl-entry" role="doc-biblioentry">
YE, G. et al. <strong>Yet another text captcha solver: A generative adversarial network based approach</strong>. 2018.
</div>
<div id="ref-yuan2019" class="csl-entry" role="doc-biblioentry">
YUAN, X. et al. Adversarial examples: Attacks and defenses for deep learning. <strong>IEEE transactions on neural networks and learning systems</strong>, v. 30, n. 9, p. 28052824, 2019.
</div>
<div id="ref-zhao2017" class="csl-entry" role="doc-biblioentry">
ZHAO, B. Web scraping. <strong>Encyclopedia of big data</strong>, p. 13, 2017. Disponível em: &lt;<a href="https://www.researchgate.net/profile/Bo-Zhao-3/publication/317177787_Web_Scraping/links/5c293f85a6fdccfc7073192f/Web-Scraping.pdf">https://www.researchgate.net/profile/Bo-Zhao-3/publication/317177787_Web_Scraping/links/5c293f85a6fdccfc7073192f/Web-Scraping.pdf</a>&gt;.
</div>
<div id="ref-zhou2018" class="csl-entry" role="doc-biblioentry">
ZHOU, Z.-H. A brief introduction to weakly supervised learning. <strong>National science review</strong>, v. 5, n. 1, p. 4453, 2018.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Para os fins dessa tese, a menos que mencionado de forma explícita, os termos “máquina” e “robô”, ou ainda “procedimento automatizado” serão tratados como sinônimos, geralmente com o nome “robô”.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Link: <a href="https://okfn.org/opendata/" class="uri">https://okfn.org/opendata/</a>. Último acesso em 01/11/2022.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>O CNJ só consegue listar os processos relacionados a um tema a partir da definição de Classes e Assuntos, disponíveis no <a href="https://www.cnj.jus.br/sgt/consulta_publica_classes.php">Sistema de Gestão de Tabelas (SGT) do CNJ</a>. Processos relacionados a recuperação judicial, no entanto, não respeitam a taxonomia do SGT <span class="citation" data-cites="observat">(<a href="bibliografia.html#ref-observat" role="doc-biblioref">ABJ, 2021</a>)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><a href="https://2captcha.com/make-money-online">Link do 2Captcha</a>. Último acesso em 01/11/2022.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><a href="https://2captcha.com/terms-of-service">Link dos termos de serviço do 2Captcha</a>. Último acesso em 01/11/2022.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Fonte: <a href="http://www.ipeadata.gov.br/exibeserie.aspx?stub=1&amp;serid1739471028=1739471028">IPEA</a>. Último acesso em 01/11/2022.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Outro termo para <em>resolver</em> Captchas é <em>quebrar</em> Captchas. Nesta tese, optou-se por utilizar o termo <em>resolver</em>, para enfatizar a interpretação do Captcha como um desafio, não como um problema de criptografia.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Spark, 2011. <a href="https://web.archive.org/web/20120603142110/http://www.cbc.ca/spark/2011/11/full-interview-luis-von-ahn-on-duolingo/">Link no Web Archive</a>. Último acesso em 01/11/2022.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p><a href="https://www.youtube.com/watch?v=i_5ew4btJiQ">Link do vídeo no YouTube</a>. Último acesso em 01/11/2022.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Segundo o <a href="https://www.wired.com/2007/06/ff-humancomp/">texto da Wired</a>: “<em>So he’s fighting back. In late May, von Ahn launched reCaptcha, a service that he believes is the toughest Captcha yet devised. ReCaptcha presents users with two stretched and skewed words, each bisected by a diagonal line”.</em> Último acesso em 01/11/2022.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p><em>Are you a robot? Introducing No Captcha reCaptcha.</em> Acessível no <a href="https://security.googleblog.com/2014/12/are-you-robot-introducing-no-captcha.html">blog da Google</a>. Último acesso em 01/11/2022.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiada");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiada");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Sobre este documento</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./metodologia.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Metodologia</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>