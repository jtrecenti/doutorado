<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="pt" xml:lang="pt"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Resolvendo Captchas - 2&nbsp; Metodologia</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 1em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./resultados.html" rel="next">
<link href="./introducao.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "Nenhum resultado",
    "search-matching-documents-text": "documentos correspondentes",
    "search-copy-link-title": "Copiar link para a busca",
    "search-hide-matches-text": "Esconder correspondências adicionais",
    "search-more-match-text": "mais correspondência neste documento",
    "search-more-matches-text": "mais correspondências neste documento",
    "search-clear-button-title": "Limpar",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar"
  }
}</script>
<link href="site_libs/tabwid-1.1.1/tabwid.css" rel="stylesheet">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="citation_title" content="[[2]{.chapter-number}&nbsp; [Metodologia]{.chapter-title}]{#sec-metodologia .quarto-section-identifier}">
<meta name="citation_language" content="pt">
<meta name="citation_reference" content="citation_title=Recognizing objects in adversarial clutter: Breaking a visual CAPTCHA;,citation_author=Greg Mori;,citation_author=Jitendra Malik;,citation_publication_date=2003;,citation_cover_date=2003;,citation_year=2003;,citation_volume=1;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Recognizing objects in adversarial clutter: Breaking a visual CAPTCHA;,citation_author=Greg Mori;,citation_author=Jitendra Malik;,citation_publication_date=2003;,citation_cover_date=2003;,citation_year=2003;,citation_volume=1;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Designing human friendly human interaction proofs (HIPs);,citation_author=Kumar Chellapilla;,citation_author=Kevin Larson;,citation_author=Patrice Simard;,citation_author=Mary Czerwinski;,citation_publication_date=2005-04-02;,citation_cover_date=2005-04-02;,citation_year=2005;,citation_fulltext_html_url=https://doi.org/10.1145/1054972.1055070;,citation_doi=10.1145/1054972.1055070;,citation_conference=Association for Computing Machinery;,citation_series_title=CHI ’05;">
<meta name="citation_reference" content="citation_title=Using machine learning to break visual human interaction proofs (HIPs);,citation_author=Kumar Chellapilla;,citation_author=Patrice Simard;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_volume=17;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=Multi-digit number recognition from street view imagery using deep convolutional neural networks;,citation_author=Ian J. Goodfellow;,citation_author=Yaroslav Bulatov;,citation_author=Julian Ibarz;,citation_author=Sacha Arnoud;,citation_author=Vinay Shet;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_journal_title=arXiv preprint arXiv:1312.6082;">
<meta name="citation_reference" content="citation_title=Deep learning;,citation_author=Yann LeCun;,citation_author=Yoshua Bengio;,citation_author=Geoffrey Hinton;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_issue=7553;,citation_volume=521;,citation_journal_title=nature;">
<meta name="citation_reference" content="citation_title=Generative adversarial networks;,citation_author=Ian Goodfellow;,citation_author=Jean Pouget-Abadie;,citation_author=Mehdi Mirza;,citation_author=Bing Xu;,citation_author=David Warde-Farley;,citation_author=Sherjil Ozair;,citation_author=Aaron Courville;,citation_author=Yoshua Bengio;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=11;,citation_volume=63;,citation_journal_title=Communications of the ACM;">
<meta name="citation_reference" content="citation_title=Generative Adversarial Networks;,citation_author=Ian J. Goodfellow;,citation_author=Jean Pouget-Abadie;,citation_author=Mehdi Mirza;,citation_author=Bing Xu;,citation_author=David Warde-Farley;,citation_author=Sherjil Ozair;,citation_author=Aaron Courville;,citation_author=Yoshua Bengio;,citation_doi=10.48550/arXiv.1406.2661;">
<meta name="citation_reference" content="citation_title=A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs;,citation_author=Dileep George;,citation_author=Wolfgang Lehrach;,citation_author=Ken Kansky;,citation_author=Miguel Lázaro-Gredilla;,citation_author=Christopher Laan;,citation_author=Bhaskara Marthi;,citation_author=Xinghua Lou;,citation_author=Zhaoshi Meng;,citation_author=Yi Liu;,citation_author=Huayan Wang;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=6368;,citation_volume=358;,citation_journal_title=Science;">
<meta name="citation_reference" content="citation_title=Yet another text captcha solver: A generative adversarial network based approach;,citation_author=Guixin Ye;,citation_author=Zhanyong Tang;,citation_author=Dingyi Fang;,citation_author=Zhanxing Zhu;,citation_author=Yansong Feng;,citation_author=Pengfei Xu;,citation_author=Xiaojiang Chen;,citation_author=Zheng Wang;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Make complex captchas simple: a fast text captcha solver based on a small number of samples;,citation_author=Yao Wang;,citation_author=Yuliang Wei;,citation_author=Mingjin Zhang;,citation_author=Yang Liu;,citation_author=Bailing Wang;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_volume=578;,citation_journal_title=Information Sciences;">
<meta name="citation_reference" content="citation_title=A survey of CAPTCHA technologies to distinguish between human and computer;,citation_author=Xin Xu;,citation_author=Lei Liu;,citation_author=Bo Li;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_volume=408;,citation_journal_title=Neurocomputing;">
<meta name="citation_reference" content="citation_title=Adversarial examples: Attacks and defenses for deep learning;,citation_author=Xiaoyong Yuan;,citation_author=Pan He;,citation_author=Qile Zhu;,citation_author=Xiaolin Li;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=9;,citation_volume=30;,citation_journal_title=IEEE transactions on neural networks and learning systems;">
<meta name="citation_reference" content="citation_title=Regularizing deep neural networks by noise: Its interpretation and optimization;,citation_author=Hyeonwoo Noh;,citation_author=Tackgeun You;,citation_author=Jonghwan Mun;,citation_author=Bohyung Han;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_volume=30;,citation_journal_title=Advances in Neural Information Processing Systems;">
<meta name="citation_reference" content="citation_title=reCAPTCHA: Human-Based Character Recognition via Web Security Measures;,citation_author=Luis Ahn;,citation_author=Benjamin Maurer;,citation_author=Colin McMillen;,citation_author=David Abraham;,citation_author=Manuel Blum;,citation_publication_date=2008-09-12;,citation_cover_date=2008-09-12;,citation_year=2008;,citation_fulltext_html_url=https://www.science.org/doi/10.1126/science.1160379;,citation_issue=5895;,citation_doi=10.1126/science.1160379;,citation_volume=321;,citation_language=en;,citation_journal_title=Science;">
<meta name="citation_reference" content="citation_title=Reinforcement learning: An introduction;,citation_author=Richard S. Sutton;,citation_author=Andrew G. Barto;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Deep Generative Positive-Unlabeled Learning under Selection Bias;,citation_author=Byeonghu Na;,citation_author=Hyemi Kim;,citation_author=Kyungwoo Song;,citation_author=Weonyoung Joo;,citation_author=Yoon-Yeong Kim;,citation_author=Il-Chul Moon;,citation_publication_date=2020-10-19;,citation_cover_date=2020-10-19;,citation_year=2020;,citation_fulltext_html_url=https://doi.org/10.1145/3340531.3411971;,citation_doi=10.1145/3340531.3411971;,citation_conference=Association for Computing Machinery;,citation_series_title=CIKM ’20;">
<meta name="citation_reference" content="citation_title=Análise de sobrevivência aplicada;,citation_author=Enrico Antonio Colosimo;,citation_author=Suely Ruiz Giolo;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;">
<meta name="citation_reference" content="citation_title=A brief introduction to weakly supervised learning;,citation_author=Zhi-Hua Zhou;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=1;,citation_volume=5;,citation_journal_title=National science review;">
<meta name="citation_reference" content="citation_title=Computing machinery and intelligence;,citation_author=Alan M. Turing;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Telling humans and computers apart automatically or how lazy cryptographers do AI (Tech. Rep. No. CMU-CS-02-117);,citation_author=L. Ahn;,citation_author=M. Blum;,citation_author=J. Langford;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_fulltext_html_url=http://reports-archive.adm.cs.cmu.edu/anon/2002/CMU-CS-02-117.pdf;">
<meta name="citation_reference" content="citation_title=Inaccessibility of CAPTCHA;,citation_author=undefined W3C;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://www.w3.org/TR/turingtest/;">
<meta name="citation_reference" content="citation_title=Estado brasileiro e transparência avaliando a aplicação da Lei de Acesso à Informação;,citation_author=Gregory Michener;,citation_author=Luiz Fernando Moncau;,citation_author=Rafael Braem Velasco;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=Open data in science;,citation_author=Peter Murray-Rust;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_journal_title=Nature Precedings;">
<meta name="citation_reference" content="citation_title=Observatório da insolvência: Rio de Janeiro;,citation_author=undefined ABJ;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://abj.org.br/pesquisas/obsrjrj/;">
<meta name="citation_reference" content="citation_title=Tempo dos processos relacionados à adoção;,citation_author=undefined ABJ;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_fulltext_html_url=https://abj.org.br/pesquisas/adocao/;">
<meta name="citation_reference" content="citation_title=Diagnóstico do Contencioso Tributário Administrativo;,citation_author=undefined ABJ;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://abj.org.br/pesquisas/bid-tributario/;">
<meta name="citation_reference" content="citation_title=Diagnóstico do Contencioso Tributário Administrativo;,citation_author=undefined ABJ;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://abj.org.br/pesquisas/bid-tributario/;">
<meta name="citation_reference" content="citation_title=Web scraping;,citation_author=Bo Zhao;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://www.researchgate.net/profile/Bo-Zhao-3/publication/317177787_Web_Scraping/links/5c293f85a6fdccfc7073192f/Web-Scraping.pdf;,citation_journal_title=Encyclopedia of big data;">
<meta name="citation_reference" content="citation_title=A brief introduction to weakly supervised learning;,citation_author=Zhi-Hua Zhou;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=1;,citation_volume=5;,citation_journal_title=National science review;">
<meta name="citation_reference" content="citation_title=Semi-supervised learning literature survey;,citation_author=Xiaojin Jerry Zhu;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;">
<meta name="citation_reference" content="citation_title=A note on learning from multiple-instance examples;,citation_author=Avrim Blum;,citation_author=Adam Kalai;,citation_publication_date=1998;,citation_cover_date=1998;,citation_year=1998;,citation_issue=1;,citation_volume=30;,citation_journal_title=Machine learning;">
<meta name="citation_reference" content="citation_title=Learning with multiple labels;,citation_author=Rong Jin;,citation_author=Zoubin Ghahramani;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_volume=15;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=Gradient-based learning applied to document recognition;,citation_author=Yann LeCun;,citation_author=Léon Bottou;,citation_author=Yoshua Bengio;,citation_author=Patrick Haffner;,citation_publication_date=1998;,citation_cover_date=1998;,citation_year=1998;,citation_issue=11;,citation_volume=86;,citation_journal_title=Proceedings of the IEEE;">
<meta name="citation_reference" content="citation_title=Generalized linear models;,citation_author=John Ashworth Nelder;,citation_author=Robert WM Wedderburn;,citation_publication_date=1972;,citation_cover_date=1972;,citation_year=1972;,citation_issue=3;,citation_volume=135;,citation_journal_title=Journal of the Royal Statistical Society: Series A (General);">
<meta name="citation_reference" content="citation_title=Hierarchical Text-Conditional Image Generation with CLIP Latents;,citation_author=Aditya Ramesh;,citation_author=Prafulla Dhariwal;,citation_author=Alex Nichol;,citation_author=Casey Chu;,citation_author=Mark Chen;,citation_publication_date=2022-04;,citation_cover_date=2022-04;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2204.06125;,citation_issue=arXiv:2204.06125;,citation_doi=10.48550/arXiv.2204.06125;,citation_publisher=arXiv;">
<meta name="citation_reference" content="citation_title=Feature engineering and selection: A practical approach for predictive models;,citation_author=Max Kuhn;,citation_author=Kjell Johnson;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;">
<meta name="citation_reference" content="citation_title=Understanding dropout;,citation_author=Pierre Baldi;,citation_author=Peter J. Sadowski;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_volume=26;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=A review on ensembles for the class imbalance problem: bagging-, boosting-, and hybrid-based approaches;,citation_author=Mikel Galar;,citation_author=Alberto Fernandez;,citation_author=Edurne Barrenechea;,citation_author=Humberto Bustince;,citation_author=Francisco Herrera;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=4;,citation_volume=42;,citation_journal_title=IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews);">
<meta name="citation_reference" content="citation_title=Adam: A Method for Stochastic Optimization;,citation_author=Diederik P. Kingma;,citation_author=Jimmy Ba;,citation_publication_date=2017-01;,citation_cover_date=2017-01;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1412.6980;,citation_issue=arXiv:1412.6980;,citation_doi=10.48550/arXiv.1412.6980;,citation_publisher=arXiv;">
<meta name="citation_reference" content="citation_title=Efficient backprop;,citation_author=Yann A. LeCun;,citation_author=Léon Bottou;,citation_author=Genevieve B. Orr;,citation_author=Klaus-Robert Müller;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Learning from partial labels;,citation_author=Timothee Cour;,citation_author=Ben Sapp;,citation_author=Ben Taskar;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_volume=12;,citation_journal_title=The Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=Provably consistent partial-label learning;,citation_author=Lei Feng;,citation_author=Jiaqi Lv;,citation_author=Bo Han;,citation_author=Miao Xu;,citation_author=Gang Niu;,citation_author=Xin Geng;,citation_author=Bo An;,citation_author=Masashi Sugiyama;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_volume=33;,citation_journal_title=Advances in Neural Information Processing Systems;">
<meta name="citation_reference" content="citation_title=Logistic regression for partial labels;,citation_author=Yves Grandvalet;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;">
<meta name="citation_reference" content="citation_title=Learning from ambiguously labeled examples;,citation_author=Eyke Hüllermeier;,citation_author=Jürgen Beringer;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_issue=5;,citation_volume=10;,citation_journal_title=Intelligent Data Analysis;">
<meta name="citation_reference" content="citation_title=A conditional multinomial mixture model for superset label learning;,citation_author=Liping Liu;,citation_author=Thomas Dietterich;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_volume=25;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=Learning from complementary labels;,citation_author=Takashi Ishida;,citation_author=Gang Niu;,citation_author=Weihua Hu;,citation_author=Masashi Sugiyama;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_volume=30;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=magick: Advanced Graphics and Image-Processing in R;,citation_author=Jeroen Ooms;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://CRAN.R-project.org/package=magick;">
<meta name="citation_reference" content="citation_title=Learning with biased complementary labels;,citation_author=Xiyu Yu;,citation_author=Tongliang Liu;,citation_author=Mingming Gong;,citation_author=Dacheng Tao;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Learning with multiple complementary labels;,citation_author=Lei Feng;,citation_author=Takuo Kaneko;,citation_author=Bo Han;,citation_author=Gang Niu;,citation_author=Bo An;,citation_author=Masashi Sugiyama;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=xml2: Parse XML;,citation_author=Hadley Wickham;,citation_author=Jim Hester;,citation_author=Jeroen Ooms;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://CRAN.R-project.org/package=xml2;">
<meta name="citation_reference" content="citation_title=stringr: Simple, Consistent Wrappers for Common String Operations;,citation_author=Hadley Wickham;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=stringr;">
<meta name="citation_reference" content="citation_title=rvest: Easily Harvest (Scrape) Web Pages;,citation_author=Hadley Wickham;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=rvest;">
<meta name="citation_reference" content="citation_title=Captcha and Its Techniques: A Review;,citation_author=Kiranjot Kaur;,citation_author=Sunny Behal;,citation_publication_date=2014-01-01;,citation_cover_date=2014-01-01;,citation_year=2014;,citation_volume=5;,citation_journal_title=International Journal of Computer Science and Information Technologies,;">
<meta name="citation_reference" content="citation_title=Multivariate binomial/multinomial control chart;,citation_author=Jian Li;,citation_author=Fugee Tsung;,citation_author=Changliang Zou;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_issue=5;,citation_volume=46;,citation_journal_title=IIE Transactions;">
<meta name="citation_reference" content="citation_title=Batch normalization: Accelerating deep network training by reducing internal covariate shift;,citation_author=Sergey Ioffe;,citation_author=Christian Szegedy;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=torch: Tensors and Neural Networks with ’GPU’ Acceleration;,citation_author=Daniel Falbel;,citation_author=Javier Luraschi;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=torch;">
<meta name="citation_reference" content="citation_title=luz: Higher Level ’API’ for ’torch’;,citation_author=Daniel Falbel;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=luz;">
<meta name="citation_reference" content="citation_title=torchvision: Models, Datasets and Transformations for Images;,citation_author=Daniel Falbel;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=torchvision;">
<meta name="citation_reference" content="citation_title=R: A Language and Environment for Statistical Computing;,citation_author=R Core Team;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://www.R-project.org/;">
<meta name="citation_reference" content="citation_title=Feature engineering and selection: A practical approach for predictive models;,citation_author=Max Kuhn;,citation_author=Kjell Johnson;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;">
<meta name="citation_reference" content="citation_title=tensorflow: R Interface to ’TensorFlow’;,citation_author=JJ Allaire;,citation_author=Yuan Tang;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=tensorflow;">
<meta name="citation_reference" content="citation_title=decryptr: An extensible API for breaking captchas;,citation_author=Julio Trecenti;,citation_author=Caio Lente;,citation_author=Daniel Falbel;,citation_author=Milene Farhat;,citation_author=Beatriz Vianna;,citation_author=Evelin Angelica;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=reticulate: Interface to ’Python’;,citation_author=Kevin Ushey;,citation_author=JJ Allaire;,citation_author=Yuan Tang;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=reticulate;">
<meta name="citation_reference" content="citation_title=piggyback: Managing Larger Data on a GitHub Repository;,citation_author=Carl Boettiger;,citation_author=Tan Ho;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=piggyback;">
<meta name="citation_reference" content="citation_title=usethis: Automate Package and Project Setup;,citation_author=Hadley Wickham;,citation_author=Jennifer Bryan;,citation_author=Malcolm Barrett;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://CRAN.R-project.org/package=usethis;">
<meta name="citation_reference" content="citation_title=Hierarchical Text-Conditional Image Generation with CLIP Latents;,citation_author=Aditya Ramesh;,citation_author=Prafulla Dhariwal;,citation_author=Alex Nichol;,citation_author=Casey Chu;,citation_author=Mark Chen;,citation_publication_date=2022-04;,citation_cover_date=2022-04;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2204.06125;,citation_issue=arXiv:2204.06125;,citation_doi=10.48550/arXiv.2204.06125;,citation_publisher=arXiv;">
<meta name="citation_reference" content="citation_title=AVA: A large-scale database for aesthetic visual analysis;,citation_author=Naila Murray;,citation_author=Luca Marchesotti;,citation_author=Florent Perronnin;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_conference=IEEE;">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Metodologia</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Resolvendo Captchas</a> 
        <div class="sidebar-tools-main">
    <a href="./Resolvendo-Captchas.pdf" title="Download PDF" class="sidebar-tool px-1"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Sobre este documento</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introducao.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introdução</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./metodologia.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Metodologia</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./resultados.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Resultados</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./conclusoes.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Conclusões</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliografia.html" class="sidebar-item-text sidebar-link">Bibliografia</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Apêndices</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pacote.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Pacotes</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Índice</h2>
   
  <ul>
  <li><a href="#definição-do-problema" id="toc-definição-do-problema" class="nav-link active" data-scroll-target="#definição-do-problema">Definição do problema</a>
  <ul class="collapse">
  <li><a href="#sec-definicao-captcha" id="toc-sec-definicao-captcha" class="nav-link" data-scroll-target="#sec-definicao-captcha">Captcha</a></li>
  <li><a href="#redes-neurais" id="toc-redes-neurais" class="nav-link" data-scroll-target="#redes-neurais">Redes neurais</a></li>
  <li><a href="#aprendizado-estatístico" id="toc-aprendizado-estatístico" class="nav-link" data-scroll-target="#aprendizado-estatístico">Aprendizado estatístico</a></li>
  </ul></li>
  <li><a href="#sec-wawl" id="toc-sec-wawl" class="nav-link" data-scroll-target="#sec-wawl">Método WAWL</a></li>
  <li><a href="#dados" id="toc-dados" class="nav-link" data-scroll-target="#dados">Dados</a>
  <ul class="collapse">
  <li><a href="#escolha-dos-captchas-analisados" id="toc-escolha-dos-captchas-analisados" class="nav-link" data-scroll-target="#escolha-dos-captchas-analisados">Escolha dos Captchas analisados</a></li>
  <li><a href="#construção-dos-dados" id="toc-construção-dos-dados" class="nav-link" data-scroll-target="#construção-dos-dados">Construção dos dados</a></li>
  </ul></li>
  <li><a href="#simulacoes" id="toc-simulacoes" class="nav-link" data-scroll-target="#simulacoes">Simulações</a>
  <ul class="collapse">
  <li><a href="#primeiro-passo-modelo-inicial" id="toc-primeiro-passo-modelo-inicial" class="nav-link" data-scroll-target="#primeiro-passo-modelo-inicial">Primeiro passo: modelo inicial</a></li>
  <li><a href="#segundo-passo-dados" id="toc-segundo-passo-dados" class="nav-link" data-scroll-target="#segundo-passo-dados">Segundo passo: dados</a></li>
  <li><a href="#sec-modelo-final" id="toc-sec-modelo-final" class="nav-link" data-scroll-target="#sec-modelo-final">Terceiro passo: modelo final</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-metodologia" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Metodologia</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div>
<blockquote class="blockquote">
<p>If machine learning can decipher captchas, what’s next? Toilets that can read our minds?</p>
<p>– ChatGPT</p>
</blockquote>
</div>
<!-- ----------------------------------------------------------- -->
<!-- Trabalhos de estatística costumam ter uma parte de metodologia maior do que introdução e resultados. Essa tese não será diferente. A metodologia é essencial para garantir a reprodutibilidade do trabalho, pois, mais do que simplesmente apresentar o código utilizado, mostra o passo a passo, as dores e as decisões difíceis que tiveram de ser tomadas para concluir o projeto. -->
<p>O capítulo está organizado em três seções: definição do problema, dados e simulações. A primeira mostra a base matemática do problema estudado e as escolhas de sintaxe e terminologias. A segunda descreve as fontes de dados e o processo de coleta, já que a base foi construída totalmente do zero. A terceira mostra como foram planejadas as simulações para verificar se a solução proposta funciona bem empiricamente.</p>
<!-- Pode parecer incomum uma seção de definições no capítulo de metodologia. No entanto, ela é muito importante para as discussões que seguem. Como será mostrado mais adiante, a pesquisa também apresenta resultados teóricos da solução proposta, mas, para apresentá-los, é necessário colocar as definições do objeto matemático que está sendo estudado. -->
<p>Na parte de redes neurais, optou-se por trabalhar nas pontes entre os modelos clássicos de estatística e os modelos de redes neurais, mas sem tecer todos os detalhes técnicos que podem ser encontrados em livros didáticos. Antes de 2015, a pesquisa em redes neurais nos departamentos de estatística era uma novidade, sofrendo até certo preconceito por ser uma classe de modelos “caixa preta”. Com o passar do tempo, no entanto, a área está ficando cada vez mais popular, envolvendo até mesmo trabalhos de iniciação científica no tema. Por isso, optou-se por trazer poucos detalhes da área, focando nas pontes entre os modelos clássicos e as redes neurais. Espera-se que o conteúdo possa ser aproveitado por pessoas interessadas em ensinar redes neurais para estudantes de estatística.</p>
<p>Na seção de dados, procurou-se apresentar a metodologia de coleta em detalhes. Isso foi feito porque a área de raspagem de dados não é comum para estudantes de estatística, existindo até uma percepção entre acadêmicos de que trata-se de uma área separada da estatística. Uma das hipóteses de pesquisa, bem como a solução técnica apresentada neste trabalho é justamente uma ponte entre as duas áreas, justificando um detalhamento maior dos conceitos.</p>
<p>Implementações de raspagem de dados, no entanto, são inconstantes. Um site de interesse pode mudar sua estrutura ou simplesmente trocar o Captcha para um reCaptcha da noite para o dia, alterando completamente o fluxo de coleta. Isso aconteceu, inclusive, com um dos sites mais importantes dentro do contexto da jurimetria: em 2018, o Tribunal de Justiça de São Paulo (TJSP) passou a utilizar o reCaptcha para bloquear ferramentas automatizadas. Qualquer código ou fluxo para acessar as fontes de dados consideradas no trabalho, portanto, estariam datadas no momento da publicação. Por isso, optou-se por apresentar essa parte de forma genérica e deixar as atualizações para os códigos, que estão disponíveis publicamente e serão mantidos com contribuições da comunidade.</p>
<p>Na seção de simulações, procurou-se descrever os passos dados em detalhe. Nesse caso, a escolha do detalhamento se deu por motivos puramente científicos, para que qualquer pessoa possa reproduzir os passos dados. Dessa forma, pessoas interessadas na área podem replicar os resultados em outros exemplos com alterações mínimas no fluxo, além de sugerir melhorias.</p>
<section id="definição-do-problema" class="level2">
<h2 class="anchored" data-anchor-id="definição-do-problema">Definição do problema</h2>
<!-- Nesta seção, buscou-se definir o problema trabalhado de forma precisa. Escolheu-se fazer isso de baixo para cima, a partir da estrutura de dados dos Captchas e depois do problema estatístico. Cabe, no entanto, um adiantamento do que vem pela frente para facilitar a leitura. -->
<p>O problema a ser trabalhado é um caso de <em>aprendizado fracamente supervisionado</em> <span class="citation" data-cites="zhou2018">(<a href="bibliografia.html#ref-zhou2018" role="doc-biblioref">ZHOU, 2018</a>)</span>. Trata-se de uma generalização do aprendizado supervisionado e também do aprendizado <em>semi-supervisionado</em>. Usualmente, a área de aprendizado estatístico (ou aprendizado de máquinas) se concentra em dois tipos de problemas principais: o aprendizado supervisionado e o aprendizado não supervisionado. Isso ocorre principalmente por fins didáticos, pois é mais fácil passar os modelos que fazem parte de cada área.</p>
<p>No entanto, a estatística evolui com os problemas que ocorrem no mundo. E, no mundo, os problemas nem sempre recaem em uma ou outra categoria. O que temos, na verdade, é que os problemas não supervisionados e supervisionados estão conectados, desde que o objetivo de uma pesquisa seja o de predizer valores (regressão) ou categorias (classificação).</p>
<p>Nesse sentido, uma área que ficou popular nos últimos anos, até por conta dos avanços na área de Deep Learning, é o <em>aprendizado semi-supervisionado</em> <span class="citation" data-cites="zhu2005">(<a href="bibliografia.html#ref-zhu2005" role="doc-biblioref">ZHU, 2005</a>)</span>. Trata-se de uma classe de problemas contendo uma amostra completamente anotada e uma amostra sem anotações. A amostra sem anotações é usada para compreender como os dados foram gerados, e os parâmetros podem ser compartilhados com a parte supervisionada do modelo. Isso poderia indicar que existem três classes de problemas: o não supervisionado, o supervisionado e o semi-supervisionado.</p>
<p>Mas isso também não representa todas as classes de problemas. Em muitas aplicações reais, obter uma anotação completa e correta pode ser custoso ou até impraticável. Além disso por envolver trabalho humano, é comum que classificações contenham erros. Para lidar com esses casos existe uma área, que generaliza as anteriores, que é o aprendizado fracamente supervisionado.</p>
<p>Aprendizado fracamente supervisionado é um termo guarda-chuva. Dentro da área existem diversos tipos de problemas, como aprendizado semi-supervisionado, aprendizado de múltiplas instâncias <span class="citation" data-cites="blum1998">(<a href="bibliografia.html#ref-blum1998" role="doc-biblioref">BLUM; KALAI, 1998</a>)</span> e o aprendizado com rótulos incorretos ou incompletos <span class="citation" data-cites="zhou2018">(<a href="bibliografia.html#ref-zhou2018" role="doc-biblioref">ZHOU, 2018</a>)</span>. O caso dos Captchas com o uso do oráculo será apresentado como outra classe de problemas, chamada <strong><em>aprendizado com rótulos parciais</em></strong> (<em>partial label learning</em>, PLL, <span class="citation" data-cites="jin2002">(<a href="bibliografia.html#ref-jin2002" role="doc-biblioref">JIN; GHAHRAMANI, 2002</a>)</span>). Essa classe apresenta uma especialização ainda mais próxima do problema estudado, chamado <strong><em>aprendizado com rótulos complementares</em></strong> (<em>complementary label learning</em>, <span class="citation" data-cites="ishida2017">(<a href="bibliografia.html#ref-ishida2017" role="doc-biblioref">ISHIDA et al., 2017</a>)</span>).</p>
<p>A intepretação do Captcha como um problema de PLL será apresentada no final do capítulo. A jornada começa de onde deve começar, com aqueles que são objeto de análise deste trabalho: os Captchas.</p>
<section id="sec-definicao-captcha" class="level3">
<h3 class="anchored" data-anchor-id="sec-definicao-captcha">Captcha</h3>
<p>Captcha é um <em>desafio</em> do tipo <em>desafio-resposta</em> usado para determinar se a usuário do sistema é um humano. Existem diversos tipos de Captcha diferentes, que envolvem desde identificar textos em imagens até resolver expressões matemáticas complexas.</p>
<p>O foco deste trabalho reside nos Captchas baseados em imagens rotuladas, que é o tipo mais comum. Em seguida, a menos que se mencione ao contrário, todos os Captchas apresentados são desse tipo.</p>
<p>O fluxo completo de um Captcha envolve cinco componentes: um <em>rótulo</em>, um <em>gerador</em>, uma <em>imagem</em>, um agente e um <em>oráculo</em>. Um ciclo do Captcha é completado ao seguir os passos:</p>
<ol type="1">
<li>O rótulo é definido, usualmente com algum procedimento aleatório, ocultado do agente.</li>
<li>A imagem é gerada a partir do rótulo e apresentada para o agente.</li>
<li>O agente preenche sua resposta a partir da imagem (que pode estar certa ou errada)</li>
<li>O oráculo verifica se a resposta está correta.</li>
<li>Dependendo da resposta, o agente é direcionado para a página autenticada ou para uma página de erro.</li>
</ol>
<p>A <a href="#fig-esquema-captcha">Figura&nbsp;<span>2.1</span></a> esquematiza o fluxo do Captcha.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-esquema-captcha" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assets/img/esquema-captcha.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;2.1: Fluxo do Captcha</figcaption><p></p>
</figure>
</div>
</div>
</div>
<section id="imagem-rótulo-e-variável-resposta" class="level4">
<h4 class="anchored" data-anchor-id="imagem-rótulo-e-variável-resposta">Imagem, rótulo e variável resposta</h4>
<p>A imagem é uma matriz <span class="math inline">\(\mathbf x = \{x_{nmr} \in [0,1]\}_{N\times M \times R}\)</span>, contendo padrões que, a partir da análise humana, levam ao rótulo do Captcha. O <em>rótulo</em> é dado por um vetor de caracteres <span class="math inline">\(\mathbf c = [c_1,\dots,c_L]^\top\)</span>. O comprimento <span class="math inline">\(L\)</span> pode ser fixo ou variável, ou seja, duas imagens criadas pelo mesmo gerador podem vir com comprimentos diferentes. Nas definições que seguem considerou-se <span class="math inline">\(L\)</span> como fixo, por simplicidade.</p>
<p>Captchas costumam ter dimensões relativamente pequenas, com a altura <span class="math inline">\(N\)</span> variando entre 30 e 200 <em>pixels</em> e a largura <span class="math inline">\(M\)</span> variando entre 100 e 300 <em>pixels</em>. As imagens costumam ser retangulares para comportar várias letras lado a lado, ou seja, geralmente <span class="math inline">\(M &gt; N\)</span>. O valor de <span class="math inline">\(R\)</span> é 1 para imagens em escala de cinza e 3 para imagens coloridas.</p>
<p>Os elementos do vetor <span class="math inline">\(\mathbf c\)</span> fazem parte de um alfabeto <span class="math inline">\(\mathcal A\)</span>, com cardinalidade <span class="math inline">\(|\mathcal A|\)</span>, finito e conhecido. O alfabeto contém todos os possíveis caracteres que podem aparecer na imagem. Na maioria dos casos, <span class="math inline">\(\mathcal A\)</span> corresponde a uma combinação de algarismos arábicos (0-9) e letras do alfabeto latino (a-z), podendo diferenciar ou não as letras maiúsculas e minúsculas<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>O elemento da matriz <span class="math inline">\(x_{nm\cdot}\)</span> é denominado <em>pixel</em>. Um pixel representa a menor unidade possível da imagem. Em uma imagem colorida, por exemplo, <span class="math inline">\(R=3\)</span>. Nesse caso, um pixel é um vetor de três dimensões com valores entre zero e um, representando a intensidade de vermelho, verde e azul da coordenada <span class="math inline">\((n, m)\)</span> da imagem. Em imagens em escala de cinza, <span class="math inline">\(R=1\)</span> e o pixel, de uma dimensão, representa a intensidade do cinza, sendo 1 o equivalente da cor branca e 0 da cor preta.</p>
<p>A <a href="#fig-exemplo-tjmg">Figura&nbsp;<span>2.2</span></a> mostra um exemplo Captcha do Tribunal de Justiça de Minas Gerais (TJMG). Nesse caso, tem-se <span class="math inline">\(L=5\)</span> e <span class="math inline">\(|\mathcal A|=10\)</span>, apenas os dez algarismos arábicos. A imagem tem dimensões <span class="math inline">\(N=110\)</span>, <span class="math inline">\(M=40\)</span> e <span class="math inline">\(R=3\)</span>. O rótulo da imagem é <span class="math inline">\([5,2,4,3,2]^\top\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-exemplo-tjmg" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assets/img/tjmg16283c1e6d06.jpeg" class="img-fluid figure-img" width="110"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;2.2: Exemplo de Captcha no TJMG.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>A <strong>variável resposta</strong> é uma matriz binária <span class="math inline">\(\mathbf y_{L \times |\mathcal A|}\)</span>, em que cada linha representa um dos valores do vetor <span class="math inline">\(\mathbf c\)</span>, enquanto as colunas possuem um representante para cada elemento de <span class="math inline">\(\mathcal A\)</span>. Um elemento <span class="math inline">\(y_{ij}\)</span> vale 1 se o elemento <span class="math inline">\(i\)</span> do rótulo <span class="math inline">\(\mathbf c\)</span> corresponde ao elemento <span class="math inline">\(j\)</span> do alfabeto <span class="math inline">\(\mathcal A\)</span>, valendo zero caso contrário. A variável resposta pode ser pensada também como o <em>one-hot encoding</em> do rótulo.</p>
<p>Uma maneira alternativa de definir a variável resposta seria com um vetor de índices representando cada elemento do alfabeto em um vetor. O problema de trabalhar dessa forma é que a variável resposta <span class="math inline">\(\mathbf y\)</span> tem um número exponencial de combinações: o rótulo possui <span class="math inline">\(L\)</span> caracteres, sendo que cada caractere pode ter <span class="math inline">\(|\mathcal A|\)</span> valores, totalizando <span class="math inline">\(|\mathcal A|^L\)</span> combinações.</p>
<p>Por exemplo, um Captcha com <span class="math inline">\(L=6\)</span> letras e <span class="math inline">\(|\mathcal A| = 36\)</span> possibilidades em cada letra (26 letras do alfabeto latino e 10 algarismos arábicos), possui um total de 2.176.782.336 (<span class="math inline">\(&gt;\)</span> 2 bilhões) combinações. Por isso, modelar as imagens diretamente através de uma única variável resposta categórica é tecnicamente inviável.</p>
<p>A forma <em>one-hot</em> da resposta pode ser entendida como uma <strong>multinomial</strong> <strong>multivariada</strong> <span class="citation" data-cites="li2014">(<a href="bibliografia.html#ref-li2014" role="doc-biblioref">LI; TSUNG; ZOU, 2014</a>)</span>. A resposta é multivariada porque temos <span class="math inline">\(L\)</span> caracteres na imagem e multinomial porque temos <span class="math inline">\(|\mathcal A|\)</span> possíveis caracteres em cada posição. Dessa forma, podemos pensar que um modelo que resolve o Captcha envolve <span class="math inline">\(L\)</span> classificadores com resposta multinomial, cada um dando conta de um dos caracteres. Os classificadores podem ser independentes e podem até contar com etapas de pré-processamento separadas.</p>
<p>Seguindo o exemplo da <a href="#fig-exemplo-tjmg">Figura&nbsp;<span>2.2</span></a>, é possível representar o rótulo da seguinte forma:</p>
<p><span class="math display">\[
\mathbf c = \left[\begin{array}{c}
     5  \\
     2 \\
     4 \\
     3 \\
     2
\end{array}\right] \rightarrow \mathbf{y} = \left[\begin{array}{cccccccccc}
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
\end{array}\right]
\]</span></p>
<p>A forma <em>dummy</em> da resposta facilita os trabalhos que seguem. Como será visto mais adiante, o modelo de rede neural gerará uma matriz de probabilidades que somam <span class="math inline">\(1\)</span> em cada linha, com as probabilidades de cada caractere em cada posição.</p>
</section>
<section id="gerador" class="level4">
<h4 class="anchored" data-anchor-id="gerador">Gerador</h4>
<p>O <strong>gerador</strong> é uma função <span class="math inline">\(g\)</span> que recebe um rótulo como entrada e devolve uma imagem como saída. Um bom gerador é aquele que é capaz de gerar uma imagem fácil de interpretar por humanos, mas difícil de se resolver por máquinas.</p>
<p>Um exemplo de gerador é a função <code>captcha_generate()</code> criada no pacote <code>{captcha}</code>, como descrito no <a href="pacote.html"><span>Apêndice&nbsp;A</span></a>. A função foi criada para realizar simulações do sistema de resolução proposto na tese, a partir do pacote <code>{magick}</code> <span class="citation" data-cites="magick">(<a href="bibliografia.html#ref-magick" role="doc-biblioref">OOMS, 2021</a>)</span>, que utiliza o software <em>ImageMagick</em>. A função aplica uma série de distorções e efeitos comuns no contexto de Captchas, gerando imagens como a da <a href="#fig-captcha-r-exemplo">Figura&nbsp;<span>2.3</span></a>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-captcha-r-exemplo" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="metodologia_files/figure-html/fig-captcha-r-exemplo-1.png" class="img-fluid figure-img" width="110"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;2.3: Exemplo de captcha gerado pela função <code>captcha::captcha_generate()</code></figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>O gerador segue os passos abaixo, a partir do momento em que um rótulo <span class="math inline">\(\mathbf c\)</span> existe:</p>
<ol type="1">
<li>É criada uma matriz <span class="math inline">\(N\times M \times R\)</span>, com valores entre zero e um gerados por simulações de uma <span class="math inline">\(\mathcal U(0,1)\)</span>.</li>
<li>É adicionada uma cor base ao ruído, definida de forma aleatória.</li>
<li>A matriz é transformada em um objeto do tipo <code>magick-image</code>.</li>
<li>A imagem é preenchida com o valor do rótulo, adicionando-se efeitos como rotação, uma linha unindo as letras e variação de cores.</li>
<li>A imagem recebe outros tipos de distorções, como adição de ruído, alteração de cores e outros efeitos.</li>
</ol>
<p>No final, o gerador retorna a imagem, que é a única informação enviada ao agente. O rótulo fica escondido para verificação do oráculo.</p>
</section>
<section id="sec-oraculo" class="level4">
<h4 class="anchored" data-anchor-id="sec-oraculo">Oráculo</h4>
<p>Para definir o oráculo, utilizou-se uma terminologia que é facilmente encaixada com a teoria de aprendizado fracamente supervisionado. Seja <span class="math inline">\(g\)</span> um classificador utilizado para predizer o rótulo de uma imagem e seja <span class="math inline">\(\mathbf X_{n+1}\)</span> uma nova imagem que é observada, com sua resposta <span class="math inline">\(\mathbf Y_{n+1}\)</span>, desconhecida. A operação <span class="math inline">\(g(\mathbf X_{n+1}) = \hat {\mathbf Y}_{n+1}\)</span> retorna um candidato para <span class="math inline">\(\mathbf Y_{n+1}\)</span>, que pode estar correto ou errado.</p>
<p>O oráculo é uma função <span class="math inline">\(\mathcal O: \mathcal Y \rightarrow 2^{\mathcal Y}\)</span>, ou seja, uma função que recebe um elemento do domínio da resposta <span class="math inline">\(\mathcal Y\)</span> (ou seja, do conjunto de todas as combinações de rótulos) para o conjunto de subconjuntos (as partes) de <span class="math inline">\(\mathcal Y\)</span>. Na prática, a função retorna uma lista de possíveis valores de <span class="math inline">\({\mathbf Y}_{n+1}\)</span>, da seguinte forma:</p>
<p><span class="math display">\[
\mathcal O(\hat {\mathbf Y}_{n+1}) = \left\{\begin{array}{ll}
    \{\mathbf Y_{n+1}\}, &amp; \text{ se } \mathbf Y_{n+1} = \hat {\mathbf Y}_{n+1}  \\
    \mathcal Y \setminus \{\hat {\mathbf Y}_{n+1}\}, &amp; \text{ se } \mathbf Y_{n+1} \neq \hat {\mathbf Y}_{n+1}
\end{array}\right.
\]</span></p>
<p>Quando o classificador <span class="math inline">\(g\)</span> acerta o rótulo, o oráculo retorna uma lista que contém apenas um elemento: o próprio rótulo. Para simplificar, também é possível utilizar a notação de <em>rótulo complementar</em> <span class="math inline">\(\mathbf Y_{n+1} \neq \hat {\mathbf Y}_{n+1} = \bar{\mathbf Y}\)</span>. Quando o classificador <span class="math inline">\(g\)</span> retorna o rótulo errado, o oráculo retorna uma lista com todos os outros possíveis rótulos do rótulo, o que inclui o verdadeiro valor <span class="math inline">\(\mathbf Y_{n+1}\)</span>.</p>
<p>A <a href="#fig-esquema-oraculo">Figura&nbsp;<span>2.4</span></a> mostra o funcionamento do oráculo no exemplo do TJMG. Quando a predição é igual ao rótulo, o resultado apresentado é o valor um, indicando que o rótulo está correto. Quando a predição é diferente do rótulo, o resultado apresentado é o valor zero, indicando que o valor testado está incorreto e que, portanto, o rótulo real é um dentre todos os outros possíveis rótulos.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-esquema-oraculo" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assets/img/esquema-oraculo.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;2.4: Esquema mostrando o funcionamento do oráculo.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>É possível generalizar naturalmente o oráculo para múltiplos chutes mudando a definição da função que faz predições. Seja <span class="math inline">\(h\)</span> uma função que retorna um conjunto de <span class="math inline">\(k\)</span> respostas possíveis, <span class="math inline">\(k\in \mathbb N\)</span>, <span class="math inline">\(k\geq 1\)</span>, com <span class="math inline">\(\mathbf x_{n+1}\)</span> e <span class="math inline">\(\mathbf y_{n+1}\)</span> iguais aos definidos definidos anteriormente. Então o oráculo tem o funcionamento definido abaixo:</p>
<p><span class="math display">\[
\mathcal O(h(\mathbf x_{n+1})) = \left\{\begin{array}{ll}
    \{\mathbf y_{n+1}\}, &amp; \text{ se } \mathbf y_{n+1} \in h(\mathbf x_{n+1})  \\
   \mathcal Y \setminus h(\mathbf x_{n+1}), &amp; \text{ se } \mathbf y_{n+1} \notin h(\mathbf x_{n+1})
\end{array}\right..
\]</span></p>
<p>Nesse caso, o oráculo também retorna uma lista com a resposta <span class="math inline">\(\mathbf y_{n+1}\)</span>. A única diferença é que, quando o Captcha aceita múltiplos chutes, a lista retornada em caso de erro tem um comprimento menor.</p>
<p>O oráculo tem um papel fundamental na solução proposta. O fato do oráculo sempre retornar a resposta correta na lista de opções faz com que ela necessariamente reduza o espaço de respostas a serem buscadas em uma tentativa futura. Esse fato será explorado a partir de um método iterativo para encontrar o valor real do rótulo.</p>
</section>
<section id="fatos-estilizados" class="level4">
<h4 class="anchored" data-anchor-id="fatos-estilizados">Fatos estilizados</h4>
<p>Historicamente, uma alternativa para resolver Captchas é separando o problema em duas tarefas: segmentar e classificar. A tarefa de segmentação consiste em receber uma imagem com várias letras e detectar pontos de corte, separando-a em várias imagens de uma letra. Já a classificação consiste em receber uma imagem com uma letra e identificar o caractere correspondente. Nesse caso, a resposta é reduzida para <span class="math inline">\(|\mathcal A|\)</span> categorias, que cresce linearmente e, portanto, tratável.</p>
<p>A tarefa de resolver Captchas também poderia ser vista como um problema de reconhecimento óptico de caracteres (<em>Optical Character Recognition</em>, OCR). No entanto, as distorções encontradas em Captchas são bem diferentes das distorções encontradas em textos escaneados, que são o objeto de aplicação de ferramentas de OCR. Por esse motivo, as ferramentas usuais de OCR apresentam resultados pouco satisfatórios em vários Captchas.</p>
<p>As distorções encontradas em Captchas podem ser agrupadas em distorções para dificultar a segmentação e distorções para dificultar a classificação. Na parte de classificação, as principais formas de dificultar o trabalho dos modelos são i) mudar as fontes (serifa ou sem serifa ou negrito/itálico, por exemplo), ii) mudar letras minúsculas para maiúsculas e iii) adicionar distorções nos caracteres. Já na parte de segmentação, as principais formas são i) colar os caracteres e ii) adicionar linhas ligando os dígitos. Essas técnicas são combinadas com a adição de ruído e distorção nas imagens completas para compor a imagem final.</p>
</section>
</section>
<section id="redes-neurais" class="level3">
<h3 class="anchored" data-anchor-id="redes-neurais">Redes neurais</h3>
<p>A abordagem discutida ao longo da tese utiliza redes neurais convolucionais. Para explicar o funcionamento dessa técnica, apresenta-se as definições para redes neurais e para a operação de convolução no contexto de Captchas, construindo o modelo utilizado nas simulações do modelo proposto.</p>
<!-- Uma rede neural neste contexto pode ser entendida como uma extensão de modelos lineares generalizados com a alteração da arquitetura dos componentes do modelo. Para mostrar esse conceito, parte-se da definição de um modelo regressão logística até construir uma rede neural com camadas ocultas. -->
<p>A ideia abaixo é apresentar como funcionam as redes neurais no contexto de Captchas. O modelo apresentado é o que foi utilizado nas simulações, que é um modelo de redes neurais convolucionais simples, similar ao LeNet, com três camadas convolucionais e duas camadas densas <span class="citation" data-cites="lecun1998">(<a href="bibliografia.html#ref-lecun1998" role="doc-biblioref">LECUN et al., 1998</a>)</span>.</p>
<p>A técnica proposta pela tese pode utilizar diversas arquiteturas de redes neurais. A escolha de uma arquitetura mais simples foi feita para demonstrar a eficácia do procedimento de forma mais contundente. Outras arquiteturas mais rebuscadas, como as apresentadas no referencial teórico <span class="citation" data-cites="george2017 ye2018">(<a href="bibliografia.html#ref-george2017" role="doc-biblioref">GEORGE et al., 2017</a>; <a href="bibliografia.html#ref-ye2018" role="doc-biblioref">YE et al., 2018</a>)</span> podem melhorar a aplicação do modelo. A única restrição é que ela possa receber uma função de perda modificada, como será mostrado a seguir.</p>
<p>É possível organizar a estrutura de uma rede neural em três componentes: a <strong>arquitetura da rede</strong>, a <strong>função de perda</strong> e o <strong>otimizador</strong>. Os componentes são detalhados nas próximas subseções.</p>
<p>Como uma rede neural possui muitos componentes e subcomponentes, é usual apresentar sua estrutura na forma de um diagrama. Redes neurais costumam ser fáceis de representar através de grafos, que podem ser utilizados de forma mais ou menos detalhada, dependento do interesse.</p>
<p>A <a href="#fig-diagrama-modelo-cnn">Figura&nbsp;<span>2.5</span></a> mostra, de forma esquemática, os componentes (retângulos tracejados) e subcomponentes (partes internas dos componentes) do modelo utilizado.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-diagrama-modelo-cnn" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assets/img/diagrama-modelo-cnn.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;2.5: Diagrama representando o modelo utilizado de forma genérica, com todos os componentes e subcomponentes apresentados de forma esquemática. As partes de fora dos componentes são entradas de dados ou decisões de parada do ajuste.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<section id="sec-arquitetura-rede" class="level4">
<h4 class="anchored" data-anchor-id="sec-arquitetura-rede">Arquitetura da rede</h4>
<p>A arquitetura da rede é uma função que leva os dados de entrada na estrutura de dados da variável resposta. A arquitetura tem papel similar ao exercido pelo componente sistemático em um modelo linear generalizado <span class="citation" data-cites="nelder1972">(<a href="bibliografia.html#ref-nelder1972" role="doc-biblioref">NELDER; WEDDERBURN, 1972</a>)</span>. Trata-se da parte mais complexa da rede neural, carregando todos os parâmetros que serão otimizados.</p>
<p>A arquitetura da rede possui três componentes principais, separados em dois itens cada:</p>
<ul>
<li>as camadas ocultas: camadas <strong>convolucionais</strong> e camadas <strong>densas</strong>;</li>
<li>as técnicas de regularização: <strong>normalização em lote</strong> (<em>batch normalization</em>), <strong><em>dropout</em></strong> e junção de pixels (<em>max pooling</em>);</li>
<li>as funções de ativação: função de ativação linear retificada (<em>rectified linear unit</em>, ReLU) e a função de normalização exponencial (<em>softmax</em>).</li>
</ul>
<p>Abaixo, apresenta-se as definições seguindo-se a ordem de aplicação das operações na arquitetura da rede neural: camada convolucional, ReLU, <em>max pooling</em>, <em>batch normalization</em>, <em>dropout</em>, camada densa e <em>softmax</em>.</p>
<p>A <strong>convolução</strong> é uma operação linear que recebe como entrada uma matriz e retorna outra matriz. Ela é diferente de uma operação usual de multiplicação de matrizes vista no contexto de modelos lineares generalizados, por envolver uma operação nos elementos na vizinhança de cada pixel.</p>
<p>Uma forma organizada de fazer essa soma ponderada é criando uma matriz de pesos. Com ela, não é necessário procurar os pontos da vizinhança. Para cada ponto <span class="math inline">\((i,j)\)</span>, obtem-se a matriz de vizinhança, multiplica-se pontualmente pela matriz de pesos e soma-se os valores resultantes. A matriz de pesos é chamada de núcleo, ou <em>kernel</em>.</p>
<p>Considere</p>
<p><span class="math display">\[
K = \left[\begin{array}{rrr}-1&amp;-1&amp;-1\\0&amp;0&amp;0\\1&amp;1&amp;1\end{array}\right]
\]</span></p>
<p>e a imagem da <a href="#fig-tjmg-exemplo-conv">Figura&nbsp;<span>2.6</span></a>. Como visto anteriormente, trata-de de uma matriz de dimensão <span class="math inline">\(40\times110\times3\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-tjmg-exemplo-conv" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assets/img/tjmg16283c1e6d06.jpeg" class="img-fluid figure-img" width="110"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;2.6: Imagem de Captcha utilizado em exemplos anteriores.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Tome por exemplo a primeira dimensão do pixel <span class="math inline">\((i,j,k) = (12,16,1)\)</span>. A vizinhança 3x3 em torno desse ponto é dada por</p>
<div class="cell">

</div>
<p><span class="math display">\[
P_{i,j,k} = \left[\begin{array}{rrr}
0.094 &amp; 0.412 &amp; 0.686 \\
0.051 &amp; 0.063 &amp; 0.529 \\
0.071 &amp; 0.000 &amp; 0.086
\end{array}\right]
\]</span></p>
<p>A operação de convolução é feita da seguinte forma:</p>
<p><span class="math display">\[
\begin{aligned}
(P_{12,16,1} *K )_{12,16,1}
&amp;= k_{1,1}p_{11,15,1} + k_{1,2}p_{11,16,1} + k_{1,3}p_{11,17,1} + \\
&amp;+ k_{2,1}p_{12,15,1} + k_{2,2}p_{12,16,1} + k_{2,3}p_{12,17,1} + \\
&amp;+ k_{3,1}p_{13,15,1} + k_{3,2}p_{13,16,1} + k_{3,3}p_{13,17,1}
\end{aligned}
\]</span></p>
<p>Esse é o valor a ser colocado no ponto <span class="math inline">\((i,j,k)\)</span>. Isso funciona em todos os pontos que não estão na borda da imagem.</p>
<p>Existem duas formas de trabalhar com as bordas da imagem. A primeira é preenchendo as bordas com zeros, de forma a considerar apenas os pontos da imagem. A segunda é descartar os pontos da borda e retornar uma imagem menor, contendo somente os pixels em que foi possível aplicar todo o <em>kernel</em>.</p>
<p>No caso do exemplo, o resultado da convolução fica como na <a href="#fig-tjmg-exemplo-conv-horizontal">Figura&nbsp;<span>2.7</span></a>. A matriz não foi escolhida por acaso: ela serve para destacar padrões horizontais da imagem. Como a primeira linha é formada por <span class="math inline">\(-1\)</span> e a última é formada por <span class="math inline">\(1\)</span>, a matriz fica com valor alto se a parte de cima do pixel for preta e a parte de baixo for branca (<span class="math inline">\(\text{grande} * 1 + \text{pequeno} * (-1)\)</span>). A parte destacada da imagem acabou sendo a parte de baixo dos números e, principalmente, a linha que une os números.</p>
<div class="cell">

</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-tjmg-exemplo-conv-horizontal" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assets/img/tjmg_conv_horizontal.jpeg" class="img-fluid figure-img" width="110"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;2.7: Aplicação de uma convolução com kernel horizontal.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Aplicando o kernel vertical abaixo</p>
<p><span class="math display">\[
K = \left[\begin{array}{rrr}-1&amp;0&amp;1\\-1&amp;0&amp;1\\-1&amp;0&amp;1\end{array}\right],
\]</span></p>
<p>as partes destacadas são as laterais dos números, conforme <a href="#fig-tjmg-exemplo-conv-vertical">Figura&nbsp;<span>2.8</span></a>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-tjmg-exemplo-conv-vertical" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assets/img/tjmg_conv_vertical.jpeg" class="img-fluid figure-img" width="110"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;2.8: Aplicação de uma convolução com kernel horizontal.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>O resultado da convolução pode ter números negativos ou maiores que um. Para que seja possível visualizar, as imagens mostradas acima foram normalizadas.</p>
<p>Uma característica das imagens mostradas acima é que elas ficaram escuras, ou seja, com muitos valores próximos de zero. Uma técnica para modificar a imagem é adicionar uma constante numérica ao resultado da convolução. Esse é o chamado <strong>viés</strong> (<em>bias</em>) da convolução.</p>
<p>A <a href="#fig-tjmg-exemplo-conv-vertical-bias">Figura&nbsp;<span>2.9</span></a> mostra o efeito de adicionar um viés de <code>0.6</code> após aplicação da convolução com kernel vertical. É possível idenificar claramente a diferença entre os números (mais suaves) e as curvas usadas para conectar os números (mais proeminetes).</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-tjmg-exemplo-conv-vertical-bias" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assets/img/tjmg_conv_vertical_bias.jpeg" class="img-fluid figure-img" width="110"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;2.9: Aplicação de uma convolução com kernel horizontal.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Uma <strong>camada convolucional</strong> envolve a aplicação de convoluções com <span class="math inline">\(d\)</span> <em>kernels</em> em uma matriz, além da adição do <em>bias</em>. O resultado da aplicação de uma camada convolucional com preenchimento das bordas é uma matriz com as mesmas dimensões <span class="math inline">\(N\)</span> e <span class="math inline">\(M\)</span> da matriz de entrada, mas com <span class="math inline">\(d\)</span> entradas na dimensão das cores. Como o valor de <span class="math inline">\(d\)</span> pode ser diferente de 1 ou 3, não faz mais sentido tratar essa dimensão como cores, por isso essa dimensão é chamada de <strong>canais</strong> da imagem resultante.</p>
<p>É importante notar que, nos exemplos apresentados anteriormente, a convolução foi aplicada a apenas um dos canais da imagem: o primeiro. Quando a imagem de entrada possui vários canais, camada convolucional aplica cada <em>kernel</em> em cada canal da imagem e, depois, faz a soma dos valores resultantes.</p>
<p>A <a href="#fig-tjmg-exemplo-camada-conv">Figura&nbsp;<span>2.10</span></a> mostra um exemplo de aplicação de camada convolucional para a imagem utilizada nos exemplos anteriores. Os <em>kernels</em> foram escolhidos com base em um modelo que já foi ajustado para o Captcha. Note que os canais capturam a informação dos números e dos ruídos, focando em detalhes diferentes.</p>
<div class="cell">

</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-tjmg-exemplo-camada-conv" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assets/img/tjmg_conv1_modelo.jpeg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;2.10: Resultado da aplicação da primeira convolução à imagem.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Antes da aplicação da camada convolucional, a operação de <strong><em>batch normalization</em></strong> foi aplicada. Essa operação normaliza os números da matriz de entrada antes da aplicação da convolução, retirando a média e dividindo pelo desvio padrão.</p>
<p><span class="math display">\[
x_z = \left(\frac{x-\bar x}{\sqrt{\sigma^2_x + \epsilon}}\right) \gamma + \beta
\]</span></p>
<p>O valor <span class="math inline">\(\epsilon\)</span>, geralmente um valor pequeno, é adicionado para evitar problemas numéricos quando a variância é muito baixa. Os parâmetros <span class="math inline">\(\gamma\)</span> e <span class="math inline">\(\beta\)</span> podem ser adicionados no passo da normalização, fazendo parte do fluxo de aprendizagem do modelo. Apesar de não ser uma teoria fechada, alguns resultados indicam que o uso de <em>batch normalization</em> reduz o tempo de aprendizado dos modelos <span class="citation" data-cites="ioffe2015">(<a href="bibliografia.html#ref-ioffe2015" role="doc-biblioref">IOFFE; SZEGEDY, 2015</a>)</span>. O passo foi adicionado nos modelos por apresentar bons resulados nas simulações.</p>
<p>Após a aplicação da convolução, também é aplicada a função não linear <strong>ReLU</strong>. A transformação ReLU é a mais simples das funções da ativação, sendo igual à função identidade quando a entrada é positiva e zero caso contrário:</p>
<p><span class="math display">\[
\text{ReLU}(x) = x\mathbb I_{(x&gt;0)}.
\]</span></p>
<p>A função ReLU serve para tornar a arquitetura do modelo uma operação não linear. Qualquer operação não linear poderia ser utilizada, mas a mais simples e mais popular é a ReLU.</p>
<p>Em seguida, aplica-se uma operação para reduzir a dimensão da imagem, chamada <strong><em>max pooling</em></strong>. Trata-se de uma operação que recebe a imagem e um <em>kernel</em>, retornando, para cada janela, o maior valor dos pixels. Usualmente, a técnica também utiliza <em>strides</em> fazendo com que cada pixel seja avaliado apenas uma vez. Por exemplo, para uma matriz com dimensões <span class="math inline">\(M_{10\times10}\)</span> e <em>kernel</em> com dimensões <span class="math inline">\(2\times2\)</span>, o resultado é uma matriz <span class="math inline">\(M^p_{5\times5}\)</span> onde cada elemento é o valor máximo da janela correspondente ao pixel.</p>
<p>A operação <em>max pooling</em> é muito comum no contexto de redes neurais convolucionais. Sua aplicação é importante para que os <em>kernels</em> sejam aplicados em diferentes níveis da imagem de entrada.</p>
<p>A aplicação das camadas convolucionais é repetida três vezes. Ou seja, as seguintes operações são aplicadas a partir da imagem original:</p>
<ol type="1">
<li><em>batch normalization</em>: 6 parâmetros</li>
<li>camada convolucional: 896 parâmetros</li>
<li>ReLU</li>
<li><em>max pooling</em></li>
<li><em>batch normalization</em>: 64 parâmetros</li>
<li>camada convolucional: 18.496 parâmetros</li>
<li>ReLU</li>
<li><em>max pooling</em></li>
<li><em>batch normalization</em>: 128 parâmetros</li>
<li>camada convolucional: 36.928 parâmetros</li>
<li>ReLU</li>
<li><em>max pooling</em></li>
<li><em>batch normalization</em>: 128 parâmetros</li>
</ol>
<p>A dimensão da imagem de entrada, bem como quantidade de canais gerados por cada camada convolucional foram fixadas. Tais números podem ser considerados como hiperparâmetros do modelo, mas foram fixados para facilitar as simulações, que já contam com diversos hiperparâmetros.</p>
<p>A imagem de entrada foi fixada na dimensão <span class="math inline">\(32\times192\)</span>. O valor foi definido dessa forma porque um dos Captchas de referência, da Receita Federal do Brasil (RFB), possui 6 letras e <span class="math inline">\(32*6=192\)</span>. Ou seja, é como se a imagem fosse a colagem lado a lado de 6 imagens <span class="math inline">\(32\times32\)</span>.</p>
<p>A quantidade de canais gerados pelas camadas convolucionais foram fixadas em 32, 64 e 64. A utilização de números crescentes de canais nas camadas convolucionais é comum <span class="citation" data-cites="lecun1998">(<a href="bibliografia.html#ref-lecun1998" role="doc-biblioref">LECUN et al., 1998</a>)</span>, bem como a utilização de números que são potências de 2 <span class="citation" data-cites="lecun2015">(<a href="bibliografia.html#ref-lecun2015" role="doc-biblioref">LECUN; BENGIO; HINTON, 2015</a>)</span>. Nesse sentido, um possível valor para a terceira camada era de 128 canais, mas optou-se por 64 canais para que a quantidade de parâmetros não ficasse grande demais, já que isso exigiria mais tempo de computação e computadores mais poderosos.</p>
<p>O total de parâmetros que podem ser otimizados até o final das camadas convolucionais é 56.646. Esse número pode parecer grande no contexto de modelos estatísticos tradicionais como uma regressão linear, que teria, considerando cada pixel como uma covariável, 4.401 parâmetros (<span class="math inline">\(40\times110\)</span> e o intercepto). No entanto, é uma quantidade relativamente pequena no contexto de redes neurais. Redes neurais recentes aplicadas a imagens, como o DALL-E 2 possui 3,5 bilhões de parâmetros <span class="citation" data-cites="rameshHierarchicalTextConditionalImage2022">(<a href="bibliografia.html#ref-rameshHierarchicalTextConditionalImage2022" role="doc-biblioref">RAMESH et al., 2022</a>)</span>.</p>
<p>Em seguida, o resultado é transformado para um formato retangular, similar ao que se encontra em modelos de regressão. Aqui, as dimensões da imagem não são mais importantes e os pixels de cada canal são tratados como variáveis preditoras. Esse passo pode ser interpretado da seguinte forma: as camadas convolucionais funcionam como um pré-processamento aplicado às imagens, como uma engenharia de variáveis <span class="citation" data-cites="kuhn2019">(<a href="bibliografia.html#ref-kuhn2019" role="doc-biblioref">KUHN; JOHNSON, 2019</a>)</span> otimizada, já que os parâmetros são ajustados no modelo.</p>
<p>Uma vez obtidas as variáveis preditoras com o pré-processamento, é a hora de aplicar as camadas densas. Tais camadas são as mais comuns no contexto de redes neurais. Nesse caso, a operação linear aplicada é uma multiplicação de matrizes, similar ao que é feito em um modelo linear generalizado. Na verdade, o componente sistemático de um modelo linear generalizado é equivalente a uma camada densa com a aplicação de viés, com a função de ativação da fazendo o papel da função de ligação.</p>
<p>Assim como existem os canais das camadas convolucionais, existem os filtros das camadas densas. A quantidade de filtros define a dimensão do vetor de saída. O número de parâmetros da camada densa é igual ao número de itens no vetor de entrada multiplicado pelo número de filtros, somado à quantidade de filtros novamente, por conta do <em>bias</em>. No caso do exemplo, a saída das camadas convolucionais tem dimensão <span class="math inline">\(2\times22\times64\)</span> , ou seja, 64 canais de imagens <span class="math inline">\(2\times 22\)</span>. Com a transformação em vetor, a quantidade de colunas da base passa a ser a multiplicação das dimensões, ou 2.816. No modelo ajustado que foi utilizado como exemplo, aplicou-se 200 filtros na camada densa, totalizando 563.400 parâmetros. Nas simulações, a quantidade de filtros foi variada para produzir modelos com menor ou maior capacidade.</p>
<p>É no contexto da grande quantidade de parâmetros que entra o conceito do <em>dropout</em> <span class="citation" data-cites="baldi2013">(<a href="bibliografia.html#ref-baldi2013" role="doc-biblioref">BALDI; SADOWSKI, 2013</a>)</span>. Trata-se de uma regra de regularização muito simples de implementar, mas que possui grande impacto no ajuste dos modelos. A técnica consiste em selecionar uma amostra dos parâmetros em uma das camadas e apagá-los, forçando que os valores sejam fixados em zero. Na prática, essa técnica obriga o modelo a ser ajustado de forma que amostras aleatórias dos parâmetros sejam boas para predizer a variável resposta. Quando o modelo ajustado é usado para inferências, o <em>dropout</em> é desativado e o modelo pode utilizar todos os parâmetros, obtendo-se, na prática, uma média ponderada das predições de cada sub-modelo. Dessa forma, o dropout tem um efeito similar à aplicação da técnica de <em>bagging</em> <span class="citation" data-cites="galar2011">(<a href="bibliografia.html#ref-galar2011" role="doc-biblioref">GALAR et al., 2011</a>)</span>, muito utilizada na área de árvores de decisão.</p>
<p>O <em>dropout</em> é aplicado após a finalização das camadas convolucionais. Em seguida, vem a primeira camada densa, um ReLU e um <em>batch normalization</em>. Depois, é aplicada mais um <em>dropout</em> e mais uma camada densa. Com isso, a aplicação de operações é finalizada. O total de parâmetros na configuração do modelo apresentado foi de 630.496. Os modelos mais simples utilizados nas simulações, com 100 filtros na camada densa, têm 343.696. Os mais complexos, com 300 filtros na camada densa, têm 917.396 parâmetros.</p>
<p>Para finalizar a arquitetura do modelo, as quantidades resultantes devem ser ajustadas ao formato da variável resposta. O número de filtros da segunda camada densa precisa ser escolhido cuidadosamente, pois deve ser igual à multiplicação das dimensões da variável resposta. No caso do TJMG, os rótulos têm comprimento igual a 5 e vocabulário de comprimento 10 (algarismos arábicos), organizados em uma matriz <span class="math inline">\(5\times10\)</span>, com 50 entradas. Por isso, a quantidade de filtros da última camada densa também é 50, e o vetor de saída é formatado para uma matriz de dimensão <span class="math inline">\(5\times10\)</span>.</p>
<p>No final, o resultado precisa ser normalizado para que fique no mesmo escopo de variação da resposta. A resposta possui apenas zeros e uns, sendo que cada linha da matriz tem somente um número “1”, correspondendo ao índice do rótulo no alfabeto e, nas outras entradas, o valor zero. A saída do modelo deve, portanto, apresentar números entre zero e um que somam 1 em cada linha.</p>
<p>Isso é feito através da função <em>softmax</em>, aplicada a cada linha da matriz de saída. A função softmax é uma normalização que utiliza a função exponencial no denominador, forçando que a soma dos valores do vetor seja um.</p>
<p><span class="math display">\[
\text{soft}\max(y_i) = \frac{e^{y_i}}{\sum_{j=1}^{|\mathcal A|} e^{y_j}}
\]</span></p>
<p>No exemplo, a saída do modelo é a matriz abaixo:</p>
<p><span class="math display">\[
\hat{\mathbf z} = \left[\begin{array}{rrrrrrrrrr}
  -17.5 &amp; -13.5 &amp; -15.4 &amp; -6.6 &amp; -9.9 &amp; 9.9 &amp; -11.4 &amp; -10.9 &amp; -11.8 &amp; -9.3 \\
  -10.9 &amp; -15.6 &amp; 8.3 &amp; -6.5 &amp; -11.0 &amp; -10.3 &amp; -10.0 &amp; -5.8 &amp; -11.4 &amp; -15.1 \\
  -10.5 &amp; -13.6 &amp; -9.6 &amp; -11.4 &amp; 11.2 &amp; -14.3 &amp; -9.9 &amp; -11.3 &amp; -9.9 &amp; -10.0 \\
  -18.1 &amp; -9.6 &amp; -10.9 &amp; 5.3 &amp; -10.1 &amp; -6.6 &amp; -15.5 &amp; -13.3 &amp; -6.8 &amp; -10.8 \\
  -11.3 &amp; -8.7 &amp; 6.4 &amp; -7.0 &amp; -6.1 &amp; -9.2 &amp; -18.9 &amp; -10.3 &amp; -16.1 &amp; -9.6 \\
\end{array}\right].
\]</span></p>
<p>Note que a matriz apresenta valores negativos e positivos. Na primeira linha, por exemplo, o valor positivo está na sexta coluna, correspondendo ao algarismo “5”. De fato, esse é o valor do primeiro elemento do rótulo para esta imagem. Após a aplicação do <em>softmax</em>, a matriz de predições obtida é a matriz abaixo. O modelo de exemplo aparenta ter confiança nas respostas, já que dá probabilidades bem altas para alguns valores e quase zero para outros valores.</p>
<p><span class="math display">\[
\hat{\mathbf y}\times 1000 = \left[\begin{array}{rrrrrrrrrr}
  0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 1000.0 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 \\
  0.00 &amp; 0.00 &amp; 1000.0 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 \\
  0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 1000.0 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 \\
  0.00 &amp; 0.00 &amp; 0.00 &amp; 999.99 &amp; 0.00 &amp; 0.01 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 \\
  0.00 &amp; 0.00 &amp; 999.99 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.01 &amp; 0.00 &amp; 0.00 \\
\end{array}\right].
\]</span></p>
<p>Vale notar que, dependendo da implementação, nem sempre é necessário aplicar a função <em>softmax</em>. Em alguns pacotes computacionais como o <code>torch</code><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, utilizado nesta tese, a normalização pode ser feita diretamente na função de perda, que aproveita a expressão completa para realizar algumas simplificações matemáticas e, com isso, melhorar a precisão das computações. O uso da função de perda ficará claro na próxima subseção.</p>
</section>
<section id="perda" class="level4">
<h4 class="anchored" data-anchor-id="perda">Perda</h4>
<p>A função de perda utilizada em um problema de classificação deve levar em conta as probabilidades (ou log-probabilidades) associadas aos rótulos. A perda deve ser pequena se a probabilidade associada ao rótulo correto for alta e a perda deve ser grande se a probabilidade associada ao rótulo correto for baixa.</p>
<p>Uma função de perda natural e popular nesse sentido é a de entropia cruzada, ou <em>cross-entropy</em>. Trata-se de uma perda com a formulação</p>
<p><span class="math display">\[
\ell(g(x), y) = -\sum_{i=1}^c \mathbb I(y=i)\log(g_i(x)),
\]</span></p>
<p>em que <span class="math inline">\(g_i(x)\)</span> é a probabilidade dada ao rótulo <span class="math inline">\(i\)</span> pela função <span class="math inline">\(g\)</span>. Se o rótulo <span class="math inline">\(i\)</span> é diferente do rótulo correto <span class="math inline">\(y\)</span>, a função de perda vale zero por conta da função indicadora. Quando <span class="math inline">\(i=y\)</span>, a perda é igual ao oposto do logaritmo da probabilidade associada ao rótulo <span class="math inline">\(i\)</span>. Quanto menor a probabilidade, maior o valor da perda.</p>
<p>Ao trabalhar com o oráculo, a entropia cruzada passa a não fazer sentido nos casos em que o modelo inicial erra. Por isso, a função de perda terá de ser adaptada no método WAWL.</p>
</section>
<section id="sec-otimizador" class="level4">
<h4 class="anchored" data-anchor-id="sec-otimizador">Otimizador</h4>
<p>O otimizador utilizado para os modelos ajustados na tese foi o ADAM <span class="citation" data-cites="kingmaAdamMethodStochastic2017">(<a href="bibliografia.html#ref-kingmaAdamMethodStochastic2017" role="doc-biblioref">KINGMA; BA, 2017</a>)</span>. A sigla significa <em>Adaptive Moment Estimator</em> e funciona como uma extensão da descida de gradiente estocástica <span class="citation" data-cites="lecun2012">(<a href="bibliografia.html#ref-lecun2012" role="doc-biblioref">LECUN et al., 2012</a>)</span>, atualizando os parâmetros da seguinte forma:</p>
<p><span class="math display">\[
\begin{array}{cl}
m_{\theta}^{(t+1)} &amp;\leftarrow \beta_1m_{\theta}^{(t)} + (1-\beta_1)\nabla_\theta L^{(t)} \\
v_{\theta}^{(t+1)} &amp;\leftarrow \beta_2v_{\theta}^{(t)} + (1-\beta_2)(\nabla_\theta L^{(t)})^2 \\
\hat{m}_{\theta} &amp;= \frac{m_\theta^{(t+1)}}{1-\beta_1^t} \\
\hat{v}_{\theta} &amp;= \frac{v_\theta^{(t+1)}}{1-\beta_2^t} \\
\theta^{(t+1)} &amp;\leftarrow \theta^{(t)} - \eta \frac{\hat{m}_{\theta}}{\sqrt{\hat{v}_{\theta}} + \epsilon},
\end{array}
\]</span></p>
<p>onde <span class="math inline">\(m\)</span> e <span class="math inline">\(v\)</span> são médias moveis para atualização dos parâmetros, ponderando a perda e a perda ao quadrado com o passo anterior usando pesos <span class="math inline">\(\beta_1\)</span> e <span class="math inline">\(\beta_2\)</span>, respectivamente. Nessa notação <span class="math inline">\(\eta\)</span> é a taxa de aprendizado, um hiperparâmetro a ser ajustado. Por último, o valor de <span class="math inline">\(\epsilon\)</span> é uma constante, usualmente pequena, para evitar divisão por zero.</p>
</section>
</section>
<section id="aprendizado-estatístico" class="level3">
<h3 class="anchored" data-anchor-id="aprendizado-estatístico">Aprendizado estatístico</h3>
<p>Apresentados o objeto de estudo, as redes neurais utilizadas e a proposta da pesquisa, passa-se a discutir o significado disso tudo no contexto de aprendizado estatístico. Essa parte foi escrita para proporcionar a base teórica e a notação para apresentar as propriedades do modelo WAWL.</p>
<p>O aprendizado fracamente supervisionado pode ser dividido em três tipos principais. A supervisão com erros, a supervisão com rótulos incompletos e a supervisão de grupos de observações. O caso do Captcha pode ser entendido como uma sub-área do aprendizado fracamente supervisionado com rótulos incompletos chamada aprendizado com dados parcialmente rotulados (PLL), já que uma parte da base pode ser anotada sem erros e uma parte da base é a resposta do oráculo indicando uma lista de rótulos possíveis incluindo o correto.</p>
<p>A área de PLL não é nova <span class="citation" data-cites="grandvalet2002">(<a href="bibliografia.html#ref-grandvalet2002" role="doc-biblioref">GRANDVALET, 2002</a>)</span> e aparece com outros nomes, como aprendizado com rótulos ambíguos <span class="citation" data-cites="hullermeier2006">(<a href="bibliografia.html#ref-hullermeier2006" role="doc-biblioref">HÜLLERMEIER; BERINGER, 2006</a>)</span> e aprendizado de rótulos em superconjuntos (<em>superset-label learning)</em> <span class="citation" data-cites="liu2012">(<a href="bibliografia.html#ref-liu2012" role="doc-biblioref">LIU; DIETTERICH, 2012</a>)</span>. Um caso particular de PLL, aplicável ao tema do Captcha são rótulos complementares <span class="citation" data-cites="ishida2017">(<a href="bibliografia.html#ref-ishida2017" role="doc-biblioref">ISHIDA et al., 2017</a>)</span>, que considera os chutes errados na notação do problema.</p>
<p>As definições seguem uma terminologia adaptada a partir da leitura de <span class="citation" data-cites="jin2002">JIN; GHAHRAMANI (<a href="bibliografia.html#ref-jin2002" role="doc-biblioref">2002</a>)</span>, <span class="citation" data-cites="cour2011">COUR; SAPP; TASKAR (<a href="bibliografia.html#ref-cour2011" role="doc-biblioref">2011</a>)</span> e <span class="citation" data-cites="feng2020">FENG et al. (<a href="bibliografia.html#ref-feng2020" role="doc-biblioref">2020a</a>)</span>. Sempre que possível, os casos são adaptados para o problema do Captcha diretamente. Quando necessário, apresenta-se primeiro a definição genérica e depois a formulação para o Captcha.</p>
<p>Em um problema de aprendizado supervisionado tradicional, tem-se um conjunto de casos rotulados <span class="math inline">\(S=\{(\mathbf x_i,y_i), i=1,\dots, m\}\)</span> com uma distribuição <span class="math inline">\(p(\mathbf X,Y)\)</span> desconhecida, onde <span class="math inline">\(\mathbf X\in \mathcal X\)</span> é uma imagem e <span class="math inline">\(\mathbf Y\)</span> é o rótulo, que possui <span class="math inline">\(|A|^L\)</span> possíveis valores. O objetivo é obter um classificador <span class="math inline">\(g\)</span> que leva um valor de <span class="math inline">\(\mathbf x\)</span> para o rótulo correto <span class="math inline">\(\mathbf y\)</span>.</p>
<p>Para delimitar se o resultado da aplicação do classificador está bom ou ruim, utiliza-se uma função de perda. No caso do Captcha, como o interesse é simplesmente acertar o rótulo inteiro (não importa se o classificador acerta só uma parte do rótulo), utiliza-se uma função chamada 0-1:</p>
<p><span id="eq-perda"><span class="math display">\[
\mathcal L(g(\mathbf x),\mathbf y) = \mathbb I (g(\mathbf x) \neq \mathbf y),
\tag{2.1}\]</span></span></p>
<p>em que <span class="math inline">\(\mathbb I(\cdot)\)</span> é uma função indicadora. Como a função de perda é aplicada a apenas um par <span class="math inline">\((\mathbf x,y)\)</span>, define-se formalmente que o objetivo do problema de aprendizado é minimizar o <em>risco</em>, que é o valor esperado da função de perda:</p>
<p><span id="eq-risco"><span class="math display">\[
\mathcal R(g) = \mathbb E_{p(\mathbf X,Y)}[\mathcal L(g(\mathbf X),Y)].
\tag{2.2}\]</span></span></p>
<p>A função de risco, no entanto, não é observada, já que depende da distribuição desconhecida de <span class="math inline">\(p(\mathbf X,Y)\)</span>. Para lidar com esse problema, usualmente é utilizado um estimador do risco, calculado tanto em bases usadas na validação cruzada quanto na base de teste.</p>
<p><span class="math display">\[
\hat{\mathcal R}(g) = \sum_{i=1}^n \ell(g(\mathbf x),y))
\]</span></p>
<p>Na base de teste, utilizada para estimar o risco, a função de perda 0-1 é apropriada. Na etapa de validação cruzada de um modelo de aprendizado profundo, é útil considerar uma aproximação da função de perda que seja contínua e derivável, funcionando como uma versão suavizada da perda 0-1. A partir de um vetor de parâmetros <span class="math inline">\(\boldsymbol \theta\)</span> originados da arquitetura do modelo, uma escolha de função de perda é a entropia cruzada, como mostrado anteriormente. Os parâmetros são estimados a partir de um otimizador, como o ADAM, apresentado na <a href="#sec-otimizador"><span>Seção&nbsp;2.1.2.3</span></a>.</p>
<p>As definições começam a precisar de ajustes quando <span class="math inline">\(y\)</span> deixa de ser um rótulo fixado. Como descrito na <a href="#sec-oraculo"><span>Seção&nbsp;2.1.1.3</span></a>, a base de dados observada contém tanto rótulos observados de forma exata quanto rótulos apenas parcialmente informados. Nesse caso, os dados são gerados por uma distribuição</p>
<p><span class="math display">\[
p(\mathbf X,\mathbf Y,\bar{\mathbf Y})=p(\mathbf X, \mathbf Y)p(\bar{\mathbf Y}|\mathbf X,\mathbf Y),
\]</span></p>
<p>em que <span class="math inline">\(\bar{\mathbf Y}\)</span> é um conjunto de rótulos <em>incorretos</em>. Nesse caso observam-se, além das instâncias <span class="math inline">\((\mathbf x_i,\mathbf y_i)\)</span> quando o modelo inicial acerta, as instâncias <span class="math inline">\((\mathbf x_j, \bar{\mathbf {y}}_j)\)</span> quando o modelo inicial erra. Supondo que <span class="math inline">\(\bar{\mathbf Y}\)</span> é condicionalmente independente de <span class="math inline">\({\mathbf Y}\)</span> dado <span class="math inline">\({\mathbf X}\)</span>, temos que</p>
<p><span class="math display">\[
p(\mathbf X,\mathbf Y,\bar{\mathbf Y})=p(\mathbf X, \mathbf Y)p(\bar{\mathbf Y}|\mathbf Y).
\]</span></p>
<p>No caso dos Captchas, essa suposição é verificada. A probabilidade do modelo inicial errar depende apenas do rótulo e não das distorções realizadas pela imagem gerada a partir do rótulo. Além disso, a partir do modelo inicial, é possível estimar os valores de <span class="math inline">\(p(\bar{\mathbf Y}|\mathbf Y)\)</span> a partir da base de teste utilizada para medir a acurácia do modelo.</p>
<p>Nos casos em que <span class="math inline">\(|\hat{\mathbf Y}|=1\)</span>, as probabilidades <span class="math inline">\(p(\bar{\mathbf Y}|\mathbf Y)\)</span> podem ser organizadas em uma matriz de transição <span class="math inline">\(\mathbf Q\)</span>, contendo as probabilidades de se obter um rótulo incorreto para cada possível valor do rótulo. Isso acontece nos Captchas em que não é possível realizar múltiplos chutes. Para resolver problemas desse tipo, é possível realizar um ajuste na função de predição que a torna a função de perda consistente e com taxa de convergência conhecida <span class="citation" data-cites="yu2018">(<a href="bibliografia.html#ref-yu2018" role="doc-biblioref">YU et al., 2018</a>)</span>:</p>
<p><span class="math display">\[
f_{\text{adj}} (\mathbf X) = \mathbf Q ^{\top}f(\mathbf X)
\]</span></p>
<p>O tipo de problema apresentado acima é conhecido como <em>biased complementary label</em>, ou seja, rótulo complementar com viés. Também é possível considerar um caso sem viés, ou seja, quando <span class="math inline">\(p(\bar{\mathbf Y}|\mathbf Y) = \frac{1}{c-1}\)</span> para todos os valores de <span class="math inline">\(\mathbf Y\)</span>. Esse caso também foi resolvido do ponto de vista teórico<span class="citation" data-cites="ishida2017">(<a href="bibliografia.html#ref-ishida2017" role="doc-biblioref">ISHIDA et al., 2017</a>)</span>. As conclusões são parecidas, ou seja, é possível encontrar taxas de convergência para que o problema com rótulos complementares se aproxime de um problema com observações completas.</p>
<p>Quando os rótulos complementares não apresentam viés, existe ainda uma extensão para rótulos complementares múltiplos <span class="citation" data-cites="feng2020a">(<a href="bibliografia.html#ref-feng2020a" role="doc-biblioref">FENG et al., 2020b</a>)</span>. Neste caso, é possível derivar uma função de risco empírica que, novamente, converge para a função de risco do problema completamente supervisionado, além de apresentar taxas de convergência para essa função de risco.</p>
<p>O caso do oráculo e dos Captchas é um problema com múltiplos rótulos complementares e com viés. Até o momento, não existe uma solução geral para este tipo de problema. No entanto, espera-se que as soluções para problemas desse tipo tenham taxas de convergência mais estreitas do que o caso de rótulos complementares, com ou sem viés, já que rótulos complementares múltiplos trazem mais informação do que rótulos complementares simples.</p>
</section>
</section>
<section id="sec-wawl" class="level2">
<h2 class="anchored" data-anchor-id="sec-wawl">Método WAWL</h2>
<p>O método WAWL (<em>Web Automatic Weak Learning</em>) é a solução proposta na pesquisa. Trata-se da técnica baixar dados da web para compor parte da amostra que é utilizada no ajuste do modelo.</p>
<p>O método WAWL é inovador por dois motivos. Primeiro, porque o método faz a ponte entre áreas que até o momento eram partes separadas do ciclo da ciência de dados: a raspagem de dados e o aprendizado estatístico. Além disso, o método é uma nova alternativa para resolver Captchas com pouca ou nenhuma intervenção humana.</p>
<p>Existem duas formas principais de aplicar o método WAWL. A primeira criando novas bases de treino a partir de um modelo inicial e atualizando os modelos com os dados baixados. A segunda é baixando os dados dentro do próprio ciclo de ajuste do modelo, acessando a web no momento de construção de um <em>minibatch</em>.</p>
<p>A arquitetura do modelo WAWL pode ser a mesma de um modelo ajustado com uma base completamente anotada. O modelo pode, inclusive, aproveitar os parâmetros já ajustados em uma eventual versão inicial do modelo para acelerar o aprendizado. Nada impede, no entanto, que uma arquitetura diferente seja utilizada, desde que a entrada seja uma imagem e a saída seja uma matriz com as dimensões da variável resposta. O WAWL é agnóstico à arquitetura do modelo.</p>
<p>A função de perda deve ser adaptada para considerar a informação limitada fornecida pelo oráculo. Quando o rótulo fornecido pelo modelo está correto, a informação é considerada normalmente, através da função de perda da regressão multinomial multivariada. Já quando o rótulo fornecido pelo modelo é incorreto, a função de perda é calculada com base na probabilidade do rótulo estar incorreto:</p>
<p><span class="math display">\[
1 - p(\mathbf y|\boldsymbol \theta),
\]</span></p>
<p>Considerando o rótulo complementar <span class="math inline">\(\bar y\)</span> e a função <span class="math inline">\(\hat f\)</span> dada pela rede neural, a fórmula para descrever a função de perda é descrita da seguinte forma:</p>
<p><span class="math display">\[
l(\bar y, \hat f(\mathbf x)) = -\log\left[1 - \sum_{y}\hat {f_y}(\mathbf x) \mathbb I(y=\bar y)\right]
\]</span></p>
<p>A função de perda proposta pode ser explicada de maneira intuitiva através de um exemplo. Considere um problema com apenas <span class="math inline">\(c\)</span> possíveis valores para o rótulo (ou seja, uma resposta multinomial, sem ser multivariada). Considere também que a rede neural retorna uma alta probabilidade, por exemplo, <span class="math inline">\(0.99\)</span>, para o valor <span class="math inline">\(i\)</span>, que o oráculo identificou como incorreta. Nesse caso, a função de perda é dada por</p>
<p><span class="math display">\[
l(i,\hat f(\mathbf x)) = -\log\left[1-\hat {f_i}(\mathbf x)\right] = -\log\left[1-0.99 \right] = 4.61
\]</span></p>
<p>Como é possível ver no exemplo, quanto maior a probabilidade dada a um rótulo identificado como incorreto pelo oráculo, mais a função de perda penaliza essa predição. Dessa forma, a função de perda consegue incorporar completamente a informação dada pelo oráculo.</p>
<p>Quando o Captcha aceita múltiplos chutes, a mesma conta é válida, bastando subtrair as probabilidades de todos os rótulos incorretos:</p>
<p><span class="math display">\[
l(\bar {\mathbf y}, \hat f(\mathbf x)) = -\log\left[1 - \sum_{y}\hat {f_y}(\mathbf x) \mathbb I(y \in \bar {\mathbf y})\right]
\]</span></p>
<p>No final, o valor que é passado para a função de perda é a soma das perdas para todas as observações do <em>minibatch</em>. A soma considera tanto as perdas calculadas com base nos rótulos corretos quanto as perdas calculadas com base nos rótulos incorretos.</p>
<p>O otimizador que obtém novas estimativas dos parâmetros também não precisa ser modificado. Basta aplicar a mesma técnica utilizada na modelagem usual, como descida de gradiente estocástica ou métodos adaptativos, como o ADAM.</p>
<p>Um detalhe importante sobre o método é sobre a implementação. Com a utilização de ferramentas que fazem diferenciação automática como o <em>torch</em> e o <em>TensorFlow</em><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, basta implementar a parte da arquitetura, a função de perda e especificar o otimizador, já que o processo de atualização dos parâmetros é feito automaticamente. No entanto, dependendo da implementação, não é possível fazer a atualização dos parâmetros usando o componente de computação gráfica, que potencialmente acelera o ajuste dos modelos de forma significativa. Na implementação atual, a função de perda apresentada não permite utilização desse componente, sendo uma melhoria possível em futuros trabalhos.</p>
<p>O ajuste dos modelos, tanto para simulações quanto para construção dos modelos finais, utilizou o pacote <code>{torch}</code> <span class="citation" data-cites="torch">(<a href="bibliografia.html#ref-torch" role="doc-biblioref">FALBEL; LURASCHI, 2022</a>)</span>, que é uma implementação do PyTorch para a linguagem de programação R <span class="citation" data-cites="rcran">(<a href="bibliografia.html#ref-rcran" role="doc-biblioref">R CORE TEAM, 2021</a>)</span>. O pacote <code>{luz}</code> <span class="citation" data-cites="luz">(<a href="bibliografia.html#ref-luz" role="doc-biblioref">FALBEL, 2022a</a>)</span> foi utilizado para organizar as funções de perda e hiperparâmetros, enquanto o pacote <code>{torchvision}</code> <span class="citation" data-cites="torchvision">(<a href="bibliografia.html#ref-torchvision" role="doc-biblioref">FALBEL, 2022b</a>)</span> foi utilizado para utilidades no tratamento de imagens.</p>
</section>
<section id="dados" class="level2">
<h2 class="anchored" data-anchor-id="dados">Dados</h2>
<p>Nesta seção, descreve-se em detalhes como foi a obtenção dos dados para realizar a pesquisa. Como comentado anteriormente, a base foi construída do zero para os fins do projeto, sendo uma parte significativa dos esforços para chegar nos resultados.</p>
<p>No total, foram construídas bases de dados de dez Captchas que estavam disponíveis publicamente no período de realização da pesquisa. Os Captchas foram revisados pela última vez no dia 14/09/2022, para verificar se ainda estavam ativos. Além disso, foram construídas duas bases de dados de Captchas desenvolvidos internamente para fins de teste.</p>
<p>Parte dos dados foram obtidos como um passo intermediário das simulações. A presente seção descreve como os robôs de coleta foram construídos, bem como a metodologia para obter rótulos via classificação manual. Na subseção de dados da seção de simulação, é possível acessar informações sobre os dados baixados para realizar as simulações.</p>
<section id="escolha-dos-captchas-analisados" class="level3">
<h3 class="anchored" data-anchor-id="escolha-dos-captchas-analisados">Escolha dos Captchas analisados</h3>
<p>Para selecionar os Captchas, foram adotados alguns critérios objetivos. Os critérios foram:</p>
<ol type="1">
<li>O site acessado é de um serviço público (governo federal, tribunal, etc).</li>
<li>O Captcha contém letras (A a Z) e números (0 a 9) em uma imagem com extensão <code>jpeg</code> ou <code>png</code>.</li>
<li>O comprimento do Captcha é fixo, ou seja, dois Captchas da mesma origem devem ter sempre o mesmo comprimento.</li>
</ol>
<p>A primeira restrição para escolha dos Captchas é de ordem principiológica. Um serviço público não deveria restringir o acesso aos dados para robôs. Como já discutido anteriormente, nesses casos, a existência do Captcha não tem como finalidade dar maior segurança ao serviço prestado, mas sim limitar o acesso aos servidores por robôs.</p>
<p>As restrições 2 e 3 foram escolhidas com o objetivo de facilitar as simulações para obtenção dos resultados. Em princípio, nada impede que os modelos desenvolvidos trabalhem com outros tipos de rótulos, desde que exista uma lista prévia de rótulos. Além disso, é possível realizar adaptações no pré-processamento base de dados para lidar com diferentes comprimentos de Captchas.</p>
<p>A <a href="#tbl-lista-captcha">Tabela&nbsp;<span>2.1</span></a> mostra os Captchas trabalhados. Dos 10 exemplos trabalhados, 6 têm origem em tribunais, que são conhecidos por não disponibilizarem os dados de forma aberta.</p>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-lista-captcha" class="anchored">

<div class="tabwid"><style>.cl-b7f5011a{}.cl-b7e4f69e{font-family:'Times New Roman';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b7ec68ac{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 2;background-color:transparent;}.cl-b7ec8c92{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b7ec8c93{width:1.5in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b7ec8c9c{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b7ec8c9d{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b7ec8c9e{width:1.5in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b7ec8ca6{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b7ec8ca7{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b7ec8ca8{width:1.5in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b7ec8cb0{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class="cl-b7f5011a"><caption>Tabela&nbsp;2.1:  Lista de captchas analisados. </caption><thead><tr style="overflow-wrap:break-word;"><th class="cl-b7ec8c92"><p class="cl-b7ec68ac"><span class="cl-b7e4f69e">Captcha</span></p></th><th class="cl-b7ec8c93"><p class="cl-b7ec68ac"><span class="cl-b7e4f69e">Exemplo</span></p></th><th class="cl-b7ec8c9c"><p class="cl-b7ec68ac"><span class="cl-b7e4f69e">Descrição</span></p></th></tr></thead><tbody><tr style="overflow-wrap:break-word;"><td class="cl-b7ec8c9d"><p class="cl-b7ec68ac"><a href="https://pje.trf5.jus.br/pje/ConsultaPublica/listView.seam"><span class="cl-b7e4f69e">trf5</span></a></p></td><td class="cl-b7ec8c9e"><p class="cl-b7ec68ac"><img style="vertical-align:baseline;width:65px;height:32px;" src="data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAyAGQBAREA/8QAGwABAAIDAQEAAAAAAAAAAAAAAAQFAgMGAQf/xAAyEAABAwMDAgMHAgcAAAAAAAABAgMEAAURBhIxEyFRYaEHFSJBUnGRFIEjMkJUkpPR/9oACAEBAAA/APv9KUpSlKUpSlKUpSlKUpSlKqIM1C7RGWuUkrMdBUpToznYMk9+a2251blsiLU4pSlMNkqJySSkd6obnqT9QhyBEEpiUpQH8RpQJQMkqTtIPA8jgmqZ+8+7URpL9wmLaWhYI2SsFW8ds7+du7B8qudN39q4uuoYnB1vquHpLK+o2kJTg/EonHP7544qXH1jZ5MlthEtxK3FbU9RpaEk+GT2rbfNVWnTgb95zgyt0EobAK1qA5O0d8edWbElMiM3IbWS24gLSe4yCMj0rnontB07NmNxW7g4266rajrsLaSo/IblDGa6J6SiMwt550NtNgqWtRwABUK1X6Be0OKgSS70yAsFJSRnjsfsfxVhuV9R/NNyvqP5qN7vhf2cb/Sn/lZvvsQoynn3EMsowCpXYJ+Qrlo6Xbjq2Xco6UPCG4htsFzaCNqgcK4PPrSei8KjKtdvkxUSRhXRkvIO1GSrPZO7w71V6Wd6WoNRXG+vNszmUpbkJQAGtp/q7ck4Ax5+J7SbjMe1s6zb7dFcTbm3Qt6W6nHH0+Hz+/birTXEaGvSd0W8w0t9UYttKUgKVkZIAOM+J/NadLaitqdFw3pNyZK4kNCpJU5lSBx3HPl9+1c3erpJ9pIRabLbnRb25CXHbjJG1KduchI/c9uT88c1f6pdceuMeHMiTF2ZG11wxWC4X3Bwg4/lA5P3qJ7PJkMu3JhCQh954upSlBwGxwM4wACewru6VI6aPD1rFcdlxBQ42laTylQyPxUCPp+1RJaZUeGlp1IwChSgMYxwDj0rXdNL2W9OIduEBDziBtSsKUlWPDKSCR5V7C0zZbdBehRre0mO+cuoUSvf9yok1ixpayxZqJkeEGX0HILbi0j/ABBx+2K23fT9rvsZEe5Ri80he9KQ6tHfGM/CRnmoUTQ+moMeQwxaWenIAS6HFKc3AEED4ie2QDjxFYxNDabgXFE+HbuhJQrclTb7iQPLaFbceWMV0HTSPl61Et1pg2mL+mhMdJncV7d6ldzz3JPhUvpo8PWnTR4etZUpSlKUpSlKUpSlKUpSlKUr/9k="></p></td><td class="cl-b7ec8ca6"><p class="cl-b7ec68ac"><span class="cl-b7e4f69e">Tribunal Regional Federal 5</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b7ec8c9d"><p class="cl-b7ec68ac"><a href="https://www4.tjmg.jus.br/juridico/sf/proc_resultado.jsp?comrCodigo=24&amp;numero=1&amp;listaProcessos=50718889720218130024&amp;btn_pesquisar=Pesquisar"><span class="cl-b7e4f69e">tjmg</span></a></p></td><td class="cl-b7ec8c9e"><p class="cl-b7ec68ac"><img style="vertical-align:baseline;width:65px;height:32px;" src="data:image/jpeg;base64,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"></p></td><td class="cl-b7ec8ca6"><p class="cl-b7ec68ac"><span class="cl-b7e4f69e">Tribunal de Justiça de Minas Gerais</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b7ec8c9d"><p class="cl-b7ec68ac"><a href="https://pje-consulta.trt3.jus.br/pje-consulta-api/api/processos/2104879"><span class="cl-b7e4f69e">trt</span></a></p></td><td class="cl-b7ec8c9e"><p class="cl-b7ec68ac"><img style="vertical-align:baseline;width:65px;height:32px;" src="data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAyAGQBAREA/8QAGwABAAMBAQEBAAAAAAAAAAAAAAIDBAUBBgf/xAAvEAACAQMCBQMDAgcAAAAAAAABAgMABBESIQUTMUFhIlGRFDJxofAGIzNSgsHh/9oACAEBAAA/AP3+lKUpSlKUpSlZZZnM/Ji+4DJ/f77VZpuRvzIyf7dBA+c1CCZjIYpDk74z1/B+RvU7mf6a2eXSW0jZR3PYfNSgWVYgJnDydWIGBn2Hio3ImMRMDYkU5AIGG8H81OGVZoUlXOl1BGanWZDLM8h5rIqsVAUDt3OQanbuzIyudTIxUsB18/rV1Ky8Qv4uHW3OkV3JdY0SMZZmY4AGapsZNd3M7KUaREkVSQSAR7gkHB9q6Gcda5ZuRLxJTDFJIqrlmUADoRtk79RU+I3kScMlmOr+UyMU0ktkMMDHk1osrxL6HmxxyoucDmxlCfODVs0yQQtI59K+3U+wHmq7KMw2cUbn1qo1b9+9XswVSxOANyaw6LiUmaEiEPvgndvJ2IBrRaaeTpVWUgkMGOTnvk965/Ev4gtuFO4uba95aacyx2zMm+w9Q8kCuvXI4/wyLiNrAWs47iSK4iZdaBtK8xdfXtpzmt72cZSMRjkmIYjMYA0j2x0x4rj8ctOI3NtBGZYnjEylxHHu43wNLNg742JrfwazNlYaGDB2ZmIZVUjJ2GASAPYZrzisUTovMgacPmNokwGcYJ23HQjPWsnCuHyRTXUkIubK2kKmOKRwx1b6mwdWM7bZ7E1sFrzpn1yyScrYMxGdWO2AMbHr181g4bwk2z2DLC0DfRlZ2Q4JfK41e5+7rmuupM8MsEhGsZRiO+RsfivmbO2u4eMrLdpMswneR5Vt8h1OcKHDbrjG2nbHbrX1Fsr4kkcaTI2Qp6gYAGfO1YOI209/xWwgMZFlCxuZXzszr/TTzudX+IrrUpUJYhNGUJIz3HUHsapWS5jXS8IlI6MjAA/kHp+texRSPLzp9OQMKi7hfffua0Vmt/RNNEdiW1r5B/7/AKrT0FZI3AFxdYLKftx3AHX5zV0EvOQnA2OMqcg/g96tpSlKUpSlKUpSlKUpSlKUpSlK/9k="></p></td><td class="cl-b7ec8ca6"><p class="cl-b7ec68ac"><span class="cl-b7e4f69e">Tribunal Regional do Trabalho 3</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b7ec8c9d"><p class="cl-b7ec68ac"><a href="http://esaj.tjba.jus.br/cpopg/open.do"><span class="cl-b7e4f69e">esaj</span></a></p></td><td class="cl-b7ec8c9e"><p class="cl-b7ec68ac"><img style="vertical-align:baseline;width:65px;height:32px;" src="data:image/png;base64,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"></p></td><td class="cl-b7ec8ca6"><p class="cl-b7ec68ac"><span class="cl-b7e4f69e">Tribunal de Justiça da Bahia</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b7ec8c9d"><p class="cl-b7ec68ac"><a href="https://www.jucesponline.sp.gov.br/ResultadoBusca.aspx"><span class="cl-b7e4f69e">jucesp</span></a></p></td><td class="cl-b7ec8c9e"><p class="cl-b7ec68ac"><img style="vertical-align:baseline;width:65px;height:32px;" src="data:image/jpeg;base64,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"></p></td><td class="cl-b7ec8ca6"><p class="cl-b7ec68ac"><span class="cl-b7e4f69e">Junta Comercial de São Paulo</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b7ec8c9d"><p class="cl-b7ec68ac"><a href="https://srv01.tjpe.jus.br/consultaprocessualunificada/"><span class="cl-b7e4f69e">tjpe</span></a></p></td><td class="cl-b7ec8c9e"><p class="cl-b7ec68ac"><img style="vertical-align:baseline;width:65px;height:32px;" src="data:image/png;base64,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"></p></td><td class="cl-b7ec8ca6"><p class="cl-b7ec68ac"><span class="cl-b7e4f69e">Tribunal de Justiça de Pernambuco</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b7ec8c9d"><p class="cl-b7ec68ac"><a href="https://www.tjrs.jus.br/site_php/consulta/verificador.php"><span class="cl-b7e4f69e">tjrs</span></a></p></td><td class="cl-b7ec8c9e"><p class="cl-b7ec68ac"><img style="vertical-align:baseline;width:65px;height:32px;" src="data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD//gA+Q1JFQVRPUjogZ2QtanBlZyB2MS4wICh1c2luZyBJSkcgSlBFRyB2NjIpLCBkZWZhdWx0IHF1YWxpdHkK/9sAQwAIBgYHBgUIBwcHCQkICgwUDQwLCwwZEhMPFB0aHx4dGhwcICQuJyAiLCMcHCg3KSwwMTQ0NB8nOT04MjwuMzQy/9sAQwEJCQkMCwwYDQ0YMiEcITIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIy/8AAEQgAMgBkAwEiAAIRAQMRAf/EABsAAAIDAQEBAAAAAAAAAAAAAAAHAwUGAgQB/8QAMRAAAgEDAwIGAAUEAwEAAAAAAQIDBBEhAAUSBjETFCJBUWEWI0JScRUygZFicqHw/8QAGAEAAwEBAAAAAAAAAAAAAAAAAAEDAgT/xAArEQABAwMCAwgDAQAAAAAAAAABAAIRAxIhMfBBUdEiYXGBkbHB8QQFM6H/2gAMAwEAAhEDEQA/AH/o0iD1XvBJqIdwrkWMlTA9RIbBj6SzsBYkE4I9j72OpKnft4SneT+obnC4Abj51iXJfjZAcWvYXzYAk/erHX2AaJgYkp56NIiPqHckoGq23jc5ZObEotW5U3OQBiwXABHwRb3M9N1FuUcSRNvlQY7ckeWre55B+7EhiBxVc2sT3v3GtJJHJDQSHE4jmnho0jqfe99Q8Du1eQKhF8VZndDckhQxYi5HpsfcWBJvopt53yamlkk3Op4CRmSQVc4VU7+oYb3sAB2BN8WKjOU7TMJ46NI09V7j4k7HdNwjdZVK8pyVLm/NbC9gt7+18C2DflOodxQGafedxh5euKRJ3ZHu3EkhibDiLBcZ5EG9tZGdQmWECU9NGksOpN0dHSDca6RmlHpaZiVyFOVyBgkDIJJA7YE3jdBUz07bhuhN1VCs8zCwbKg3ux9Nr4JLEfenBiSptlwJ5J06NKOWo3CnM1FL1lVw1pi9UKo8zwAjubE2Ivk/5FsajrajqPawVqN4nkgAPl5TXFVYD9xsGLWt797k3GNInBKV7JiU4NGkzT7x1I1LCwqdzyg9ScpQ/ty5Fh3+hbRqRqunAWTUgwFnaN6UwlKmNVZQ0cckjiTGCTYG/ICxxixGde9i0UscqrN4wkMKNZgYx7nkCGzYDl3v3BtcxmaGObwZZZZSxv4yRIxRbEoUDAEjIAv25D+RxJJUvRtPElREjSqrxXBnjAbI75ODgA+33qrrCSZJPhr36+6u4AuIfjy5bwtJJFtW3dJVbU88TQidPEmCjiOwyTm37jbuTj21nnpXiemkFOz1Sc28UxGyhlZQq/OWxgWAuSLgC+WpjPSdbPBD4CCdYka5uBYAkk5Gb3t8HOqBXIglozXFmEbPCFXmWJVr+rjlvSbHsbdux0QXNlvBSo1Wi5hk56ZU1DBPTVCpXJO9LLGzKyAMHfkTxdQMYJsT/wCXxYbnSvU7lNNEGnfy/hWMnERk5Vvm3f4znOqimqKeOoikeZqZIqcNKir+W1j/AGqTc+oMGsBlvcjtY7+Q9bJCDGhaJW5NETe18kgW7MBnIuTnRJbUaXCJ3KnVqMb+S3skgg8fCeC8+2QT/iSkqEWlYtUBZ2BDBlNmsrZLYbtxAzgdiNPWVe/NX1y0RvCkhaF1pBJwC8SFKhL5u3vyHt2Os3QVj0y0beAprFZJVaoRYzhgfULXGQRcfqAvawsbnve5VgigMQj8Nzb+nymKMMTzXBcWJ9Vzc3Nvq6dTdUII4desLpq2uN0SN5U26Ve57iqUdRNRsBI0Z8SIcnZQBwUqo42ZrEXJz8jEe1vT0m7wVU0KyywDkzRS80sLjkzMQEHbiALYNyc6uoJZdw6b3ak3CoJkoQainqW/uEZJIU3+AvHke4IIPY6relY0G/REuiU3FRToLjmwBsWAsL3LgYN+P3fQHAiDiFG5rabsA+ucrvdOmYzujVQr6en83crHWTcWOSTbsTlu38G+pN3WGk2Wi2SvkcmSYvCaZil0UYHIgjPKx/7Yta+qvdTPE0qSQCOTnIxldmUxlnBJtnne3ewsAMDNrynikPSlRJI8UHgTA07ByYoip4sVvmx9Rt9gC9r63cTDnjz+VJz2gMcACN8e5ZUzUMMsqGlqJbObGJXZV+rqQDo1PTxVFKjCbdqqbxCHRkjdwEIHEXW47C9r3F9Gix/Aj/V2N/YMaIuHovIk5oql41lkE05iflEytGHH6+OLLi1iLXt8G80yJWQTwU0ngmAi8yL40inncEkYJsCSbHv3+HAOhOmFdXTaYkdbcWjd1IsSRax+Sf8Aevi9BdMLH4Y2pAgAAXxXstlCiw5YIAFiNIkyCFIDmVh0hoW2iooRuO2u9dOsyBJmCuAq8xc3x37WwR/Os/WwtTVrUc01OZacco0y0bA3sFFwPrB+bm9rt09GbAeXGhZOXfw55EtkHFmFsgHGiPozp+KQulBZvnxpMZvj1YzoLnk4MdVinTDHEz9pSQ0qPH4ddXU/KMJ+TYuo43uCO3K59jezWFrY9e+jbfN+dnqVkWJHXwU5AuQDdQyjJxb7tb7DM/AvTRphTHbFaEcvQ0rkHkADe7ZwP8e2p/wnsoZWFI6so4grUSAgfGG/+sPgamWuLpHDe8pPoB1UVLjj51SgoKut2XdvMxQ+ZjpgryzKoZuBB8Qcv03svo/4g5I1e1e0w1tdLuG11FDNQ1MolZJJQjQEXDWV8D1cTYAC/f51vpeiunpp1megPiKCAVnkXve/Zvfkf96jfoPpmRCj7UjAqVu0rkgH2B5XA7f6Gq6EuaYJ9la4t/njosDUT0m0dI10NHufmaiRzTyTUqCQRqOTcL5HsQSe1z2FtUUq1EG3R19BNFTSOjmV2kZjGLNxIAOQcnnY2vi3fTdj6I6ehxHRSKvINxFVLxJAIFxyscEi2vv4J6f4hfIuFBvYVMoHe/7u31phtMaIgBwcCRzSvm3fdPLxiVaOuqeTxh56USHIuFDYutrnGbCxz353Kvlq44fMtNUuEkPBPy1RCrBgoFrggYNvf+RpnL0H0ykaou22CyCRfz5Lg/zy7Z7dj8ak/A/TZJJ2qIk3uS7EkYwTftgY7e2sOmOzvKwKbGkFo+0k+NI55JBFPGwV0ZZpUADKGsFUWGSbge99GnbH0T01GvFdnp7fDAm38XOB9aNdTawAiTvzU30rnEg79Vf6NGjXMrI0aNGhCNGjRoQjRo0aEI0aNGhCNGjRoQjRo0aEL//Z"></p></td><td class="cl-b7ec8ca6"><p class="cl-b7ec68ac"><span class="cl-b7e4f69e">Tribunal de Justiça do Rio Grande do Sul</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b7ec8c9d"><p class="cl-b7ec68ac"><a href="https://www.cadesp.fazenda.sp.gov.br/(S(vyfz1cfybbxj3sgpf4eqhxd3"><span class="cl-b7e4f69e">cadesp</span></a></p></td><td class="cl-b7ec8c9e"><p class="cl-b7ec68ac"><img style="vertical-align:baseline;width:65px;height:32px;" src="data:image/jpeg;base64,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"></p></td><td class="cl-b7ec8ca6"><p class="cl-b7ec68ac"><span class="cl-b7e4f69e">Centro de Apoio ao Desenvolvimento da Saúde Pública</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b7ec8c9d"><p class="cl-b7ec68ac"><a href="https://sei.economia.gov.br/sei/modulos/pesquisa/md_pesq_processo_pesquisar.php?acao_externa=protocolo_pesquisar&amp;acao_origem_externa=protocolo_pesquisar&amp;id_orgao_acesso_externo=0"><span class="cl-b7e4f69e">sei</span></a></p></td><td class="cl-b7ec8c9e"><p class="cl-b7ec68ac"><img style="vertical-align:baseline;width:65px;height:32px;" src="data:image/png;base64,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"></p></td><td class="cl-b7ec8ca6"><p class="cl-b7ec68ac"><span class="cl-b7e4f69e">Sistema Eletrônico de Informações - ME</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b7ec8ca7"><p class="cl-b7ec68ac"><a href="https://servicos.receita.fazenda.gov.br/servicos/cnpjreva/Cnpjreva_Solicitacao_CS.asp"><span class="cl-b7e4f69e">rfb</span></a></p></td><td class="cl-b7ec8ca8"><p class="cl-b7ec68ac"><img style="vertical-align:baseline;width:65px;height:32px;" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAAAyCAAAAACPXiFiAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAJcEhZcwAADsMAAA7DAcdvqGQAAAAHdElNRQfmCRkSBQhAxiMyAAAKR0lEQVRYw+2X53OU9xHHv7+nXdOd7nSnXu7UJbCMhBBVEiCBEb15bDNgTJwJJHbGNIexTezYGWrw2NjYFMd2wNhBiDFNAkwvEghEB4G6ECp3aifd6Xp5nl9eEP8HSSYv+L7cndnPzO58Z3cJxX9fzP+A8RzyHPIc8v8CYfdsq3VpVAyhABnyCJT8mqFewtBjJ4x31UoAHr/wLOx5oOPhOfudIYqC9G1QxgOA75duIwDYXAoC+EpJJABA9HGEegnDLHaVbV7w9oFm32DAUf3phft+u/ismlRpAW06dV6pqTzkDnY0iwAg2n29IsBc398dHKAIeHQA0LNhzR0AgK0vSAHUXQkCcNkDDgq4/GDP8TXFC1sPl120+SzKTa7pfJ3O4+DcPEHQw3Hp02NihH8GsxxRQ6fUYcEOsSkiRU5EscKwQg6BsGHpAnB39Y33lgjBGqKRy6qiBEiN3lwwtKxibBgBlDxhN46sOzL9oxlJAycqHg21jS9xykKau4K1MYLU2r3zvi4jQklVBVoVL5Xa88S28ASZQGg/n9fFJQsEktTjCzn9O27PNEHyXyVN2wPxUeIj3fjx9q+TQm3JKVTqFTgC9mPBcPdUbvbwqQv1J7rNvQ0P7PrE8Ih4OaGWxCc6jzceTH+DKhQhLw5TM4xlnyuFQMGr2nQmAJy61n39vbCVxQRVD4bf+nHUlHiF/Xiyhniqueiq8LDasAadHCB+yje8odwfB1pTPb/3QnUrk5M/NllGgIBNxdWGJmOIQsk/G1ODdZC0zkvAo9Ypt4QxAJy/b+t6baWeB1rsQsfBzerzRWqfQIDrJ01kouBM4DyMGpw7qM94f9m2LQokh8ca81xnDnp2bEyfPDYjwr5vUexIf3PnD+FxL0wiAMBkAhcPj0mAIBceChlsCFjh6szVEQAkPetlYqvsdS0CQl9IUz3ZQQS1TAjLyjcNl3MaAEX532qzZCP6o0Wi6rV+7zpQ+bExPCt/GIvKnXd7BxH6QSH5t6Fcxp8iW5GSCta+Stoa9fDmrOVhAL24v9HmdXicz7wXPX69ZGey4z29/RVjMhmAUkp7SsCwUTel6mUbvQONQerbqk+XgdVMWl/MqIG4MvfjFkoppb57ZZ9cH9q60U07Nxzf8npT29SF3aJEad14OQFIRNr4SWNyjDJg1FJWWH9bDNgOmCmlHChluEW1FtHrsFuHPFSnA87usFqB8HEJtxtjdQ9p4kSBa1AIetFxey3nPBU9dZYcYtGYEhGrOg9GAkDkh97aL/qFdSOznX0hPUd2BO8Y9L3XnadNEwr1ANiPH5YnhqXf7PJyJTdu5UVySp7AJTy1QxeVvW7E0qy6NgQsx/fv34eRwYuf3emwdjaVTCFQx9iJ//TnelR32R84o9IzIw4PInVYOiO/VDXunBv5zp62G+dPVLTrjL56DgY1vGJ6BXyfWtsDnN6UPaMkZ3hVO4r/qlebVcOSLyOy8XGfiGvdK19Kae/48tGzvnsPnm972t99G5zOMfwvUxQAmBGF0v5zCUW7+xF11VAUHpJlq91f9XPsYw4xrwW+8Wu8CFTriqYFdjVeO/xBsbcP6Lxlbuwwd9mA+e/WbyvFpaqu6WMKxUOPAEgt/pN/dYIrGe0332gJ9kg8QMGQdpeSXvulCbkfdqQXSi4NxA+2f70lnQPATZXKATBpI5eoX+r47tia6OgOoOYOK/gNU2+0CzFeyQPICrgd21JTOwGAwnHMSRC3cgQpDdX1mGwUnAD/R5usXh+45DmLjIye3reNrM/I312+5DQHgKThAhgppKVQwWdlMecCS7lWBxAMLXDkrlnQLouq+c4MSAsn9t/7vOUJxIujM5NTV9TbxM7FasMT5bI3s/usURwPaiVCmsk8fL3Ruc/yQkpqgmDovuEdlsJwtt5oNcCynI+NLU+fp3SWDxWuch3qR25xXYu57WkLpPLhb9+771cwV74q4Lv9wNGrGYUTxpee+ybXa20bGNi0b+14RAAQPuMqlqdVytMgm7brbWVavOLO4KBysS6XVFfNtsmHbdzMu6D1M+lqV2PY5sntrzyB6WjcGbr3sh8AX7x8Z5U3lLdCFuX2qpx6Ru1Q54h1hzVD5X+eeMk24VAk0zy7Xv5TwVMLHUvDGdvDpC9OOoioSM0bNyEExHsiZYNx5vImELXGzlAPM/2t0Zfdq3rAjl4xf98nfHSGgta3GnRDvEyuaFg7cXDo1NBcx7B1FwB9UX7iuX26Lu2CpFeaT35vZ0aE2T3CnPxxKsvK+XPbOw1EE8sTAJxQfJ8sSMwhy9Qm/Zb5/N0Dt/6W23/fSuTMI6qZHR+XLOvUyh7dqgqaVWlabmwK6+wxTpIGPQCshw6TIIn8zeSrTTVruiFjGsGrUnPyfE9Nr8ilsLSz/AgAkDo40vCVIzKqwPzHK4GEnMId7RvijpW1i1DkkX6dx2QCGtqmsDn6Qrn36uVm89LsLH5eLKSqmZk1thkDtwp8V0nMi5FhEZvMMr03NkSvCQkdqv9h5RxLBwzdzza2/yih90rVq5Un1+097twocRWyKXJz15U2R4dfmmKcGQngTPX7rZX18ycQyddbytU9GNSOHJdy6GXTwjnF3f5Q7SmcH5yx+EXPtdH81tw5AGCuSzS2vjNjVpwIOYCAm3LI1pc9HjVC9XOOyAWlEgWDmJjsgNJmVcq/7omsE4yFecLF/TJutMAoYmYkKXsablduo7zH7MzZLfvT9pfiXl+48+6JeQt6XH8/PgsAqMwUw9omTqrRaYHB+sSmA6sJBfqcRmbP1vWLbHcSh5ldSRwAdFizWLtb8Zn7rSRKBgfYIGvqbyfV00PCCRUtZ3+4BYMmoqQEqTLaOJh2bHdwQUGtf1KcAXC5DYRKJHAtLRYYuiikrp3MvueRqXQEmebtbX25qWyfM5wFAKWeJ/Lm9nEFsa1OrXdA17mn8HFptOX+P7h04ihruan6YjbTeubQw2aHPDwkJG+a7ZuGd9JvJmgATs4QwhA2WiP5ODmVxzKj2Dc7dVYZC8/5CGvFzQ671iQDkUSG5dwuRd37eUnKoKDau0sxxpgQNzEtT3mUCzny5bftxikrsiYQbcngjYq9F5uG7FHTMk40F6ZFPj6i1TMEoCAc43MrxONcckoi8Yvdjane9Mo96xNun63uJfLo+GiWzwrhf7a8O3jK5X0jTI6awHBW9Dv93b1PK/0clzKtKIJo4Pn8yaunYxuu5zc8scUkmuqvf5Wn3HVgyavtrD633ZItH7wzTgm77FLvEkIhwWWLcfcYeWBgfVTEU4vV4haV1MMa/GTIx3BqEMLZ+0IZMAYlQn+bEs8BAKxWd6vO43cvZMo2ltReTctap9DAoiv/UTZqo73fnuk5Pf2uejRn84dzAAO1Gmo1gt1aVUZ+rhQUA3aHr6tbT2vUEeR477KBCzMSek6/nCcjFzpLwpN+vWP5uL7mEk86I8PczMxfUv4Qr/V6tOHimMT4ECHcfkKbMVc62xNr1ALk+Yv9HPIc8hzyn9W/AL93mjMjzDTLAAAAJXRFWHRkYXRlOmNyZWF0ZQAyMDIyLTA5LTI1VDE3OjM3OjE5KzAwOjAwkJqgZQAAACV0RVh0ZGF0ZTptb2RpZnkAMjAyMi0wOS0yNVQxNzozNzoxOSswMDowMOHHGNkAAAAASUVORK5CYII="></p></td><td class="cl-b7ec8cb0"><p class="cl-b7ec68ac"><span class="cl-b7e4f69e">Receita Federal</span></p></td></tr></tbody></table></div>
</div>
</div>
</div>
<p>Além dos Captchas de sites, também foram consideradas imagens geradas artificialmente. O motivo de criar Captchas artificiais é a facilidade de rodar modelos e simulações, já que nos casos reais é necessário ter acesso à internet e também construir bases de dados de cada Captcha.</p>
<p>Foram gerados dois tipos de Captchas artificiais. O primeiro, chamado <strong>MNIST-Captcha</strong>, é simplesmente uma adaptação da conhecida base MNIST para ficar no formato de um Captcha. A partir da escolha do comprimento e dos caracteres que fazem parte da imagem, o gerador simplesmente faz uma amostra aleatória da base do MNIST e compõe as imagens horizontalmente.</p>
<p>A <a href="#fig-captcha-mnist">Figura&nbsp;<span>2.11</span></a> mostra um exemplo do Captcha gerado a partir da base MNIST. No exemplo, o comprimento escolhido para o Captcha foi de 4 valores.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-captcha-mnist" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assets/img/mnist128c49c36e13_6297.png" class="img-fluid figure-img" width="150"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;2.11: Exemplo de MNIST-Captcha</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>O problema do MNIST-Captcha é que a base de dados original é finita. Apesar de possuir por volta de 60 mil observações e de um Captcha crescer em ordem exponencial, o MNIST-Captcha pode gerar Captchas repetidos. Além disso, é necessário tomar cuidado com as bases de treino e teste, já que os elementos de teste não poderiam fazer parte de nenhuma observação de treino.</p>
<p>Pelos motivos supracitados, também foi criado um Captcha gerado inteiramente por programação, chamado <strong>R-Captcha</strong>. O Captcha é gerado utilizando a ferramenta ImageMagick, com a possibilidade de customizar diversos parâmetros, como</p>
<ul>
<li>Quais caracteres usar na imagem</li>
<li>O comprimento do Captcha</li>
<li>Dimensões da imagem</li>
<li>Probabilidade de rotação da imagem</li>
<li>Probabilidade de adicionar um risco entre as letras</li>
<li>Probabilidade de adicionar uma borda nas letras</li>
<li>Probabilidade de adicionar uma caixa (retângulo) em torno das letras</li>
<li>Probabilidade de adicionar um ruído branco no fundo da imagem</li>
<li>Probabilidade de adicionar efeitos de tinta óleo e implosão</li>
</ul>
<p>A <a href="#fig-captcha-r">Figura&nbsp;<span>2.12</span></a> mostra um exemplo de R-Captcha. O exemplo apresenta uma linha ligando as letras, comprimento 4, dígitos maiúsculos e minúsculos e distorções.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-captcha-r" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assets/img/captcha128c4a9c32e4_boy4.png" class="img-fluid figure-img" width="150"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;2.12: Exemplo de MNIST-Captcha</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Por ser uma versão mais flexível e completa, optou-se por trabalhar principalmente com o R-Captcha nas simulações. O MNIST-Captcha foi implementado mas não foi utilizado nas simulações.</p>
</section>
<section id="construção-dos-dados" class="level3">
<h3 class="anchored" data-anchor-id="construção-dos-dados">Construção dos dados</h3>
<p>Para obter os dados da pesquisa, foram utilizadas técnicas de raspagem de dados <span class="citation" data-cites="zhao2017">(<a href="bibliografia.html#ref-zhao2017" role="doc-biblioref">ZHAO, 2017</a>)</span>. A raspagem de dados é uma área da ciência da computação responsável por criar rotinas que automatizam a coleta de dados provenientes da web. Trata-se de uma atividade muito comum em pesquisas aplicadas, especialmente as que envolvem análise de dados públicos que não estão disponíveis de forma aberta, como os dados do Judiciário.</p>
<p>Dentro do ciclo da ciência de dados, pode-se considerar que a raspagem de dados está inserida nas tarefas de coleta e arrumação de dados. De certa forma, é possível comparar a raspagem com uma consulta a um banco de dados remoto, ou mesmo à obtenção de informações através de uma <em>Application Programming Interface</em> (API).</p>
<p>Para raspar uma página da web, usualmente se segue o fluxo descrito na <a href="#fig-fluxo-web-scraping">Figura&nbsp;<span>2.13</span></a>. Nem todos os passos foram seguidos na obtenção dos dados necessários para realizar as simulações, mas é importante conhecê-los para compreender bem a origem da ideia de utilizar raspagem em conjunto com métodos de aprendizado de máquinas. O exemplo da RFB foi utilizado para dar contexto aos passos.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-fluxo-web-scraping" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assets/img/cycle.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;2.13: Ciclo da raspagem de dados. Fonte: <a href="https://curso-r.github.io/main-web-scraping">curso de Web Scraping da Curso-R</a>.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>No caso da RFB, o trabalho é iniciado acessando-se a <a href="http://servicos.receita.fazenda.gov.br/Servicos/cnpjreva/Cnpjreva_Solicitacao.asp">página inicial de busca de CNPJ</a>, como mostrado na <a href="#fig-raspagem-rfb-inicial">Figura&nbsp;<span>2.14</span></a>. É possível notar que o desafio disponível é do tipo <em>hCaptcha</em>, que não é o Captcha de interesse da pesquisa. No entanto, ao clicar em “Captcha Sonoro”, é possível acessar o Captcha de interesse, como mostrado na <a href="#fig-raspagem-rfb-sonoro">Figura&nbsp;<span>2.15</span></a>. O motivo pelo qual o Captcha de texto em imagem foi mantido após a implementação do <em>hCaptcha</em> não foi encontrado.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-raspagem-rfb-inicial" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assets/img/raspagem-rfb-inicial.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;2.14: Página de busca de CNPJ da RFB.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-raspagem-rfb-sonoro" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assets/img/raspagem-rfb-sonoro.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;2.15: Página de busca de CNPJ da RFB, com Captcha de texto.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>A segunda tarefa é a de navegar pelo site, registrando as requisições realizadas pelo navegador para realizar a consulta. Isso envolve abrir o inspetor de elementos do navegador, na aba Rede (ou <em>Network</em>, em inglês), anotando as requisições que são realizadas.</p>
<p>No exemplo, testamos o CNPJ 13.612.840/0001-57, da Associação Brasileira de Jurimetria. Ao preencher o CNPJ e o rótulo do Captcha, algumas requisições aparecem na aba “Rede”, como mostrado na <a href="#fig-raspagem-rfb-rede">Figura&nbsp;<span>2.16</span></a>. A primeira requisição é do tipo POST<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>, responsável por enviar os dados de CNPJ e do rótulo da imagem para o servidor, que retorna com os dados da empresa.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-raspagem-rfb-rede" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assets/img/raspagem-rfb-rede.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;2.16: Resultado da busca por CNPJ, mostrando a aba Rede.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Investigando a requisição POST, na sub-aba “Requisição”, é possível observar os dados da consulta. Trata-se de um conjunto de parâmetros enviados na forma de lista, com as informações abaixo. Para replicar a requisição na linguagem de programação, estes são os dados enviados.</p>
<pre><code>{
    "origem": "comprovante",
    "cnpj": "13.612.840/0001-57",
    "txtTexto_captcha_serpro_gov_br": "7hkhze",
    "search_type": "cnpj"
}</code></pre>
<p>As etapas de replicar, parsear e validar envolvem baixar e processar os dados na linguagem de programação. No caso do Captcha da RFB, essa tarefa envolve os passos abaixo.</p>
<ol type="1">
<li>Acessar a página inicial de <a href="http://servicos.receita.fazenda.gov.br/Servicos/cnpjreva/Cnpjreva_Solicitacao_CS.asp">busca com Captcha sonoro</a>, através de uma requisição GET.</li>
<li>Baixar a imagem do Captcha com uma requisição GET, usando o <a href="http://servicos.receita.fazenda.gov.br/Servicos/cnpjreva/captcha/gerarCaptcha.asp">link gerado</a> ao clicar no botão de atualizar o Captcha.</li>
<li>Obter o rótulo a partir da imagem do Captcha.</li>
<li>Realizar a requisição POST com os dados do exemplo e o rótulo correto da imagem, baixando arquivo resultante em um HTML.</li>
<li>Utilizar técnicas de raspagem de arquivos HTML para obter os dados de interesse (como, por exemplo, a razão social da empresa) e validar os resultados, verificando, por exemplo, se o resultado estava completo e disponível.</li>
</ol>
<p>Todos os passos descritos acima devem ser realizados em uma sessão persistente. Isso significa que a biblioteca utilizada para realizar as requisições deve ser capaz de guardar os <em>cookies</em> entre a requisição GET do primeiro passo e a requisição POST do quarto passo, de forma que as requisições sejam interligadas.</p>
<p>O quinto passo da lista acima descreve a parte de <em>parsear,</em> que é a responsável pelo nome “raspagem” nessa área do conhecimento. O nome é adequado porque usualmente os arquivos baixados estão em um formato bruto, inadequado para realização de análises. Os dados precisam ser então extraídos – raspados – do arquivo HTML, através de ferramentas de transformação de arquivos como a <em>libxml2</em> <span class="citation" data-cites="xml2">(<a href="bibliografia.html#ref-xml2" role="doc-biblioref">WICKHAM; HESTER; OOMS, 2021</a>)</span>, técnicas para acessar pedaços do documento, como o XPath <span class="citation" data-cites="rvest">(<a href="bibliografia.html#ref-rvest" role="doc-biblioref">WICKHAM, 2022a</a>)</span> e técnicas de manipulação de textos, como expressões regulares <span class="citation" data-cites="stringr">(<a href="bibliografia.html#ref-stringr" role="doc-biblioref">WICKHAM, 2022b</a>)</span>.</p>
<p>A iteração encerra o fluxo da raspagem de dados. Nessa etapa, as operações de replicar, parsear e validar o resultado são reaplicadas iterativamente, com o fim de baixar dados para compor uma base maior. No exemplo da RFB, isso significaria montar uma base de dados a partir de uma lista de CNPJs.</p>
<p>No contexto dos Captchas, o interesse está nos passos de Replicar e Validar. Estes são os passos em que a imagem é baixado e o rótulo é anotado e testado no servidor. Esses são os passos relacionados à classificação manual, e também à implementação do oráculo.</p>
<p>A classificação manual dos Captchas envolve o trabalho de baixar, anotar (manualmente) e verificar se a anotação está correta. Trata-se de um trabalho repetitivo e dispendioso, utilizado para gerar as simulações do trabalho.</p>
<p>O oráculo envolve a possibilidade de checar, de forma automática, se uma predição do rótulo de uma imagem está correta. Por ser um teste de Turing inverso, o Captcha é obrigado a mencionar se uma predição está correta: se a predição foi correta, a página de interesse é acessada; se a predição está incorreta, o site envia uma mensagem de erro. As etapas de replicar, parsear e validar para qualquer site de interesse envolvem os passos a seguir.</p>
<ol type="1">
<li>Acessar a página do site de interesse.</li>
<li>Preencher o formulário de pesquisa com a informação a ser consultada. Por exemplo, no site da RFB, a informação é o CNPJ da empresa a ser consultada. Em um site de tribunal, a informação é um número identificador de processo.</li>
<li>Baixar a imagem do Captcha da busca.</li>
<li>Obter o rótulo da imagem, aplicando um modelo na imagem baixada ou classificado manualmente.</li>
<li>Submeter a consulta no site, informando o rótulo.</li>
<li>Verificar o resultado. Se acessou a página desejada, o rótulo está correto. Caso contrário, o rótulo está incorreto.</li>
</ol>
<p>O procedimento descrito pode ser reproduzindo indefinidamente. Isso significa que é possível criar uma base de dados virtualmente infinita de imagens rotuladas, com a informação adicional do rótulo estar correto ou incorreto. Isso foi feito para gerar os dados utilizados na simulação.</p>
<p>O problema do uso de oráculos é que a informação adicional recebida quando o modelo erra é <strong>incompleta</strong>. A única informação nova disponível é que o rótulo testado está incorreto, dentre todos os rótulos possíveis daquela imagem. Como existe uma grande quantidade de rótulos possíveis em um Captcha, muitas vezes na ordem de milhões, a informação que o oráculo fornece é fraca.</p>
<p>Uma possível abordagem para lidar com o segundo problema seria simplesmente descartar os Captchas classificados incorretamente. É possível criar uma base de dados (virtualmente infinita) somente com os rótulos corretos e ajustar um novo modelo. Essa abordagem, no entanto, tem sérios problemas, já que considera somente os casos em que o classificador já funciona bem. O trabalho realizado na tese incorpora a informação fornecida pelo oráculo quando o modelo erra.</p>
<p>Outra oportunidade que o oráculo oferece em parte dos casos é a possibilidade de testar mais de uma predição. Sites com essa característica permitem que a pessoa ou robô teste mais de uma predição caso o Captcha tenha fracassado. Como é possível observar na <a href="#tbl-lista-captcha">Tabela&nbsp;<span>2.1</span></a>, dos 10 Captchas trabalhados, 7 permitem a realização desses testes.</p>
<p>Neste momento, cabe uma observação sobre oráculos e força bruta. O poder de testar vários rótulos para o mesmo Captcha implica na possibilidade teórica de resolver um Captcha por força bruta. Bastaria testar todos os rótulos possíveis para acessar a página de interesse. Na prática, no entanto, essa estratégia não funciona, já que a quantidade de rótulos possíveis é muito grande para testar no site, seja por demorar muito tempo ou pelo site forçar a troca do desafio após a passagem de determinado tempo ou quantidade de tentativas.</p>
<p>Voltando ao ciclo da raspagem, ao longo do procedimento de baixar imagens de Captchas e aplicar o oráculo, pelo menos duas funções devem ser criadas: <strong>acesso</strong> e <strong>teste</strong>. A operação de acesso é responsável por preencher o formulário de busca e baixar o Captcha (passos 1 a 3 da lista acima). A operação de teste é responsável por submeter um rótulo do Captcha e verificar retornar se o rótulo está correto ou incorreto (passos 4 a 6 da lista acima). Em alguns casos, as funções de acesso e teste precisam compartilhar parâmetros que contêm a sessão do usuário, para garantir que o teste envolva o mesmo Captcha da etapa de acesso.</p>
<p>Os Captchas foram anotados manualmente com o procedimento chamado de semi-automático, definido a seguir. No pacote <code>{captchaDownload}</code> (ver Apêndice <a href="pacote.html#sec-pacote-download"><span>A.2</span></a>), foram desenvolvidas ferramentas para baixar e organizar cada Captcha, utilizando o oráculo para garantir que as imagens eram corretamente classificadas.</p>
<p>Cada Captcha teve as primeiras 100 observações classificadas manualmente. Isso foi feito a partir do próprio RStudio, utilizando a ferramenta de classificação manual do pacote <code>{captcha}</code>.</p>
<p>A partir das classificações iniciais, um modelo foi ajustado com a quantidade de observações disponível. Esse passo também foi feito a partir do pacote <code>{captcha}</code>, que cria um projeto de classificação para um Captcha específico.</p>
<p>O modelo, então, foi utilizado como uma ferramenta para otimizar a classificação manual, funcionando da seguinte forma. Primeiro, o modelo tenta realizar a predição automaticamente e o oráculo avisa se a predição está correta ou não. Se estiver incorreto e o site aceitar várias tentativas, o modelo tenta novamente, mas com uma segunda alternativa de predição. Caso o site não aceite várias tentativas ou o modelo não consiga acertar o Captcha em <span class="math inline">\(N\)</span> tentativas (abritrado como dez), a imagem do Captcha aparece para classificação manual.</p>
<p>Com o procedimento destacado acima, é criada uma nova base de dados, que por sua vez é utilizada para ajustar um novo modelo. O modelo, atualizado, é utilizado para classificar novos Captchas, e assim por diante, até que o modelo ajustado alcance uma acurácia razoável, que foi arbitrada em 80%. Com isso o procedimento de anotação é finalizado.</p>
<p>O único problema do procedimento de classificação diz respeito aos Captchas que não aceitam várias tentativas. Nesses casos, não é possível verificar com certeza absoluta que um caso classificado manualmente (após a tentativa do modelo) foi classificado corretamente, já que a classificação manual seria a segunda tentativa. No entanto, esse problema aparece somente em três Captchas (<code>cadesp</code>, <code>jucesp</code> e <code>trf5</code>). A classificação manual dos 100 primeiros Captchas, no entanto, mostrou que pelo menos 95% dos Captchas foram classificados corretamente quando classificados manualmente. A proporção máxima de 5% de erro é negligenciável considerando que a maior parte das bases de dados foi construída com verificação do oráculo.</p>
<p>Em alguns casos, os rótulos dos Captchas podem ser obtidos sem intervenção humana, utilizando técnicas de raspagem de dados e processamento de sinais. Um exemplo é o Captcha do SEI, que mostra informações suficientes para resolver o Captcha na própria URL que gera a imagem. Outro exemplo é o TJMG, que libera, além da imagem, um áudio contendo o mesmo rótulo da imagem, sem a adição de ruídos. Como o áudio não tem ruídos, basta ler o áudio, separar os áudios de cada caractere e calcular uma estatística simples (como a soma das amplitudes, ao quadrado). Essa estatística é utilizada para associar um pedaço de áudio a um caractere.</p>
<p>A <a href="#tbl-lista-captcha-carac">Tabela&nbsp;<span>2.2</span></a> caracteriza os Captchas anotados. Todos os Captchas possuem comprimento entre 4 e 6 dígitos e, com exceção do SEI, não são sensíveis a maiúsculas e minúsculas.</p>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-lista-captcha-carac" class="anchored">

<div class="tabwid"><style>.cl-b8bfbd6a{}.cl-b8b810ce{font-family:'Times New Roman';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b8bae8f8{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 2;background-color:transparent;}.cl-b8bafb68{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b8bafb72{width:0.8in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b8bafb73{width:1.1in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b8bafb7c{width:1.3in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b8bafb7d{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b8bafb7e{width:0.8in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b8bafb86{width:1.1in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b8bafb87{width:1.3in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b8bafb88{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b8bafb89{width:0.8in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b8bafb90{width:1.1in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b8bafb91{width:1.3in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class="cl-b8bfbd6a"><caption>Tabela&nbsp;2.2:  Lista de captchas analisados e suas características. </caption><thead><tr style="overflow-wrap:break-word;"><th class="cl-b8bafb68"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">Captcha</span></p></th><th class="cl-b8bafb72"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">Vários chutes</span></p></th><th class="cl-b8bafb73"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">Caracteres</span></p></th><th class="cl-b8bafb73"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">Comprimento</span></p></th><th class="cl-b8bafb68"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">Colorido</span></p></th><th class="cl-b8bafb7c"><p class="cl-b8bae8f8"><span class="cl-b8b810ce"># Rótulos anotados</span></p></th></tr></thead><tbody><tr style="overflow-wrap:break-word;"><td class="cl-b8bafb7d"><p class="cl-b8bae8f8"><a href="https://pje.trf5.jus.br/pje/ConsultaPublica/listView.seam"><span class="cl-b8b810ce">trf5</span></a></p></td><td class="cl-b8bafb7e"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">Não</span></p></td><td class="cl-b8bafb86"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">0:9</span></p></td><td class="cl-b8bafb86"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">6</span></p></td><td class="cl-b8bafb7d"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">não</span></p></td><td class="cl-b8bafb87"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">1000</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b8bafb7d"><p class="cl-b8bae8f8"><a href="https://www4.tjmg.jus.br/juridico/sf/proc_resultado.jsp?comrCodigo=24&amp;numero=1&amp;listaProcessos=50718889720218130024&amp;btn_pesquisar=Pesquisar"><span class="cl-b8b810ce">tjmg</span></a></p></td><td class="cl-b8bafb7e"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">Sim</span></p></td><td class="cl-b8bafb86"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">0:9</span></p></td><td class="cl-b8bafb86"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">5</span></p></td><td class="cl-b8bafb7d"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">sim</span></p></td><td class="cl-b8bafb87"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">1000</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b8bafb7d"><p class="cl-b8bae8f8"><a href="https://pje-consulta.trt3.jus.br/pje-consulta-api/api/processos/2104879"><span class="cl-b8b810ce">trt</span></a></p></td><td class="cl-b8bafb7e"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">Sim</span></p></td><td class="cl-b8bafb86"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">a-z0:9</span></p></td><td class="cl-b8bafb86"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">6</span></p></td><td class="cl-b8bafb7d"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">não</span></p></td><td class="cl-b8bafb87"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">1500</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b8bafb7d"><p class="cl-b8bae8f8"><a href="http://esaj.tjba.jus.br/cpopg/open.do"><span class="cl-b8b810ce">esaj</span></a></p></td><td class="cl-b8bafb7e"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">Sim</span></p></td><td class="cl-b8bafb86"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">a-z</span></p></td><td class="cl-b8bafb86"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">5</span></p></td><td class="cl-b8bafb7d"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">sim</span></p></td><td class="cl-b8bafb87"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">3000</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b8bafb7d"><p class="cl-b8bae8f8"><a href="https://www.jucesponline.sp.gov.br/ResultadoBusca.aspx"><span class="cl-b8b810ce">jucesp</span></a></p></td><td class="cl-b8bafb7e"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">Não</span></p></td><td class="cl-b8bafb86"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">a-z0-9</span></p></td><td class="cl-b8bafb86"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">5</span></p></td><td class="cl-b8bafb7d"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">não</span></p></td><td class="cl-b8bafb87"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">4000</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b8bafb7d"><p class="cl-b8bae8f8"><a href="https://srv01.tjpe.jus.br/consultaprocessualunificada/"><span class="cl-b8b810ce">tjpe</span></a></p></td><td class="cl-b8bafb7e"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">Sim</span></p></td><td class="cl-b8bafb86"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">a-z0-9</span></p></td><td class="cl-b8bafb86"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">5</span></p></td><td class="cl-b8bafb7d"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">não</span></p></td><td class="cl-b8bafb87"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">4000</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b8bafb7d"><p class="cl-b8bae8f8"><a href="https://www.tjrs.jus.br/site_php/consulta/verificador.php"><span class="cl-b8b810ce">tjrs</span></a></p></td><td class="cl-b8bafb7e"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">Sim</span></p></td><td class="cl-b8bafb86"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">0-9</span></p></td><td class="cl-b8bafb86"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">4</span></p></td><td class="cl-b8bafb7d"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">sim</span></p></td><td class="cl-b8bafb87"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">2000</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b8bafb7d"><p class="cl-b8bae8f8"><a href="https://www.cadesp.fazenda.sp.gov.br/(S(vyfz1cfybbxj3sgpf4eqhxd3"><span class="cl-b8b810ce">cadesp</span></a></p></td><td class="cl-b8bafb7e"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">Não</span></p></td><td class="cl-b8bafb86"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">a-z</span></p></td><td class="cl-b8bafb86"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">4</span></p></td><td class="cl-b8bafb7d"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">sim</span></p></td><td class="cl-b8bafb87"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">3000</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b8bafb7d"><p class="cl-b8bae8f8"><a href="https://sei.economia.gov.br/sei/modulos/pesquisa/md_pesq_processo_pesquisar.php?acao_externa=protocolo_pesquisar&amp;acao_origem_externa=protocolo_pesquisar&amp;id_orgao_acesso_externo=0"><span class="cl-b8b810ce">sei</span></a></p></td><td class="cl-b8bafb7e"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">Sim</span></p></td><td class="cl-b8bafb86"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">a-zA-Z0-9</span></p></td><td class="cl-b8bafb86"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">4</span></p></td><td class="cl-b8bafb7d"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">sim</span></p></td><td class="cl-b8bafb87"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">10000</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-b8bafb88"><p class="cl-b8bae8f8"><a href="https://servicos.receita.fazenda.gov.br/servicos/cnpjreva/Cnpjreva_Solicitacao_CS.asp"><span class="cl-b8b810ce">rfb</span></a></p></td><td class="cl-b8bafb89"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">Sim</span></p></td><td class="cl-b8bafb90"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">a-z0-9</span></p></td><td class="cl-b8bafb90"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">6</span></p></td><td class="cl-b8bafb88"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">não</span></p></td><td class="cl-b8bafb91"><p class="cl-b8bae8f8"><span class="cl-b8b810ce">4000</span></p></td></tr></tbody></table></div>
</div>
</div>
</div>
<p>As bases de dados com imagens anotadas foram disponibilizadas na aba de lançamentos (<em>releases</em>) do <a href="https://github.com/jtrecenti/doutorado/releases">repositório principal do projeto de pesquisa</a>. As bases com imagens e modelos ajustados estão disponíveis para quem tiver interesse em fazer novas pesquisas e utilizar os resultados em suas aplicações, sem restrições de uso.</p>
<!-- ---------------------------------------------------------------------- -->
</section>
</section>
<section id="simulacoes" class="level2">
<h2 class="anchored" data-anchor-id="simulacoes">Simulações</h2>
<p>Para verificar o poder do uso do oráculo para o aprendizado do modelo, uma série de simulações foi desenvolvidas. As simulações foram organizadas em três passos: modelo inicial, dados e modelo final. Os passos foram descritos em maior detalhe a seguir.</p>
<section id="primeiro-passo-modelo-inicial" class="level3">
<h3 class="anchored" data-anchor-id="primeiro-passo-modelo-inicial">Primeiro passo: modelo inicial</h3>
<p>A simulação do modelo inicial teve como objetivo obter modelos preditivos de Captchas com acurácias distintas. O modelo inicial seria usado, então, para baixar dados diretamente do site usando o oráculo e, por fim, ajustar um modelo final com os novos dados provenientes do oráculo.</p>
<p>Os modelos iniciais foram construídos em dois passos. O primeiro foi montar a base de dados completa, suficiente para ajustar um modelo com alta acurácia, que arbitrados em 80%, como descrito anteriormente. Depois, montou-se 10 amostras de dados com subconjuntos das bases completas, cada uma contendo 10%, 20%, e assim por diante, até a base completa. Por exemplo: no Captcha da Jucesp, construiu-se um modelo com acurácia maior que 80% com 4000 Captchas. A partir disso, foi feita uma partição dos dados com 400 imagens (10% do total), 800 imagens (20% do total) e assim por diante, até o modelo com 4000 Captchas.</p>
<p>Para cada tamanho de amostra <span class="math inline">\(A\)</span>, aplicou-se uma bateria de 27 modelos. Isso foi feito porque para diferentes quantidades de amostra, a configuração dos hiperparâmetros que resulta no melhor modelo pode ser diferente. Os modelos seguiram uma grade de hiperparâmetros considerando três informações:</p>
<ul>
<li>A quantidade de unidades computacionais na primeira camada densa após as camadas convolucionais, com os valores considerados: 100, 200 e 300.</li>
<li>O valor do <em>dropout</em> aplicado às camadas densas, com os valores considerados: 10%, 30% e 50%.</li>
<li>O fator de decaimento na taxa de aprendizado a cada época, com os valores considerados: 1%, 2% e 3%.</li>
</ul>
<p>Combinando os três valores dos três hiperparâmetros, tem-se um total de <span class="math inline">\(27=3^3\)</span> hiperparâmetros. Com isso, foi possível identificar, para cada tamanho de amostra <span class="math inline">\(A\)</span>, o classificador <span class="math inline">\(C_A\)</span> com a melhor acurácia dentre os modelos ajustados.</p>
<p>No final do primeiro passo, portanto, considera-se apenas o melhor modelo para cada tamanho de amostra, dentre os 27 ajustados. É claro que os modelos encontrados por essa técnica não são, necessariamente, os melhores modelos possíveis. No entanto, como a técnica é a mesma para todos os Captchas, é possível fazer comparações através de uma metodologia mais transparente.</p>
</section>
<section id="segundo-passo-dados" class="level3">
<h3 class="anchored" data-anchor-id="segundo-passo-dados">Segundo passo: dados</h3>
<p>O segundo passo teve como objetivo construir as bases de dados utilizando o oráculo. Primeiro, foi necessário decidir quais modelos, dentre os 10 ajustados para cada Captcha, seriam utilizados para construir novas bases. Não faria sentido, por exemplo, considerar um modelo com acurácia de 0%, já que ele não produziria nenhuma observação comparado com um modelo que chuta aleatoriamente. Também não faria sentido considerar um classificador com acurácia de 100%, já que nesse caso não há o que testar com a técnica do oráculo.</p>
<p>Decidiu-se que seria necessário considerar somente os modelos que resultaram em acurácias maiores de 1% e menores de 50%. O valor máximo foi decidido após realizar alguns testes empíricos e verificar, informalmente, que a técnica do oráculo realmente resultava em ganhos expressivos, mesmo com modelos de baixa acurácia. Concluiu-se então que não seria necessário testar a eficácia da técnica para classificadores com alta acurácia. Já o valor mínimo foi decidido de forma arbitrária, retirando-se os classificadores com acurácia muito baixa.</p>
<p>A segunda decisão a ser tomada para construção dos dados foi a quantidade de imagens que seria baixada para cada Captcha. Como são Captchas de diferentes dificuldades, a quantidade de dados seria diferente. Optou-se por baixar a quantidade de dados de forma a montar uma base de treino que contém a quantidade de observações necessária para obter o melhor modelo daquele Captcha. Por exemplo, no TJRS, um modelo com acurácia próxima de 100% foi identificado com 2000 observações. O melhor modelo com 300 imagens (240 para treino, 60 para teste) resultou em uma acurácia de 35%. Foram, então, baixadas 1760 observações para compor o total de 2000 na base de treino. As imagens de teste do modelo inicial poderiam até ser utilizadas, mas optamos por descartar para garantir que o modelo não ficasse sobreajustado para a primeira base.</p>
<p>O motivo de baixar a mesma quantidade de observações que o melhor modelo inicial foi feita por dois motivos. O primeiro é que existem evidências de que é possível construir um bom modelo com essa quantidade de imagens, ainda que em um caso as informações são completas e, no outro, incompletas. O segundo é que isso permite a comparação do resultado do modelo completamente anotado contra o modelo que é parcialmente anotado e com anotações incompletas provenientes do oráculo.</p>
<p>A terceira e última decisão tomada para baixar os dados foi a quantidade de chutes que o modelo poderia fazer, nos casos em que isso é permitido pelo site. Optou-se, de forma arbitrária, por três valores: 1, que é equivalente a um site que não permite múltiplos chutes, 5 chutes e 10 chutes.</p>
<p>Portanto, o procedimento de coleta dos dados foi feito, para cada Captcha, da seguinte forma:</p>
<ol type="1">
<li>Listou-se todos os melhores modelos ajustados para cada tamanho de amostra.</li>
<li>Filtrou-se os modelos para os que apresentavam acurácia de 5% até 50%</li>
<li>Definiu-se o tamanho da base a ser obtida, com base no tamanho da base de treino utilizada no modelo e a quantidade total que se objetivou obter.</li>
<li>Para cada quantidade de tentativas disponível (1, 5 e 10), baixou-se as imagens, anotando com o valor “1” se o rótulo de alguma das tentativas estivesse correto e com o valor “0” caso contrário.</li>
<li>Nos casos com erros, armazenou-se um arquivo de log para cada Captcha com o histórico de tentativas incorretas, que é a informação mais importante a ser passada para o modelo final.</li>
</ol>
<p>No final, obteve-se bases de dados de treino para todos os Captchas analisados, com quantidades de imagens variadas de acordo com os parâmetros definidos anteriormente, variando também pela quantidade de tentativas. A quantidade total de bases de dados geradas foi 65.</p>
<p>Além das bases de treino, foi construída uma base de teste para cada Captcha. As bases de teste foram construídas completamente do zero, sem utilizar informações de bases anteriores. Para construir as bases, utilizou-se a mesma técnica semi-automática definida anteriormente, usando o melhor modelo disponível para classificar a maioria das imagens e classificando manualmente em caso de falha. Em alguns casos, como TJMG e TJRS, a classificação humana quase não foi necessária, pois os classificadores obtidos apresentaram acurácia próxima de 100%.</p>
<p>Como o único objetivo da base de teste foi o de estimar a acurácia dos modelos finais, a quantidade de observações poderia ser arbitrada. O tamanho das bases de teste foi, então, arbitrado em 1000 imagens para cada Captcha.</p>
</section>
<section id="sec-modelo-final" class="level3">
<h3 class="anchored" data-anchor-id="sec-modelo-final">Terceiro passo: modelo final</h3>
<p>O modelo final foi ajustado para cada uma das 65 bases de treino disponíveis após a realização dos passos 1 e 2. Nesse caso, utilizou-se o modelo proposto na <a href="#sec-wawl"><span>Seção&nbsp;2.2</span></a>. Caso a imagem tenha sido corretamente classificada, a função de perda é calculada normalmente. Caso ela tenha sido classificada incorretamente, considera-se a probabilidade de não observar nenhum dos chutes.</p>
<p>Além de modificar a forma de calcular a função de perda do modelo, foi necessário realizar uma nova busca de hiperparâmetros. Optou-se por utilizar os mesmos hiperparâmetros dos modelos iniciais para manter a consistência. O único detalhe nesse ponto é que, como os parâmetros de partida são os do modelo inicial, optou-se por não modificar a quantidade de unidades na camada densa, variando somente os valores de <em>dropout</em> e de decaimento na taxa de aprendizado. Portanto, ajustou-se 9 e não 27 modelos para cada base de dados.</p>
<p>No final, assim como no primeiro passo, os classificadors com melhor acurácia foram selecionados para cada modelo. Obteve-se, então, com 65 modelos no final para comparar com os modelos iniciais e estimar a efetividade do oráculo. As comparações foram feitas através de gráficos de barras, explorando o efeito do uso do oráculo para diferentes Captchas, diferentes modelos iniciais e diferentes quantidades de chutes, além de um gráfico de dispersão para relacionar as acurácias iniciais e finais.</p>
<p>Além do terceiro passo, outros experimentos foram realizados para verificar se, ao aplicar a técnica do oráculo iterativamente, os resultados continuariam melhorando. Ou seja, é possível considerar os modelos obtidos no passo 3 como os modelos iniciais do passo 1, aplicar novamente o passo 2 (baixar dados) e o passo 3 (rodar modelo com os novos dados). Isso foi feito para apenas um conjunto selecionado de Captchas para verificar essa possibilidade, não fazendo parte das simulações principais do estudo.</p>
<p>As bases de dados das simulações também foram disponibilizadas na aba de lançamentos (<em>releases</em>) do <a href="https://github.com/jtrecenti/doutorado/releases">repositório principal do projeto de pesquisa</a>. As bases podem ser utilizadas para aumentar as bases de treino e para testar outras arquiteturas de redes neurais ao tema dos Captchas com uso de aprendizado fracamente supervisionado.</p>


<div id="refs" class="references csl-bib-body" data-entry-spacing="1" role="doc-bibliography" style="display: none">
<div id="ref-baldi2013" class="csl-entry" role="doc-biblioentry">
BALDI, P.; SADOWSKI, P. J. Understanding dropout. <strong>Advances in neural information processing systems</strong>, v. 26, 2013.
</div>
<div id="ref-blum1998" class="csl-entry" role="doc-biblioentry">
BLUM, A.; KALAI, A. A note on learning from multiple-instance examples. <strong>Machine learning</strong>, v. 30, n. 1, p. 2329, 1998.
</div>
<div id="ref-cour2011" class="csl-entry" role="doc-biblioentry">
COUR, T.; SAPP, B.; TASKAR, B. Learning from partial labels. <strong>The Journal of Machine Learning Research</strong>, v. 12, p. 15011536, 2011.
</div>
<div id="ref-luz" class="csl-entry" role="doc-biblioentry">
FALBEL, D. luz: Higher Level ’API’ for ’torch’. a2022. Disponível em: &lt;<a href="https://CRAN.R-project.org/package=luz">https://CRAN.R-project.org/package=luz</a>&gt;.
</div>
<div id="ref-torchvision" class="csl-entry" role="doc-biblioentry">
FALBEL, D. torchvision: Models, Datasets and Transformations for Images. b2022. Disponível em: &lt;<a href="https://CRAN.R-project.org/package=torchvision">https://CRAN.R-project.org/package=torchvision</a>&gt;.
</div>
<div id="ref-torch" class="csl-entry" role="doc-biblioentry">
FALBEL, D.; LURASCHI, J. torch: Tensors and Neural Networks with ’GPU’ Acceleration. 2022. Disponível em: &lt;<a href="https://CRAN.R-project.org/package=torch">https://CRAN.R-project.org/package=torch</a>&gt;.
</div>
<div id="ref-feng2020" class="csl-entry" role="doc-biblioentry">
FENG, L. et al. Provably consistent partial-label learning. <strong>Advances in Neural Information Processing Systems</strong>, v. 33, p. 1094810960, a2020.
</div>
<div id="ref-feng2020a" class="csl-entry" role="doc-biblioentry">
FENG, L. et al. <strong>Learning with multiple complementary labels</strong>. PMLR, b2020.
</div>
<div id="ref-galar2011" class="csl-entry" role="doc-biblioentry">
GALAR, M. et al. A review on ensembles for the class imbalance problem: bagging-, boosting-, and hybrid-based approaches. <strong>IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</strong>, v. 42, n. 4, p. 463484, 2011.
</div>
<div id="ref-george2017" class="csl-entry" role="doc-biblioentry">
GEORGE, D. et al. A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs. <strong>Science</strong>, v. 358, n. 6368, p. eaag2612, 2017.
</div>
<div id="ref-grandvalet2002" class="csl-entry" role="doc-biblioentry">
GRANDVALET, Y. <strong>Logistic regression for partial labels</strong>. 2002.
</div>
<div id="ref-hullermeier2006" class="csl-entry" role="doc-biblioentry">
HÜLLERMEIER, E.; BERINGER, J. Learning from ambiguously labeled examples. <strong>Intelligent Data Analysis</strong>, v. 10, n. 5, p. 419439, 2006.
</div>
<div id="ref-ioffe2015" class="csl-entry" role="doc-biblioentry">
IOFFE, S.; SZEGEDY, C. <strong>Batch normalization: Accelerating deep network training by reducing internal covariate shift</strong>. PMLR, 2015.
</div>
<div id="ref-ishida2017" class="csl-entry" role="doc-biblioentry">
ISHIDA, T. et al. Learning from complementary labels. <strong>Advances in neural information processing systems</strong>, v. 30, 2017.
</div>
<div id="ref-jin2002" class="csl-entry" role="doc-biblioentry">
JIN, R.; GHAHRAMANI, Z. Learning with multiple labels. <strong>Advances in neural information processing systems</strong>, v. 15, 2002.
</div>
<div id="ref-kaur2014" class="csl-entry" role="doc-biblioentry">
KAUR, K.; BEHAL, S. Captcha and Its Techniques: A Review. <strong>International Journal of Computer Science and Information Technologies,</strong> v. 5, 1 jan. 2014.
</div>
<div id="ref-kingmaAdamMethodStochastic2017" class="csl-entry" role="doc-biblioentry">
KINGMA, D. P.; BA, J. Adam: <span>A Method</span> for <span>Stochastic Optimization</span>. n. arXiv:1412.6980, jan. 2017. Disponível em: &lt;<a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a>&gt;.
</div>
<div id="ref-kuhn2019" class="csl-entry" role="doc-biblioentry">
KUHN, M.; JOHNSON, K. <strong>Feature engineering and selection: A practical approach for predictive models</strong>. CRC Press, 2019.
</div>
<div id="ref-lecun1998" class="csl-entry" role="doc-biblioentry">
LECUN, Y. et al. Gradient-based learning applied to document recognition. <strong>Proceedings of the IEEE</strong>, v. 86, n. 11, p. 22782324, 1998.
</div>
<div id="ref-lecun2012" class="csl-entry" role="doc-biblioentry">
LECUN, Y. A. et al. Efficient backprop. Em: Springer, 2012. p. 948.
</div>
<div id="ref-lecun2015" class="csl-entry" role="doc-biblioentry">
LECUN, Y.; BENGIO, Y.; HINTON, G. Deep learning. <strong>nature</strong>, v. 521, n. 7553, p. 436444, 2015.
</div>
<div id="ref-li2014" class="csl-entry" role="doc-biblioentry">
LI, J.; TSUNG, F.; ZOU, C. Multivariate binomial/multinomial control chart. <strong>IIE Transactions</strong>, v. 46, n. 5, p. 526542, 2014.
</div>
<div id="ref-liu2012" class="csl-entry" role="doc-biblioentry">
LIU, L.; DIETTERICH, T. A conditional multinomial mixture model for superset label learning. <strong>Advances in neural information processing systems</strong>, v. 25, 2012.
</div>
<div id="ref-nelder1972" class="csl-entry" role="doc-biblioentry">
NELDER, J. A.; WEDDERBURN, R. W. Generalized linear models. <strong>Journal of the Royal Statistical Society: Series A (General)</strong>, v. 135, n. 3, p. 370384, 1972.
</div>
<div id="ref-magick" class="csl-entry" role="doc-biblioentry">
OOMS, J. magick: Advanced Graphics and Image-Processing in R. 2021. Disponível em: &lt;<a href="https://CRAN.R-project.org/package=magick">https://CRAN.R-project.org/package=magick</a>&gt;.
</div>
<div id="ref-rcran" class="csl-entry" role="doc-biblioentry">
R CORE TEAM. <strong>R: A Language and Environment for Statistical Computing</strong>. Vienna, Austria: R Foundation for Statistical Computing, 2021. Disponível em: &lt;<a href="https://www.R-project.org/">https://www.R-project.org/</a>&gt;.
</div>
<div id="ref-rameshHierarchicalTextConditionalImage2022" class="csl-entry" role="doc-biblioentry">
RAMESH, A. et al. Hierarchical <span>Text-Conditional Image Generation</span> with <span>CLIP Latents</span>. n. arXiv:2204.06125, abr. 2022. Disponível em: &lt;<a href="https://arxiv.org/abs/2204.06125">https://arxiv.org/abs/2204.06125</a>&gt;.
</div>
<div id="ref-stringr" class="csl-entry" role="doc-biblioentry">
WICKHAM, H. stringr: Simple, Consistent Wrappers for Common String Operations. b2022. Disponível em: &lt;<a href="https://CRAN.R-project.org/package=stringr">https://CRAN.R-project.org/package=stringr</a>&gt;.
</div>
<div id="ref-rvest" class="csl-entry" role="doc-biblioentry">
WICKHAM, H. rvest: Easily Harvest (Scrape) Web Pages. a2022. Disponível em: &lt;<a href="https://CRAN.R-project.org/package=rvest">https://CRAN.R-project.org/package=rvest</a>&gt;.
</div>
<div id="ref-xml2" class="csl-entry" role="doc-biblioentry">
WICKHAM, H.; HESTER, J.; OOMS, J. xml2: Parse XML. 2021. Disponível em: &lt;<a href="https://CRAN.R-project.org/package=xml2">https://CRAN.R-project.org/package=xml2</a>&gt;.
</div>
<div id="ref-ye2018" class="csl-entry" role="doc-biblioentry">
YE, G. et al. <strong>Yet another text captcha solver: A generative adversarial network based approach</strong>. 2018.
</div>
<div id="ref-yu2018" class="csl-entry" role="doc-biblioentry">
YU, X. et al. <strong>Learning with biased complementary labels</strong>. 2018.
</div>
<div id="ref-zhao2017" class="csl-entry" role="doc-biblioentry">
ZHAO, B. Web scraping. <strong>Encyclopedia of big data</strong>, p. 13, 2017. Disponível em: &lt;<a href="https://www.researchgate.net/profile/Bo-Zhao-3/publication/317177787_Web_Scraping/links/5c293f85a6fdccfc7073192f/Web-Scraping.pdf">https://www.researchgate.net/profile/Bo-Zhao-3/publication/317177787_Web_Scraping/links/5c293f85a6fdccfc7073192f/Web-Scraping.pdf</a>&gt;.
</div>
<div id="ref-zhou2018" class="csl-entry" role="doc-biblioentry">
ZHOU, Z.-H. A brief introduction to weakly supervised learning. <strong>National science review</strong>, v. 5, n. 1, p. 4453, 2018.
</div>
<div id="ref-zhu2005" class="csl-entry" role="doc-biblioentry">
ZHU, X. J. Semi-supervised learning literature survey. 2005.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>existem exemplos de Captchas baseados em imagens que não são limitados a letras e números para constituir o rótulo <span class="citation" data-cites="kaur2014">(<a href="bibliografia.html#ref-kaur2014" role="doc-biblioref">KAUR; BEHAL, 2014</a>)</span>. Como esses casos não aparecem nas aplicações práticas de interesse, estão fora do escopo do trabalho.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Mais sobre o (py)torch: <a href="https://pytorch.org" class="uri">https://pytorch.org</a>. Último acesso em 22 de novembro de 2022.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Mais detalhes em <a href="https://www.tensorflow.org" class="uri">https://www.tensorflow.org</a>. Último acesso em 22 de novembro de 2022.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Existem dois tipos principais de requisição HTTP. A requisição GET serve para capturar uma página da internet, enquanto a requisição POST serve para enviar dados para o servidor como, um login e uma senha. A lista completa de requisições está disponível na <a href="https://www.rfc-editor.org/rfc/rfc9110.html" title="https://www.rfc-editor.org/rfc/rfc9110.html">documentação da <em>Internet Engineering Task Force</em> (IETF)</a>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiada");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiada");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./introducao.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introdução</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./resultados.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Resultados</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>