@misc{CaptchaGoogleAcademico,
  title = {Captcha - {{Google Acad\^emico}}},
  howpublished = {https://scholar.google.com.br/scholar?hl=pt-BR\&as\_sdt=0\%2C5\&q=captcha\&btnG=},
  file = {C\:\\Users\\julio\\Zotero\\storage\\S76JUQGJ\\scholar.html}
}

@inproceedings{chellapillaBuildingSegmentationBased2005,
  title = {Building {{Segmentation Based Human-Friendly Human Interaction Proofs}} ({{HIPs}})},
  booktitle = {Human {{Interactive Proofs}}},
  author = {Chellapilla, Kumar and Larson, Kevin and Simard, Patrice Y. and Czerwinski, Mary},
  editor = {Baird, Henry S. and Lopresti, Daniel P.},
  year = {2005},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {1--26},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11427896_1},
  abstract = {Human interaction proofs (HIPs) have become common place on the internet due to their effectiveness in deterring automated abuse of online services intended for humans. However, there is a co-evolutionary arms race in progress and these proofs are becoming more difficult for genuine users while attackers are getting better at breaking existing HIPs. We studied various popular HIPs on the internet to understand their strength and human friendliness. To determine HIP strength, we adopted a direct approach of building computer attacks using image processing and machine learning techniques. To understand human-friendliness, a sequence of users studies were conducted to investigate HIP character recognition by humans under a variety of visual distortions and clutter commonly employed in reading-based HIPs. We found that many of the online HIPs are pure recognition tasks that can be easily broken using machine learning. The stronger HIPs tend to pose a combination of segmentation and recognition challenges. Further, the HIP user studies show that given correct segmentation, computers are much better at HIP character recognition than humans. In light of these results, we propose that segmentation-based reading challenges are the future for building stronger human-friendly HIPs. An example of such a segmentation-based HIP is presented with a preliminary assessment of its strength and human-friendliness.},
  isbn = {978-3-540-32117-0},
  langid = {english},
  keywords = {Baseline Setting,Convolutional Neural Network,High Accuracy Rate,Optical Character Recognition,User Study}
}

@inproceedings{chellapillaDesigningHumanFriendly2005,
  title = {Designing Human Friendly Human Interaction Proofs ({{HIPs}})},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Chellapilla, Kumar and Larson, Kevin and Simard, Patrice and Czerwinski, Mary},
  year = {2005},
  month = apr,
  series = {{{CHI}} '05},
  pages = {711--720},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1054972.1055070},
  abstract = {HIPs, or Human Interactive Proofs, are challenges meant to be easily solved by humans, while remaining too hard to be economically solved by computers. HIPs are increasingly used to protect services against automatic script attacks. To be effective, a HIP must be difficult enough to discourage script attacks by raising the computation and/or development cost of breaking the HIP to an unprofitable level. At the same time, the HIP must be easy enough to solve in order to not discourage humans from using the service. Early HIP designs have successfully met these criteria [1]. However, the growing sophistication of attackers and correspondingly increasing profit incentives have rendered most of the currently deployed HIPs vulnerable to attack [2,7,12]. Yet, most companies have been reluctant to increase the difficulty of their HIPs for fear of making them too complex or unappealing to humans. The purpose of this study is to find the visual distortions that are most effective at foiling computer attacks without hindering humans. The contribution of this research is that we discovered that 1) automatically generating HIPs by varying particular distortion parameters renders HIPs that are too easy for computer hackers to break, yet humans still have difficulty recognizing them, and 2) it is possible to build segmentation-based HIPs that are extremely difficult and expensive for computers to solve, while remaining relatively easy for humans.},
  isbn = {978-1-58113-998-3},
  keywords = {completely automated public turing tests to tell computers and humans apart (CAPTCHAs),computer vision,evaluation,human interaction proofs (HIPs),human perception,visual letter recognition}
}

@article{chellapillaUsingMachineLearning2004,
  title = {Using Machine Learning to Break Visual Human Interaction Proofs ({{HIPs}})},
  author = {Chellapilla, Kumar and Simard, Patrice},
  year = {2004},
  journal = {Advances in neural information processing systems},
  volume = {17},
  file = {C\:\\Users\\julio\\Zotero\\storage\\IBYHP2ZN\\Chellapilla e Simard - 2004 - Using machine learning to break visual human inter.pdf;C\:\\Users\\julio\\Zotero\\storage\\QVBFGY7B\\283085d30e10513624c8cece7993f4de-Abstract.html}
}

@book{colosimoAnaliseSobrevivenciaAplicada2006a,
  title = {An\'alise de Sobreviv\^encia Aplicada},
  author = {Colosimo, Enrico Antonio and Giolo, Suely Ruiz},
  year = {2006},
  publisher = {{Editora Blucher}},
  file = {C\:\\Users\\julio\\Zotero\\storage\\G8XW3UW4\\books.html}
}

@misc{diagnosticoABJ,
  title = {Diagn\'ostico Do {{Contencioso Tribut\'ario Administrativo}}},
  abstract = {Estudo sobre estrutura do contencioso, processos administrativos tribut\'arios e percep\c{c}\~oes dos agentes},
  howpublished = {https://abj.org.br/pesquisas/bid-tributario/},
  file = {C\:\\Users\\julio\\Zotero\\storage\\4IA5SLM6\\bid-tributario.html}
}

@article{georgeGenerativeVisionModel2017,
  title = {A Generative Vision Model That Trains with High Data Efficiency and Breaks Text-Based {{CAPTCHAs}}},
  author = {George, Dileep and Lehrach, Wolfgang and Kansky, Ken and {L{\'a}zaro-Gredilla}, Miguel and Laan, Christopher and Marthi, Bhaskara and Lou, Xinghua and Meng, Zhaoshi and Liu, Yi and Wang, Huayan},
  year = {2017},
  journal = {Science},
  volume = {358},
  number = {6368},
  pages = {eaag2612},
  publisher = {{American Association for the Advancement of Science}},
  file = {C\:\\Users\\julio\\Zotero\\storage\\KC3B5SXB\\George et al. - 2017 - A generative vision model that trains with high da.pdf;C\:\\Users\\julio\\Zotero\\storage\\6R7A57BD\\science.html}
}

@misc{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  number = {arXiv:1406.2661},
  eprint = {1406.2661},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1406.2661},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\julio\\Zotero\\storage\\IHI84MWI\\Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf;C\:\\Users\\julio\\Zotero\\storage\\MMXSV359\\1406.html}
}

@article{goodfellowGenerativeAdversarialNetworks2020,
  title = {Generative Adversarial Networks},
  author = {Goodfellow, Ian and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2020},
  journal = {Communications of the ACM},
  volume = {63},
  number = {11},
  pages = {139--144},
  publisher = {{ACM New York, NY, USA}},
  file = {C\:\\Users\\julio\\Zotero\\storage\\S48E3C2P\\Goodfellow et al. - 2020 - Generative adversarial networks.pdf;C\:\\Users\\julio\\Zotero\\storage\\CKQ9JKS5\\3422622.html}
}

@article{goodfellowMultidigitNumberRecognition2013,
  title = {Multi-Digit Number Recognition from Street View Imagery Using Deep Convolutional Neural Networks},
  author = {Goodfellow, Ian J. and Bulatov, Yaroslav and Ibarz, Julian and Arnoud, Sacha and Shet, Vinay},
  year = {2013},
  journal = {arXiv preprint arXiv:1312.6082},
  eprint = {1312.6082},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\julio\\Zotero\\storage\\KCGB4ZNU\\Goodfellow et al. - 2013 - Multi-digit number recognition from street view im.pdf;C\:\\Users\\julio\\Zotero\\storage\\7C9FYAPR\\1312.html}
}

@misc{goodfellowMultidigitNumberRecognition2014,
  title = {Multi-Digit {{Number Recognition}} from {{Street View Imagery}} Using {{Deep Convolutional Neural Networks}}},
  author = {Goodfellow, Ian J. and Bulatov, Yaroslav and Ibarz, Julian and Arnoud, Sacha and Shet, Vinay},
  year = {2014},
  month = apr,
  number = {arXiv:1312.6082},
  eprint = {1312.6082},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1312.6082},
  abstract = {Recognizing arbitrary multi-character text in unconstrained natural photographs is a hard problem. In this paper, we address an equally hard sub-problem in this domain viz. recognizing arbitrary multi-digit numbers from Street View imagery. Traditional approaches to solve this problem typically separate out the localization, segmentation, and recognition steps. In this paper we propose a unified approach that integrates these three steps via the use of a deep convolutional neural network that operates directly on the image pixels. We employ the DistBelief implementation of deep neural networks in order to train large, distributed neural networks on high quality images. We find that the performance of this approach increases with the depth of the convolutional network, with the best performance occurring in the deepest architecture we trained, with eleven hidden layers. We evaluate this approach on the publicly available SVHN dataset and achieve over \$96\textbackslash\%\$ accuracy in recognizing complete street numbers. We show that on a per-digit recognition task, we improve upon the state-of-the-art, achieving \$97.84\textbackslash\%\$ accuracy. We also evaluate this approach on an even more challenging dataset generated from Street View imagery containing several tens of millions of street number annotations and achieve over \$90\textbackslash\%\$ accuracy. To further explore the applicability of the proposed system to broader text recognition tasks, we apply it to synthetic distorted text from reCAPTCHA. reCAPTCHA is one of the most secure reverse turing tests that uses distorted text to distinguish humans from bots. We report a \$99.8\textbackslash\%\$ accuracy on the hardest category of reCAPTCHA. Our evaluations on both tasks indicate that at specific operating thresholds, the performance of the proposed system is comparable to, and in some cases exceeds, that of human operators.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\julio\\Zotero\\storage\\JS95HUKQ\\Goodfellow et al. - 2014 - Multi-digit Number Recognition from Street View Im.pdf;C\:\\Users\\julio\\Zotero\\storage\\CK5MUERD\\1312.html}
}

@misc{InaccessibilityCAPTCHA,
  title = {Inaccessibility of {{CAPTCHA}}},
  howpublished = {https://www.w3.org/TR/turingtest/},
  file = {C\:\\Users\\julio\\Zotero\\storage\\ZUPP2IYZ\\turingtest.html}
}

@article{kaurCaptchaItsTechniques2014,
  title = {Captcha and {{Its Techniques}}: {{A Review}}},
  shorttitle = {Captcha and {{Its Techniques}}},
  author = {Kaur, Kiranjot and Behal, Sunny},
  year = {2014},
  month = jan,
  journal = {International Journal of Computer Science and Information Technologies,},
  volume = {5},
  abstract = {Captcha (Completely Automated public Turing test to tell Computers and Humans Apart) system is used to verify whether a user is human or computer program. It's also known as human Interactive Proof (HIP) and based upon Artificial Intelligence. Captcha is a program that protects websites from web-bots by generating tests that computer cannot pass but human can pass .This paper comprise introduction of captcha, various techniques, application of captcha and drawbacks of captcha.},
  file = {C\:\\Users\\julio\\Zotero\\storage\\67CED3MB\\Kaur e Behal - 2014 - Captcha and Its Techniques A Review.pdf}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  journal = {nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {{Nature Publishing Group}},
  file = {C\:\\Users\\julio\\Zotero\\storage\\AGAIXG5H\\nature14539.html}
}

@patent{lillibridgeMethodSelectivelyRestricting2001,
  title = {Method for Selectively Restricting Access to Computer Systems},
  author = {Lillibridge, Mark D. and Abadi, Martin and Bharat, Krishna and Broder, Andrei Z.},
  year = {2001},
  month = feb,
  number = {US6195698B1},
  assignee = {Compaq Computer Corp},
  nationality = {US},
  keywords = {access request,answer,computer,riddle,string},
  file = {C\:\\Users\\julio\\Zotero\\storage\\KE38TUHP\\Lillibridge et al. - 2001 - Method for selectively restricting access to compu.pdf}
}

@techreport{michenerEstadoBrasileiroTransparencia2015,
  title = {Estado Brasileiro e Transpar\^encia Avaliando a Aplica\c{c}\~ao Da {{Lei}} de {{Acesso}} \`a {{Informa\c{c}\~ao}}},
  author = {Michener, Gregory and Moncau, Luiz Fernando and Velasco, Rafael Braem},
  year = {2015},
  file = {C\:\\Users\\julio\\Zotero\\storage\\MJJWZ678\\Michener et al. - 2015 - Estado brasileiro e transparÃªncia avaliando a apli.pdf}
}

@inproceedings{moriRecognizingObjectsAdversarial2003,
  title = {Recognizing Objects in Adversarial Clutter: {{Breaking}} a Visual {{CAPTCHA}}},
  shorttitle = {Recognizing Objects in Adversarial Clutter},
  booktitle = {2003 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}, 2003. {{Proceedings}}.},
  author = {Mori, Greg and Malik, Jitendra},
  year = {2003},
  volume = {1},
  pages = {I--I},
  publisher = {{IEEE}},
  file = {C\:\\Users\\julio\\Zotero\\storage\\MZZE3LLQ\\Mori e Malik - 2003 - Recognizing objects in adversarial clutter Breaki.pdf;C\:\\Users\\julio\\Zotero\\storage\\LSC5WGXU\\1211347.html}
}

@article{murray-rustOpenDataScience2008,
  title = {Open Data in Science},
  author = {{Murray-Rust}, Peter},
  year = {2008},
  journal = {Nature Precedings},
  pages = {1--1},
  publisher = {{Nature Publishing Group}},
  file = {C\:\\Users\\julio\\Zotero\\storage\\PYHZVG6P\\Murray-Rust - 2008 - Open data in science.pdf;C\:\\Users\\julio\\Zotero\\storage\\MYNAVGF2\\npre.2008.1526.html}
}

@inproceedings{naDeepGenerativePositiveUnlabeled2020,
  title = {Deep {{Generative Positive-Unlabeled Learning}} under {{Selection Bias}}},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Na, Byeonghu and Kim, Hyemi and Song, Kyungwoo and Joo, Weonyoung and Kim, Yoon-Yeong and Moon, Il-Chul},
  year = {2020},
  month = oct,
  series = {{{CIKM}} '20},
  pages = {1155--1164},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3340531.3411971},
  abstract = {Learning in the positive-unlabeled (PU) setting is prevalent in real world applications. Many previous works depend upon theSelected Completely At Random (SCAR) assumption to utilize unlabeled data, but the SCAR assumption is not often applicable to the real world due to selection bias in label observations. This paper is the first generative PU learning model without the SCAR assumption. Specifically, we derive the PU risk function without the SCAR assumption, and we generate a set of virtual PU examples to train the classifier. Although our PU risk function is more generalizable, the function requires PU instances that do not exist in the observations. Therefore, we introduce the VAE-PU, which is a variant of variational autoencoders to separate two latent variables that generate either features or observation indicators. The separated latent information enables the model to generate virtual PU instances. We test the VAE-PU on benchmark datasets with and without the SCAR assumption. The results indicate that the VAE-PU is superior when selection bias exists, and the VAE-PU is also competent under the SCAR assumption. The results also emphasize that the VAE-PU is effective when there are few positive-labeled instances due to modeling on selection bias.},
  isbn = {978-1-4503-6859-9},
  keywords = {positive-unlabeled learning,selection bias,variational autoencoders}
}

@article{nohRegularizingDeepNeural2017,
  title = {Regularizing Deep Neural Networks by Noise: {{Its}} Interpretation and Optimization},
  shorttitle = {Regularizing Deep Neural Networks by Noise},
  author = {Noh, Hyeonwoo and You, Tackgeun and Mun, Jonghwan and Han, Bohyung},
  year = {2017},
  journal = {Advances in Neural Information Processing Systems},
  volume = {30},
  file = {C\:\\Users\\julio\\Zotero\\storage\\5P94JU58\\Noh et al. - 2017 - Regularizing deep neural networks by noise Its in.pdf;C\:\\Users\\julio\\Zotero\\storage\\P3YY4NZS\\217e342fc01668b10cb1188d40d3370e-Abstract.html}
}

@misc{ObservatorioInsolvenciaRio,
  title = {Observat\'orio Da Insolv\^encia: {{Rio}} de {{Janeiro}}},
  shorttitle = {Observat\'orio Da Insolv\^encia},
  abstract = {Estudo sobre as recupera\c{c}\~oes judiciais do estado do Rio de Janeiro.},
  howpublished = {https://abj.org.br/pesquisas/obsrjrj/},
  file = {C\:\\Users\\julio\\Zotero\\storage\\C97CUUAZ\\obsrjrj.html}
}

@patent{reshefMethodSystemDiscriminating2005,
  title = {Method and System for Discriminating a Human Action from a Computerized Action},
  author = {Reshef, Eran and Raanan, Gil and Solan, Eilon},
  year = {2005},
  month = may,
  number = {US20050114705A1},
  assignee = {Watchfire Corp},
  nationality = {US},
  keywords = {ability challenge,challenge,human,human ability,response},
  file = {C\:\\Users\\julio\\Zotero\\storage\\LGBB9HQD\\Reshef et al. - 2005 - Method and system for discriminating a human actio.pdf}
}

@article{rolnickDeepLearningRobust2017,
  title = {Deep Learning Is Robust to Massive Label Noise},
  author = {Rolnick, David and Veit, Andreas and Belongie, Serge and Shavit, Nir},
  year = {2017},
  journal = {arXiv preprint arXiv:1705.10694},
  eprint = {1705.10694},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\julio\\Zotero\\storage\\56HXNTKC\\Rolnick et al. - 2017 - Deep learning is robust to massive label noise.pdf;C\:\\Users\\julio\\Zotero\\storage\\Y2EUWLYV\\1705.html}
}

@book{suttonReinforcementLearningIntroduction2018,
  title = {Reinforcement Learning: {{An}} Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  publisher = {{MIT press}},
  file = {C\:\\Users\\julio\\Zotero\\storage\\39EYM6XI\\Sutton e Barto - 2018 - Reinforcement learning An introduction.pdf;C\:\\Users\\julio\\Zotero\\storage\\DMBBYYGU\\books.html}
}

@misc{TempoDosProcessos,
  title = {Tempo Dos Processos Relacionados \`a Ado\c{c}\~ao},
  abstract = {An\'alise do tempo dos processos relacionados \`a ado\c{c}\~ao no Brasil, especialmente de processos relativos a desconstitui\c{c}\~ao do poder familiar.},
  howpublished = {https://abj.org.br/pesquisas/adocao/},
  file = {C\:\\Users\\julio\\Zotero\\storage\\76DWFWY4\\adocao.html}
}

@incollection{turingComputingMachineryIntelligence2009,
  title = {Computing Machinery and Intelligence},
  booktitle = {Parsing the Turing Test},
  author = {Turing, Alan M.},
  year = {2009},
  pages = {23--65},
  publisher = {{Springer}},
  file = {C\:\\Users\\julio\\Zotero\\storage\\2K8D7N7Y\\Turing - 2009 - Computing machinery and intelligence.pdf;C\:\\Users\\julio\\Zotero\\storage\\3Q5YU9PL\\978-1-4020-6710-5_3.html}
}

@inproceedings{vonahnCaptchaTellingHumans2003,
  title = {Captcha: {{Telling}} Humans and Computers Apart Automatically},
  shorttitle = {Captcha},
  booktitle = {Proceedings of Eurocrypt},
  author = {Von Ahn, Luis and Blum, Manuel and Hopper, Nicholas and Langford, John},
  year = {2003},
  file = {C\:\\Users\\julio\\Zotero\\storage\\JYTDG6AN\\Von Ahn et al. - 2003 - Captcha Telling humans and computers apart automa.pdf}
}

@article{vonahnReCAPTCHAHumanBasedCharacter2008,
  title = {{{reCAPTCHA}}: {{Human-Based Character Recognition}} via {{Web Security Measures}}},
  shorttitle = {{{reCAPTCHA}}},
  author = {{von Ahn}, Luis and Maurer, Benjamin and McMillen, Colin and Abraham, David and Blum, Manuel},
  year = {2008},
  month = sep,
  journal = {Science},
  volume = {321},
  number = {5895},
  pages = {1465--1468},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1160379},
  abstract = {CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart) are widespread security measures on the World Wide Web that prevent automated programs from abusing online services. They do so by asking humans to perform a task that computers cannot yet perform, such as deciphering distorted characters. Our research explored whether such human effort can be channeled into a useful purpose: helping to digitize old printed material by asking users to decipher scanned words from books that computerized optical character recognition failed to recognize. We showed that this method can transcribe text with a word accuracy exceeding 99\%, matching the guarantee of professional human transcribers. Our apparatus is deployed in more than 40,000 Web sites and has transcribed over 440 million words.},
  langid = {english},
  file = {C\:\\Users\\julio\\Zotero\\storage\\YJ9ZW5PF\\von Ahn et al. - 2008 - reCAPTCHA Human-Based Character Recognition via W.pdf}
}

@techreport{vonahnTellingHumansComputers2002,
  title = {Telling Humans and Computers Apart Automatically or How Lazy Cryptographers Do {{AI}} ({{Tech}}. {{Rep}}. {{No}}. {{CMU-CS-02-117}})},
  author = {{von Ahn}, L. and Blum, M. and Langford, J.},
  year = {2002},
  institution = {{Carnegie Mellon University}}
}

@article{vonahnTellingHumansComputers2004,
  title = {Telling Humans and Computers Apart Automatically},
  author = {Von Ahn, Luis and Blum, Manuel and Langford, John},
  year = {2004},
  journal = {Communications of the ACM},
  volume = {47},
  number = {2},
  pages = {56--60},
  publisher = {{ACM New York, NY, USA}},
  file = {C\:\\Users\\julio\\Zotero\\storage\\KPLT2HTX\\Von Ahn et al. - 2004 - Telling humans and computers apart automatically.pdf;C\:\\Users\\julio\\Zotero\\storage\\66K88CTC\\966389.html}
}

@article{wangMakeComplexCaptchas2021,
  title = {Make Complex Captchas Simple: A Fast Text Captcha Solver Based on a Small Number of Samples},
  shorttitle = {Make Complex Captchas Simple},
  author = {Wang, Yao and Wei, Yuliang and Zhang, Mingjin and Liu, Yang and Wang, Bailing},
  year = {2021},
  journal = {Information Sciences},
  volume = {578},
  pages = {181--194},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\julio\\Zotero\\storage\\E86LGP55\\S0020025521007301.html}
}

@article{xuSurveyCAPTCHATechnologies2020,
  title = {A Survey of {{CAPTCHA}} Technologies to Distinguish between Human and Computer},
  author = {Xu, Xin and Liu, Lei and Li, Bo},
  year = {2020},
  journal = {Neurocomputing},
  volume = {408},
  pages = {292--307},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\julio\\Zotero\\storage\\E8VNTNAB\\S0925231220304896.html}
}

@inproceedings{yeAnotherTextCaptcha2018,
  title = {Yet Another Text Captcha Solver: {{A}} Generative Adversarial Network Based Approach},
  shorttitle = {Yet Another Text Captcha Solver},
  booktitle = {Proceedings of the 2018 {{ACM SIGSAC}} Conference on Computer and Communications Security},
  author = {Ye, Guixin and Tang, Zhanyong and Fang, Dingyi and Zhu, Zhanxing and Feng, Yansong and Xu, Pengfei and Chen, Xiaojiang and Wang, Zheng},
  year = {2018},
  pages = {332--348},
  file = {C\:\\Users\\julio\\Zotero\\storage\\CG2H4BN6\\Ye et al. - 2018 - Yet another text captcha solver A generative adve.pdf;C\:\\Users\\julio\\Zotero\\storage\\97VN79T2\\3243734.html}
}

@article{yuanAdversarialExamplesAttacks2019,
  title = {Adversarial Examples: {{Attacks}} and Defenses for Deep Learning},
  shorttitle = {Adversarial Examples},
  author = {Yuan, Xiaoyong and He, Pan and Zhu, Qile and Li, Xiaolin},
  year = {2019},
  journal = {IEEE transactions on neural networks and learning systems},
  volume = {30},
  number = {9},
  pages = {2805--2824},
  publisher = {{IEEE}},
  file = {C\:\\Users\\julio\\Zotero\\storage\\MSSKGEKJ\\Yuan et al. - 2019 - Adversarial examples Attacks and defenses for dee.pdf;C\:\\Users\\julio\\Zotero\\storage\\6CSAD978\\8611298.html}
}

@article{zhouBriefIntroductionWeakly2018,
  title = {A Brief Introduction to Weakly Supervised Learning},
  author = {Zhou, Zhi-Hua},
  year = {2018},
  journal = {National science review},
  volume = {5},
  number = {1},
  pages = {44--53},
  publisher = {{Oxford University Press}},
  file = {C\:\\Users\\julio\\Zotero\\storage\\G7RLUC4Y\\4093912.html;C\:\\Users\\julio\\Zotero\\storage\\LNKD7FDG\\4093912.html}
}

